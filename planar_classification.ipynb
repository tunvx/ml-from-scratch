{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import nn\n",
    "from optim import SGDOptimizer\n",
    "\n",
    "from supervised_learning import MyMLPClassifier\n",
    "from dataset.load_data import sklearn_to_df, prepare_data_loader\n",
    "from dataset.make_data import load_planar_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Binary classification with simple data\n",
    "(make classification sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = make_classification(\n",
    "        n_samples=1200,\n",
    "        n_features=5,\n",
    "        n_classes=2,\n",
    "        n_clusters_per_class=1,\n",
    "        n_redundant=0,\n",
    "        n_informative=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    # X, Y = load_planar_dataset()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    print (\"train_x's shape: \" + str(X_tr.shape))\n",
    "    print (\"test_x's shape: \" + str(X_te.shape))\n",
    "    print (\"train_y's shape: \" + str(y_tr.shape))\n",
    "    print (\"test_y's shape: \" + str(y_te.shape))\n",
    "    print()\n",
    "\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=0.2, regularization=0.015)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_class):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[10], n_output=n_class, activation='relu')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_my_mlp_for_binary_classification1():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    \n",
    "    n_in, n_class = X_tr.shape[1], 2\n",
    "\n",
    "    num_epochs = 200\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_class)\n",
    "    optimizer, loss_func = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the data loader for training data\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        \n",
    "        # Initialize counters for tracking training progress\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # forward pass: compute logits and loss\n",
    "            batch_logit = model.forward(batch_X)        # output model: logit\n",
    "            loss = loss_func.forward(batch_logit, batch_y)\n",
    "\n",
    "            # backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = loss_func.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            batch_yp = np.argmax(batch_logit, axis=1)   # logit --> label\n",
    "            total_correct += np.sum(batch_yp == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = np.argmax(model.forward(X_te), axis=1)\n",
    "    print(\"\\n My model: Classification report:\\n\", classification_report(y_te, ypred))\n",
    "\n",
    "    \n",
    "    skmodel = MLPClassifier()\n",
    "    skmodel.fit(X_tr, y_tr)\n",
    "\n",
    "     # Print information about the trained scikit-learn MLPClassifier\n",
    "    print(\"\\nScikit-learn MLPClassifier Info:\")\n",
    "    print(\"Number of layers:\", skmodel.n_layers_)\n",
    "    print(\"Number of neurons in each layer:\", skmodel.hidden_layer_sizes)\n",
    "    print(\"Number of output classes:\", skmodel.n_outputs_)\n",
    "    print(\"Activation function:\", skmodel.activation)\n",
    "    print(\"Solver:\", skmodel.solver)\n",
    "    print(\"Learning rate:\", skmodel.learning_rate)\n",
    "    print(\"Initial learning rate:\", skmodel.learning_rate_init)\n",
    "    print(\"Batch size:\", skmodel.batch_size)\n",
    "    print(\"Maximum number of iterations:\", skmodel.max_iter)\n",
    "    # Add more model-specific information as needed\n",
    "\n",
    "    ypred = skmodel.predict(X_te)\n",
    "    print(\"\\n Sklearn model: Classification report:\\n\", classification_report(y_te, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (804, 5)\n",
      "test_x's shape: (396, 5)\n",
      "train_y's shape: (804,)\n",
      "test_y's shape: (396,)\n",
      "\n",
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=5, out_features=10, bias=True)\n",
      "(relu): ReLU()\n",
      "(linear): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "Epoch: 0, loss=0.6092, train_acc=0.7201\n",
      "Epoch: 1, loss=0.4614, train_acc=0.8246\n",
      "Epoch: 2, loss=0.4141, train_acc=0.8433\n",
      "Epoch: 3, loss=0.3841, train_acc=0.8570\n",
      "Epoch: 4, loss=0.3624, train_acc=0.8669\n",
      "Epoch: 5, loss=0.3459, train_acc=0.8744\n",
      "Epoch: 6, loss=0.3328, train_acc=0.8843\n",
      "Epoch: 7, loss=0.3220, train_acc=0.8843\n",
      "Epoch: 8, loss=0.3132, train_acc=0.8893\n",
      "Epoch: 9, loss=0.3058, train_acc=0.8930\n",
      "Epoch: 10, loss=0.2996, train_acc=0.8980\n",
      "Epoch: 11, loss=0.2943, train_acc=0.8980\n",
      "Epoch: 12, loss=0.2897, train_acc=0.9005\n",
      "Epoch: 13, loss=0.2856, train_acc=0.9030\n",
      "Epoch: 14, loss=0.2820, train_acc=0.9030\n",
      "Epoch: 15, loss=0.2787, train_acc=0.9030\n",
      "Epoch: 16, loss=0.2758, train_acc=0.9030\n",
      "Epoch: 17, loss=0.2732, train_acc=0.9030\n",
      "Epoch: 18, loss=0.2707, train_acc=0.9030\n",
      "Epoch: 19, loss=0.2686, train_acc=0.9042\n",
      "Epoch: 20, loss=0.2665, train_acc=0.9042\n",
      "Epoch: 21, loss=0.2647, train_acc=0.9030\n",
      "Epoch: 22, loss=0.2630, train_acc=0.9042\n",
      "Epoch: 23, loss=0.2614, train_acc=0.9042\n",
      "Epoch: 24, loss=0.2600, train_acc=0.9055\n",
      "Epoch: 25, loss=0.2586, train_acc=0.9067\n",
      "Epoch: 26, loss=0.2573, train_acc=0.9080\n",
      "Epoch: 27, loss=0.2561, train_acc=0.9080\n",
      "Epoch: 28, loss=0.2549, train_acc=0.9067\n",
      "Epoch: 29, loss=0.2538, train_acc=0.9067\n",
      "Epoch: 30, loss=0.2528, train_acc=0.9067\n",
      "Epoch: 31, loss=0.2518, train_acc=0.9092\n",
      "Epoch: 32, loss=0.2508, train_acc=0.9092\n",
      "Epoch: 33, loss=0.2500, train_acc=0.9092\n",
      "Epoch: 34, loss=0.2491, train_acc=0.9092\n",
      "Epoch: 35, loss=0.2483, train_acc=0.9092\n",
      "Epoch: 36, loss=0.2474, train_acc=0.9092\n",
      "Epoch: 37, loss=0.2466, train_acc=0.9092\n",
      "Epoch: 38, loss=0.2459, train_acc=0.9092\n",
      "Epoch: 39, loss=0.2452, train_acc=0.9092\n",
      "Epoch: 40, loss=0.2445, train_acc=0.9092\n",
      "Epoch: 41, loss=0.2438, train_acc=0.9104\n",
      "Epoch: 42, loss=0.2432, train_acc=0.9104\n",
      "Epoch: 43, loss=0.2425, train_acc=0.9104\n",
      "Epoch: 44, loss=0.2419, train_acc=0.9104\n",
      "Epoch: 45, loss=0.2413, train_acc=0.9117\n",
      "Epoch: 46, loss=0.2407, train_acc=0.9117\n",
      "Epoch: 47, loss=0.2402, train_acc=0.9117\n",
      "Epoch: 48, loss=0.2396, train_acc=0.9117\n",
      "Epoch: 49, loss=0.2391, train_acc=0.9129\n",
      "Epoch: 50, loss=0.2386, train_acc=0.9129\n",
      "Epoch: 51, loss=0.2382, train_acc=0.9129\n",
      "Epoch: 52, loss=0.2377, train_acc=0.9129\n",
      "Epoch: 53, loss=0.2373, train_acc=0.9129\n",
      "Epoch: 54, loss=0.2368, train_acc=0.9129\n",
      "Epoch: 55, loss=0.2364, train_acc=0.9129\n",
      "Epoch: 56, loss=0.2360, train_acc=0.9129\n",
      "Epoch: 57, loss=0.2356, train_acc=0.9117\n",
      "Epoch: 58, loss=0.2353, train_acc=0.9117\n",
      "Epoch: 59, loss=0.2349, train_acc=0.9117\n",
      "Epoch: 60, loss=0.2345, train_acc=0.9117\n",
      "Epoch: 61, loss=0.2342, train_acc=0.9117\n",
      "Epoch: 62, loss=0.2338, train_acc=0.9117\n",
      "Epoch: 63, loss=0.2335, train_acc=0.9117\n",
      "Epoch: 64, loss=0.2332, train_acc=0.9117\n",
      "Epoch: 65, loss=0.2329, train_acc=0.9104\n",
      "Epoch: 66, loss=0.2326, train_acc=0.9104\n",
      "Epoch: 67, loss=0.2323, train_acc=0.9104\n",
      "Epoch: 68, loss=0.2320, train_acc=0.9104\n",
      "Epoch: 69, loss=0.2317, train_acc=0.9117\n",
      "Epoch: 70, loss=0.2314, train_acc=0.9117\n",
      "Epoch: 71, loss=0.2312, train_acc=0.9117\n",
      "Epoch: 72, loss=0.2309, train_acc=0.9117\n",
      "Epoch: 73, loss=0.2306, train_acc=0.9117\n",
      "Epoch: 74, loss=0.2304, train_acc=0.9117\n",
      "Epoch: 75, loss=0.2301, train_acc=0.9117\n",
      "Epoch: 76, loss=0.2299, train_acc=0.9117\n",
      "Epoch: 77, loss=0.2296, train_acc=0.9117\n",
      "Epoch: 78, loss=0.2294, train_acc=0.9117\n",
      "Epoch: 79, loss=0.2292, train_acc=0.9117\n",
      "Epoch: 80, loss=0.2289, train_acc=0.9117\n",
      "Epoch: 81, loss=0.2287, train_acc=0.9129\n",
      "Epoch: 82, loss=0.2285, train_acc=0.9129\n",
      "Epoch: 83, loss=0.2283, train_acc=0.9129\n",
      "Epoch: 84, loss=0.2281, train_acc=0.9129\n",
      "Epoch: 85, loss=0.2279, train_acc=0.9129\n",
      "Epoch: 86, loss=0.2277, train_acc=0.9129\n",
      "Epoch: 87, loss=0.2275, train_acc=0.9129\n",
      "Epoch: 88, loss=0.2273, train_acc=0.9129\n",
      "Epoch: 89, loss=0.2271, train_acc=0.9129\n",
      "Epoch: 90, loss=0.2269, train_acc=0.9129\n",
      "Epoch: 91, loss=0.2267, train_acc=0.9117\n",
      "Epoch: 92, loss=0.2266, train_acc=0.9117\n",
      "Epoch: 93, loss=0.2264, train_acc=0.9117\n",
      "Epoch: 94, loss=0.2262, train_acc=0.9117\n",
      "Epoch: 95, loss=0.2261, train_acc=0.9117\n",
      "Epoch: 96, loss=0.2259, train_acc=0.9117\n",
      "Epoch: 97, loss=0.2257, train_acc=0.9117\n",
      "Epoch: 98, loss=0.2256, train_acc=0.9117\n",
      "Epoch: 99, loss=0.2254, train_acc=0.9117\n",
      "Epoch: 100, loss=0.2252, train_acc=0.9129\n",
      "Epoch: 101, loss=0.2251, train_acc=0.9129\n",
      "Epoch: 102, loss=0.2249, train_acc=0.9129\n",
      "Epoch: 103, loss=0.2248, train_acc=0.9129\n",
      "Epoch: 104, loss=0.2246, train_acc=0.9142\n",
      "Epoch: 105, loss=0.2245, train_acc=0.9142\n",
      "Epoch: 106, loss=0.2243, train_acc=0.9142\n",
      "Epoch: 107, loss=0.2242, train_acc=0.9142\n",
      "Epoch: 108, loss=0.2241, train_acc=0.9142\n",
      "Epoch: 109, loss=0.2239, train_acc=0.9142\n",
      "Epoch: 110, loss=0.2238, train_acc=0.9142\n",
      "Epoch: 111, loss=0.2237, train_acc=0.9142\n",
      "Epoch: 112, loss=0.2235, train_acc=0.9142\n",
      "Epoch: 113, loss=0.2234, train_acc=0.9142\n",
      "Epoch: 114, loss=0.2233, train_acc=0.9154\n",
      "Epoch: 115, loss=0.2231, train_acc=0.9154\n",
      "Epoch: 116, loss=0.2230, train_acc=0.9154\n",
      "Epoch: 117, loss=0.2229, train_acc=0.9154\n",
      "Epoch: 118, loss=0.2228, train_acc=0.9154\n",
      "Epoch: 119, loss=0.2227, train_acc=0.9154\n",
      "Epoch: 120, loss=0.2225, train_acc=0.9154\n",
      "Epoch: 121, loss=0.2224, train_acc=0.9154\n",
      "Epoch: 122, loss=0.2223, train_acc=0.9154\n",
      "Epoch: 123, loss=0.2222, train_acc=0.9154\n",
      "Epoch: 124, loss=0.2221, train_acc=0.9154\n",
      "Epoch: 125, loss=0.2220, train_acc=0.9154\n",
      "Epoch: 126, loss=0.2218, train_acc=0.9154\n",
      "Epoch: 127, loss=0.2217, train_acc=0.9154\n",
      "Epoch: 128, loss=0.2216, train_acc=0.9154\n",
      "Epoch: 129, loss=0.2215, train_acc=0.9154\n",
      "Epoch: 130, loss=0.2214, train_acc=0.9154\n",
      "Epoch: 131, loss=0.2213, train_acc=0.9154\n",
      "Epoch: 132, loss=0.2212, train_acc=0.9154\n",
      "Epoch: 133, loss=0.2211, train_acc=0.9154\n",
      "Epoch: 134, loss=0.2210, train_acc=0.9154\n",
      "Epoch: 135, loss=0.2208, train_acc=0.9154\n",
      "Epoch: 136, loss=0.2207, train_acc=0.9154\n",
      "Epoch: 137, loss=0.2206, train_acc=0.9167\n",
      "Epoch: 138, loss=0.2205, train_acc=0.9167\n",
      "Epoch: 139, loss=0.2205, train_acc=0.9167\n",
      "Epoch: 140, loss=0.2204, train_acc=0.9167\n",
      "Epoch: 141, loss=0.2203, train_acc=0.9167\n",
      "Epoch: 142, loss=0.2202, train_acc=0.9167\n",
      "Epoch: 143, loss=0.2201, train_acc=0.9167\n",
      "Epoch: 144, loss=0.2200, train_acc=0.9167\n",
      "Epoch: 145, loss=0.2199, train_acc=0.9167\n",
      "Epoch: 146, loss=0.2198, train_acc=0.9167\n",
      "Epoch: 147, loss=0.2197, train_acc=0.9167\n",
      "Epoch: 148, loss=0.2196, train_acc=0.9167\n",
      "Epoch: 149, loss=0.2195, train_acc=0.9167\n",
      "Epoch: 150, loss=0.2194, train_acc=0.9167\n",
      "Epoch: 151, loss=0.2193, train_acc=0.9167\n",
      "Epoch: 152, loss=0.2192, train_acc=0.9167\n",
      "Epoch: 153, loss=0.2191, train_acc=0.9167\n",
      "Epoch: 154, loss=0.2190, train_acc=0.9167\n",
      "Epoch: 155, loss=0.2190, train_acc=0.9167\n",
      "Epoch: 156, loss=0.2189, train_acc=0.9167\n",
      "Epoch: 157, loss=0.2188, train_acc=0.9167\n",
      "Epoch: 158, loss=0.2187, train_acc=0.9167\n",
      "Epoch: 159, loss=0.2186, train_acc=0.9154\n",
      "Epoch: 160, loss=0.2185, train_acc=0.9154\n",
      "Epoch: 161, loss=0.2184, train_acc=0.9142\n",
      "Epoch: 162, loss=0.2183, train_acc=0.9142\n",
      "Epoch: 163, loss=0.2182, train_acc=0.9142\n",
      "Epoch: 164, loss=0.2181, train_acc=0.9142\n",
      "Epoch: 165, loss=0.2180, train_acc=0.9142\n",
      "Epoch: 166, loss=0.2179, train_acc=0.9142\n",
      "Epoch: 167, loss=0.2179, train_acc=0.9142\n",
      "Epoch: 168, loss=0.2178, train_acc=0.9142\n",
      "Epoch: 169, loss=0.2177, train_acc=0.9142\n",
      "Epoch: 170, loss=0.2176, train_acc=0.9142\n",
      "Epoch: 171, loss=0.2175, train_acc=0.9142\n",
      "Epoch: 172, loss=0.2174, train_acc=0.9142\n",
      "Epoch: 173, loss=0.2174, train_acc=0.9142\n",
      "Epoch: 174, loss=0.2173, train_acc=0.9142\n",
      "Epoch: 175, loss=0.2172, train_acc=0.9142\n",
      "Epoch: 176, loss=0.2171, train_acc=0.9142\n",
      "Epoch: 177, loss=0.2170, train_acc=0.9142\n",
      "Epoch: 178, loss=0.2170, train_acc=0.9142\n",
      "Epoch: 179, loss=0.2169, train_acc=0.9142\n",
      "Epoch: 180, loss=0.2168, train_acc=0.9142\n",
      "Epoch: 181, loss=0.2167, train_acc=0.9142\n",
      "Epoch: 182, loss=0.2167, train_acc=0.9142\n",
      "Epoch: 183, loss=0.2166, train_acc=0.9142\n",
      "Epoch: 184, loss=0.2165, train_acc=0.9142\n",
      "Epoch: 185, loss=0.2164, train_acc=0.9142\n",
      "Epoch: 186, loss=0.2164, train_acc=0.9142\n",
      "Epoch: 187, loss=0.2163, train_acc=0.9142\n",
      "Epoch: 188, loss=0.2162, train_acc=0.9142\n",
      "Epoch: 189, loss=0.2161, train_acc=0.9142\n",
      "Epoch: 190, loss=0.2161, train_acc=0.9142\n",
      "Epoch: 191, loss=0.2160, train_acc=0.9142\n",
      "Epoch: 192, loss=0.2159, train_acc=0.9142\n",
      "Epoch: 193, loss=0.2159, train_acc=0.9142\n",
      "Epoch: 194, loss=0.2158, train_acc=0.9142\n",
      "Epoch: 195, loss=0.2157, train_acc=0.9142\n",
      "Epoch: 196, loss=0.2157, train_acc=0.9142\n",
      "Epoch: 197, loss=0.2156, train_acc=0.9142\n",
      "Epoch: 198, loss=0.2155, train_acc=0.9142\n",
      "Epoch: 199, loss=0.2154, train_acc=0.9142\n",
      "\n",
      " My model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91       201\n",
      "           1       0.88      0.93      0.91       195\n",
      "\n",
      "    accuracy                           0.91       396\n",
      "   macro avg       0.91      0.91      0.91       396\n",
      "weighted avg       0.91      0.91      0.91       396\n",
      "\n",
      "\n",
      "Scikit-learn MLPClassifier Info:\n",
      "Number of layers: 3\n",
      "Number of neurons in each layer: (100,)\n",
      "Number of output classes: 1\n",
      "Activation function: relu\n",
      "Solver: adam\n",
      "Learning rate: constant\n",
      "Initial learning rate: 0.001\n",
      "Batch size: auto\n",
      "Maximum number of iterations: 200\n",
      "\n",
      " Sklearn model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92       201\n",
      "           1       0.90      0.94      0.92       195\n",
      "\n",
      "    accuracy                           0.92       396\n",
      "   macro avg       0.92      0.92      0.92       396\n",
      "weighted avg       0.92      0.92      0.92       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funny/miniconda3/envs/pytorch-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "run_my_mlp_for_binary_classification1()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binary classification with planar data \n",
    "(Use Cross Entropty Loss - CE loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = load_planar_dataset()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    print (\"train_x's shape: \" + str(X_tr.shape))\n",
    "    print (\"test_x's shape: \" + str(X_te.shape))\n",
    "    print (\"train_y's shape: \" + str(y_tr.shape))\n",
    "    print (\"test_y's shape: \" + str(y_te.shape))\n",
    "    print()\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=0.1, regularization=0.03, decay_learning_rate=False)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_class):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[4], n_output=n_class, activation='tanh')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_my_mlp_for_binary_classification2():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    \n",
    "    n_in, n_class = X_tr.shape[1], 2\n",
    "\n",
    "    num_epochs = 480\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_class)\n",
    "    optimizer, loss_func = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the data loader for training data\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        \n",
    "        # Initialize counters for tracking training progress\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # forward pass: compute logits and loss\n",
    "            batch_logit = model.forward(batch_X)        # output model: logit\n",
    "            loss = loss_func.forward(batch_logit, batch_y)\n",
    "\n",
    "            # backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = loss_func.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            batch_yp = np.argmax(batch_logit, axis=1)   # logit --> label\n",
    "            total_correct += np.sum(batch_yp == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = np.argmax(model.forward(X_te), axis=1)\n",
    "    print(\"\\n My model: Classification report:\\n\", classification_report(y_te, ypred))\n",
    "\n",
    "    \n",
    "    skmodel = MLPClassifier()\n",
    "    skmodel.fit(X_tr, y_tr)\n",
    "\n",
    "     # Print information about the trained scikit-learn MLPClassifier\n",
    "    print(\"\\nScikit-learn MLPClassifier Info:\")\n",
    "    print(\"Number of layers:\", skmodel.n_layers_)\n",
    "    print(\"Number of neurons in each layer:\", skmodel.hidden_layer_sizes)\n",
    "    print(\"Number of output classes:\", skmodel.n_outputs_)\n",
    "    print(\"Activation function:\", skmodel.activation)\n",
    "    print(\"Solver:\", skmodel.solver)\n",
    "    print(\"Learning rate:\", skmodel.learning_rate)\n",
    "    print(\"Initial learning rate:\", skmodel.learning_rate_init)\n",
    "    print(\"Batch size:\", skmodel.batch_size)\n",
    "    print(\"Maximum number of iterations:\", skmodel.max_iter)\n",
    "    # Add more model-specific information as needed\n",
    "\n",
    "    ypred = skmodel.predict(X_te)\n",
    "    print(\"\\n Sklearn model: Classification report:\\n\", classification_report(y_te, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (670, 2)\n",
      "test_x's shape: (330, 2)\n",
      "train_y's shape: (670,)\n",
      "test_y's shape: (330,)\n",
      "\n",
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=2, out_features=4, bias=True)\n",
      "(tanh): Tanh()\n",
      "(linear): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Epoch: 0, loss=0.8484, train_acc=0.4433\n",
      "Epoch: 1, loss=0.6542, train_acc=0.5881\n",
      "Epoch: 2, loss=0.6235, train_acc=0.6373\n",
      "Epoch: 3, loss=0.6097, train_acc=0.6418\n",
      "Epoch: 4, loss=0.5993, train_acc=0.6493\n",
      "Epoch: 5, loss=0.5885, train_acc=0.6582\n",
      "Epoch: 6, loss=0.5757, train_acc=0.6940\n",
      "Epoch: 7, loss=0.5607, train_acc=0.7687\n",
      "Epoch: 8, loss=0.5443, train_acc=0.7925\n",
      "Epoch: 9, loss=0.5278, train_acc=0.8134\n",
      "Epoch: 10, loss=0.5122, train_acc=0.8328\n",
      "Epoch: 11, loss=0.4980, train_acc=0.8522\n",
      "Epoch: 12, loss=0.4854, train_acc=0.8582\n",
      "Epoch: 13, loss=0.4745, train_acc=0.8612\n",
      "Epoch: 14, loss=0.4651, train_acc=0.8672\n",
      "Epoch: 15, loss=0.4570, train_acc=0.8687\n",
      "Epoch: 16, loss=0.4500, train_acc=0.8746\n",
      "Epoch: 17, loss=0.4440, train_acc=0.8776\n",
      "Epoch: 18, loss=0.4389, train_acc=0.8716\n",
      "Epoch: 19, loss=0.4346, train_acc=0.8746\n",
      "Epoch: 20, loss=0.4308, train_acc=0.8731\n",
      "Epoch: 21, loss=0.4276, train_acc=0.8761\n",
      "Epoch: 22, loss=0.4248, train_acc=0.8776\n",
      "Epoch: 23, loss=0.4224, train_acc=0.8806\n",
      "Epoch: 24, loss=0.4203, train_acc=0.8806\n",
      "Epoch: 25, loss=0.4185, train_acc=0.8806\n",
      "Epoch: 26, loss=0.4169, train_acc=0.8806\n",
      "Epoch: 27, loss=0.4156, train_acc=0.8821\n",
      "Epoch: 28, loss=0.4145, train_acc=0.8836\n",
      "Epoch: 29, loss=0.4135, train_acc=0.8821\n",
      "Epoch: 30, loss=0.4126, train_acc=0.8806\n",
      "Epoch: 31, loss=0.4119, train_acc=0.8821\n",
      "Epoch: 32, loss=0.4113, train_acc=0.8821\n",
      "Epoch: 33, loss=0.4107, train_acc=0.8821\n",
      "Epoch: 34, loss=0.4102, train_acc=0.8851\n",
      "Epoch: 35, loss=0.4098, train_acc=0.8851\n",
      "Epoch: 36, loss=0.4095, train_acc=0.8866\n",
      "Epoch: 37, loss=0.4092, train_acc=0.8866\n",
      "Epoch: 38, loss=0.4089, train_acc=0.8866\n",
      "Epoch: 39, loss=0.4087, train_acc=0.8866\n",
      "Epoch: 40, loss=0.4085, train_acc=0.8866\n",
      "Epoch: 41, loss=0.4083, train_acc=0.8866\n",
      "Epoch: 42, loss=0.4082, train_acc=0.8866\n",
      "Epoch: 43, loss=0.4080, train_acc=0.8866\n",
      "Epoch: 44, loss=0.4079, train_acc=0.8866\n",
      "Epoch: 45, loss=0.4078, train_acc=0.8866\n",
      "Epoch: 46, loss=0.4077, train_acc=0.8866\n",
      "Epoch: 47, loss=0.4076, train_acc=0.8866\n",
      "Epoch: 48, loss=0.4075, train_acc=0.8866\n",
      "Epoch: 49, loss=0.4075, train_acc=0.8866\n",
      "Epoch: 50, loss=0.4074, train_acc=0.8866\n",
      "Epoch: 51, loss=0.4074, train_acc=0.8866\n",
      "Epoch: 52, loss=0.4073, train_acc=0.8866\n",
      "Epoch: 53, loss=0.4073, train_acc=0.8866\n",
      "Epoch: 54, loss=0.4073, train_acc=0.8881\n",
      "Epoch: 55, loss=0.4072, train_acc=0.8881\n",
      "Epoch: 56, loss=0.4072, train_acc=0.8881\n",
      "Epoch: 57, loss=0.4072, train_acc=0.8881\n",
      "Epoch: 58, loss=0.4072, train_acc=0.8881\n",
      "Epoch: 59, loss=0.4071, train_acc=0.8881\n",
      "Epoch: 60, loss=0.4071, train_acc=0.8881\n",
      "Epoch: 61, loss=0.4071, train_acc=0.8881\n",
      "Epoch: 62, loss=0.4071, train_acc=0.8881\n",
      "Epoch: 63, loss=0.4071, train_acc=0.8881\n",
      "Epoch: 64, loss=0.4071, train_acc=0.8881\n",
      "Epoch: 65, loss=0.4071, train_acc=0.8866\n",
      "Epoch: 66, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 67, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 68, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 69, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 70, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 71, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 72, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 73, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 74, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 75, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 76, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 77, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 78, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 79, loss=0.4069, train_acc=0.8866\n",
      "Epoch: 80, loss=0.4069, train_acc=0.8866\n",
      "Epoch: 81, loss=0.4069, train_acc=0.8866\n",
      "Epoch: 82, loss=0.4069, train_acc=0.8866\n",
      "Epoch: 83, loss=0.4069, train_acc=0.8866\n",
      "Epoch: 84, loss=0.4069, train_acc=0.8866\n",
      "Epoch: 85, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 86, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 87, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 88, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 89, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 90, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 91, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 92, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 93, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 94, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 95, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 96, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 97, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 98, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 99, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 100, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 101, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 102, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 103, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 104, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 105, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 106, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 107, loss=0.4069, train_acc=0.8881\n",
      "Epoch: 108, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 109, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 110, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 111, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 112, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 113, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 114, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 115, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 116, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 117, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 118, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 119, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 120, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 121, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 122, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 123, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 124, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 125, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 126, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 127, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 128, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 129, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 130, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 131, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 132, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 133, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 134, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 135, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 136, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 137, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 138, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 139, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 140, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 141, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 142, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 143, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 144, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 145, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 146, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 147, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 148, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 149, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 150, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 151, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 152, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 153, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 154, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 155, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 156, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 157, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 158, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 159, loss=0.4068, train_acc=0.8881\n",
      "Epoch: 160, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 161, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 162, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 163, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 164, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 165, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 166, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 167, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 168, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 169, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 170, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 171, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 172, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 173, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 174, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 175, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 176, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 177, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 178, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 179, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 180, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 181, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 182, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 183, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 184, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 185, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 186, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 187, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 188, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 189, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 190, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 191, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 192, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 193, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 194, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 195, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 196, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 197, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 198, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 199, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 200, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 201, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 202, loss=0.4067, train_acc=0.8881\n",
      "Epoch: 203, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 204, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 205, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 206, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 207, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 208, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 209, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 210, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 211, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 212, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 213, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 214, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 215, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 216, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 217, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 218, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 219, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 220, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 221, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 222, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 223, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 224, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 225, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 226, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 227, loss=0.4066, train_acc=0.8881\n",
      "Epoch: 228, loss=0.4066, train_acc=0.8896\n",
      "Epoch: 229, loss=0.4066, train_acc=0.8896\n",
      "Epoch: 230, loss=0.4066, train_acc=0.8896\n",
      "Epoch: 231, loss=0.4066, train_acc=0.8896\n",
      "Epoch: 232, loss=0.4066, train_acc=0.8896\n",
      "Epoch: 233, loss=0.4066, train_acc=0.8896\n",
      "Epoch: 234, loss=0.4066, train_acc=0.8896\n",
      "Epoch: 235, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 236, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 237, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 238, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 239, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 240, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 241, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 242, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 243, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 244, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 245, loss=0.4065, train_acc=0.8896\n",
      "Epoch: 246, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 247, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 248, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 249, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 250, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 251, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 252, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 253, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 254, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 255, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 256, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 257, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 258, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 259, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 260, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 261, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 262, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 263, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 264, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 265, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 266, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 267, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 268, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 269, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 270, loss=0.4064, train_acc=0.8910\n",
      "Epoch: 271, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 272, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 273, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 274, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 275, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 276, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 277, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 278, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 279, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 280, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 281, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 282, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 283, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 284, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 285, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 286, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 287, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 288, loss=0.4062, train_acc=0.8925\n",
      "Epoch: 289, loss=0.4062, train_acc=0.8925\n",
      "Epoch: 290, loss=0.4062, train_acc=0.8925\n",
      "Epoch: 291, loss=0.4061, train_acc=0.8925\n",
      "Epoch: 292, loss=0.4061, train_acc=0.8925\n",
      "Epoch: 293, loss=0.4061, train_acc=0.8925\n",
      "Epoch: 294, loss=0.4061, train_acc=0.8925\n",
      "Epoch: 295, loss=0.4061, train_acc=0.8925\n",
      "Epoch: 296, loss=0.4061, train_acc=0.8925\n",
      "Epoch: 297, loss=0.4061, train_acc=0.8925\n",
      "Epoch: 298, loss=0.4060, train_acc=0.8925\n",
      "Epoch: 299, loss=0.4060, train_acc=0.8925\n",
      "Epoch: 300, loss=0.4060, train_acc=0.8925\n",
      "Epoch: 301, loss=0.4060, train_acc=0.8925\n",
      "Epoch: 302, loss=0.4060, train_acc=0.8925\n",
      "Epoch: 303, loss=0.4060, train_acc=0.8925\n",
      "Epoch: 304, loss=0.4059, train_acc=0.8925\n",
      "Epoch: 305, loss=0.4059, train_acc=0.8925\n",
      "Epoch: 306, loss=0.4059, train_acc=0.8925\n",
      "Epoch: 307, loss=0.4059, train_acc=0.8925\n",
      "Epoch: 308, loss=0.4059, train_acc=0.8925\n",
      "Epoch: 309, loss=0.4058, train_acc=0.8925\n",
      "Epoch: 310, loss=0.4058, train_acc=0.8925\n",
      "Epoch: 311, loss=0.4058, train_acc=0.8925\n",
      "Epoch: 312, loss=0.4058, train_acc=0.8940\n",
      "Epoch: 313, loss=0.4058, train_acc=0.8940\n",
      "Epoch: 314, loss=0.4057, train_acc=0.8940\n",
      "Epoch: 315, loss=0.4057, train_acc=0.8940\n",
      "Epoch: 316, loss=0.4057, train_acc=0.8940\n",
      "Epoch: 317, loss=0.4057, train_acc=0.8940\n",
      "Epoch: 318, loss=0.4056, train_acc=0.8940\n",
      "Epoch: 319, loss=0.4056, train_acc=0.8940\n",
      "Epoch: 320, loss=0.4056, train_acc=0.8940\n",
      "Epoch: 321, loss=0.4056, train_acc=0.8925\n",
      "Epoch: 322, loss=0.4055, train_acc=0.8925\n",
      "Epoch: 323, loss=0.4055, train_acc=0.8925\n",
      "Epoch: 324, loss=0.4055, train_acc=0.8925\n",
      "Epoch: 325, loss=0.4054, train_acc=0.8925\n",
      "Epoch: 326, loss=0.4054, train_acc=0.8925\n",
      "Epoch: 327, loss=0.4054, train_acc=0.8925\n",
      "Epoch: 328, loss=0.4054, train_acc=0.8910\n",
      "Epoch: 329, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 330, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 331, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 332, loss=0.4052, train_acc=0.8896\n",
      "Epoch: 333, loss=0.4052, train_acc=0.8896\n",
      "Epoch: 334, loss=0.4052, train_acc=0.8896\n",
      "Epoch: 335, loss=0.4051, train_acc=0.8896\n",
      "Epoch: 336, loss=0.4051, train_acc=0.8896\n",
      "Epoch: 337, loss=0.4051, train_acc=0.8896\n",
      "Epoch: 338, loss=0.4051, train_acc=0.8896\n",
      "Epoch: 339, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 340, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 341, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 342, loss=0.4049, train_acc=0.8896\n",
      "Epoch: 343, loss=0.4049, train_acc=0.8896\n",
      "Epoch: 344, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 345, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 346, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 347, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 348, loss=0.4047, train_acc=0.8925\n",
      "Epoch: 349, loss=0.4047, train_acc=0.8925\n",
      "Epoch: 350, loss=0.4047, train_acc=0.8925\n",
      "Epoch: 351, loss=0.4046, train_acc=0.8925\n",
      "Epoch: 352, loss=0.4046, train_acc=0.8925\n",
      "Epoch: 353, loss=0.4046, train_acc=0.8925\n",
      "Epoch: 354, loss=0.4046, train_acc=0.8925\n",
      "Epoch: 355, loss=0.4045, train_acc=0.8925\n",
      "Epoch: 356, loss=0.4045, train_acc=0.8925\n",
      "Epoch: 357, loss=0.4045, train_acc=0.8925\n",
      "Epoch: 358, loss=0.4044, train_acc=0.8925\n",
      "Epoch: 359, loss=0.4044, train_acc=0.8925\n",
      "Epoch: 360, loss=0.4044, train_acc=0.8925\n",
      "Epoch: 361, loss=0.4043, train_acc=0.8925\n",
      "Epoch: 362, loss=0.4043, train_acc=0.8925\n",
      "Epoch: 363, loss=0.4043, train_acc=0.8925\n",
      "Epoch: 364, loss=0.4043, train_acc=0.8925\n",
      "Epoch: 365, loss=0.4042, train_acc=0.8925\n",
      "Epoch: 366, loss=0.4042, train_acc=0.8925\n",
      "Epoch: 367, loss=0.4042, train_acc=0.8925\n",
      "Epoch: 368, loss=0.4042, train_acc=0.8925\n",
      "Epoch: 369, loss=0.4041, train_acc=0.8940\n",
      "Epoch: 370, loss=0.4041, train_acc=0.8940\n",
      "Epoch: 371, loss=0.4041, train_acc=0.8940\n",
      "Epoch: 372, loss=0.4041, train_acc=0.8940\n",
      "Epoch: 373, loss=0.4040, train_acc=0.8940\n",
      "Epoch: 374, loss=0.4040, train_acc=0.8940\n",
      "Epoch: 375, loss=0.4040, train_acc=0.8940\n",
      "Epoch: 376, loss=0.4040, train_acc=0.8955\n",
      "Epoch: 377, loss=0.4040, train_acc=0.8955\n",
      "Epoch: 378, loss=0.4039, train_acc=0.8955\n",
      "Epoch: 379, loss=0.4039, train_acc=0.8955\n",
      "Epoch: 380, loss=0.4039, train_acc=0.8955\n",
      "Epoch: 381, loss=0.4039, train_acc=0.8955\n",
      "Epoch: 382, loss=0.4039, train_acc=0.8955\n",
      "Epoch: 383, loss=0.4038, train_acc=0.8955\n",
      "Epoch: 384, loss=0.4038, train_acc=0.8955\n",
      "Epoch: 385, loss=0.4038, train_acc=0.8955\n",
      "Epoch: 386, loss=0.4038, train_acc=0.8955\n",
      "Epoch: 387, loss=0.4038, train_acc=0.8955\n",
      "Epoch: 388, loss=0.4037, train_acc=0.8955\n",
      "Epoch: 389, loss=0.4037, train_acc=0.8955\n",
      "Epoch: 390, loss=0.4037, train_acc=0.8955\n",
      "Epoch: 391, loss=0.4037, train_acc=0.8955\n",
      "Epoch: 392, loss=0.4037, train_acc=0.8955\n",
      "Epoch: 393, loss=0.4037, train_acc=0.8955\n",
      "Epoch: 394, loss=0.4037, train_acc=0.8955\n",
      "Epoch: 395, loss=0.4036, train_acc=0.8955\n",
      "Epoch: 396, loss=0.4036, train_acc=0.8955\n",
      "Epoch: 397, loss=0.4036, train_acc=0.8955\n",
      "Epoch: 398, loss=0.4036, train_acc=0.8955\n",
      "Epoch: 399, loss=0.4036, train_acc=0.8955\n",
      "Epoch: 400, loss=0.4036, train_acc=0.8955\n",
      "Epoch: 401, loss=0.4036, train_acc=0.8955\n",
      "Epoch: 402, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 403, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 404, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 405, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 406, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 407, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 408, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 409, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 410, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 411, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 412, loss=0.4035, train_acc=0.8955\n",
      "Epoch: 413, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 414, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 415, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 416, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 417, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 418, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 419, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 420, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 421, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 422, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 423, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 424, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 425, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 426, loss=0.4034, train_acc=0.8955\n",
      "Epoch: 427, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 428, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 429, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 430, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 431, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 432, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 433, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 434, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 435, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 436, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 437, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 438, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 439, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 440, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 441, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 442, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 443, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 444, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 445, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 446, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 447, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 448, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 449, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 450, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 451, loss=0.4033, train_acc=0.8955\n",
      "Epoch: 452, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 453, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 454, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 455, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 456, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 457, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 458, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 459, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 460, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 461, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 462, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 463, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 464, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 465, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 466, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 467, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 468, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 469, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 470, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 471, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 472, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 473, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 474, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 475, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 476, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 477, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 478, loss=0.4032, train_acc=0.8955\n",
      "Epoch: 479, loss=0.4032, train_acc=0.8955\n",
      "\n",
      " My model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90       165\n",
      "           1       0.88      0.94      0.91       165\n",
      "\n",
      "    accuracy                           0.91       330\n",
      "   macro avg       0.91      0.91      0.91       330\n",
      "weighted avg       0.91      0.91      0.91       330\n",
      "\n",
      "\n",
      "Scikit-learn MLPClassifier Info:\n",
      "Number of layers: 3\n",
      "Number of neurons in each layer: (100,)\n",
      "Number of output classes: 1\n",
      "Activation function: relu\n",
      "Solver: adam\n",
      "Learning rate: constant\n",
      "Initial learning rate: 0.001\n",
      "Batch size: auto\n",
      "Maximum number of iterations: 200\n",
      "\n",
      " Sklearn model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85       165\n",
      "           1       0.84      0.85      0.85       165\n",
      "\n",
      "    accuracy                           0.85       330\n",
      "   macro avg       0.85      0.85      0.85       330\n",
      "weighted avg       0.85      0.85      0.85       330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funny/miniconda3/envs/pytorch-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = run_my_mlp_for_binary_classification2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Binary classification with planar data \n",
    "(Use binary cross entroy loss - BCE loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = load_planar_dataset()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    print (\"train_x's shape: \" + str(X_tr.shape))\n",
    "    print (\"test_x's shape: \" + str(X_te.shape))\n",
    "    print (\"train_y's shape: \" + str(y_tr.shape))\n",
    "    print (\"test_y's shape: \" + str(y_te.shape))\n",
    "    print()\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=0.1, regularization=0.03, decay_learning_rate=False)\n",
    "    loss_func = nn.BCELoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_class):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[4], n_output=1, activation='tanh')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_my_mlp_for_binary_classification3():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    \n",
    "    n_in, n_class = X_tr.shape[1], 2\n",
    "\n",
    "    num_epochs = 480\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_class)\n",
    "    optimizer, loss_func = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the data loader for training data\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        \n",
    "        # Initialize counters for tracking training progress\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # forward pass: compute logits and loss                     \n",
    "            batch_logit = model.forward(batch_X)    # output model: logit \n",
    "            loss = loss_func.forward(batch_logit, batch_y)\n",
    "\n",
    "            # backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = loss_func.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            probability = loss_func.probability     # logit --> probability\n",
    "            batch_yp = (probability >= 0.5).astype(int).flatten()\n",
    "            total_correct += np.sum(batch_yp == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logits_test = model.forward(X_te)\n",
    "    probability = nn.Sigmoid().forward(logits_test)\n",
    "    ypred = (probability >= 0.5).astype(int)\n",
    "    print(\"\\n My model: Classification report of test set:\\n\", classification_report(y_te, ypred))\n",
    "\n",
    "    \n",
    "    skmodel = MLPClassifier()\n",
    "    skmodel.fit(X_tr, y_tr)\n",
    "\n",
    "     # Print information about the trained scikit-learn MLPClassifier\n",
    "    print(\"\\nScikit-learn MLPClassifier Info:\")\n",
    "    print(\"Number of layers:\", skmodel.n_layers_)\n",
    "    print(\"Number of neurons in each layer:\", skmodel.hidden_layer_sizes)\n",
    "    print(\"Number of output classes:\", skmodel.n_outputs_)\n",
    "    print(\"Activation function:\", skmodel.activation)\n",
    "    print(\"Solver:\", skmodel.solver)\n",
    "    print(\"Learning rate:\", skmodel.learning_rate)\n",
    "    print(\"Initial learning rate:\", skmodel.learning_rate_init)\n",
    "    print(\"Batch size:\", skmodel.batch_size)\n",
    "    print(\"Maximum number of iterations:\", skmodel.max_iter)\n",
    "    # Add more model-specific information as needed\n",
    "\n",
    "    ypred = skmodel.predict(X_te)\n",
    "    print(\"\\n Sklearn model: Classification report of test set:\\n\", classification_report(y_te, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (670, 2)\n",
      "test_x's shape: (330, 2)\n",
      "train_y's shape: (670,)\n",
      "test_y's shape: (330,)\n",
      "\n",
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=2, out_features=4, bias=True)\n",
      "(tanh): Tanh()\n",
      "(linear): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, loss=0.0203, train_acc=0.6239\n",
      "Epoch: 1, loss=0.0195, train_acc=0.5940\n",
      "Epoch: 2, loss=0.0192, train_acc=0.5060\n",
      "Epoch: 3, loss=0.0190, train_acc=0.5045\n",
      "Epoch: 4, loss=0.0189, train_acc=0.5239\n",
      "Epoch: 5, loss=0.0188, train_acc=0.5716\n",
      "Epoch: 6, loss=0.0188, train_acc=0.5970\n",
      "Epoch: 7, loss=0.0188, train_acc=0.6060\n",
      "Epoch: 8, loss=0.0189, train_acc=0.6075\n",
      "Epoch: 9, loss=0.0189, train_acc=0.6075\n",
      "Epoch: 10, loss=0.0189, train_acc=0.6075\n",
      "Epoch: 11, loss=0.0189, train_acc=0.6075\n",
      "Epoch: 12, loss=0.0189, train_acc=0.6075\n",
      "Epoch: 13, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 14, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 15, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 16, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 17, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 18, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 19, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 20, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 21, loss=0.0191, train_acc=0.5955\n",
      "Epoch: 22, loss=0.0191, train_acc=0.5955\n",
      "Epoch: 23, loss=0.0191, train_acc=0.5955\n",
      "Epoch: 24, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 25, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 26, loss=0.0191, train_acc=0.5925\n",
      "Epoch: 27, loss=0.0191, train_acc=0.5910\n",
      "Epoch: 28, loss=0.0191, train_acc=0.5910\n",
      "Epoch: 29, loss=0.0191, train_acc=0.5910\n",
      "Epoch: 30, loss=0.0191, train_acc=0.5910\n",
      "Epoch: 31, loss=0.0191, train_acc=0.5910\n",
      "Epoch: 32, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 33, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 34, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 35, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 36, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 37, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 38, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 39, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 40, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 41, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 42, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 43, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 44, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 45, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 46, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 47, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 48, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 49, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 50, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 51, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 52, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 53, loss=0.0190, train_acc=0.6075\n",
      "Epoch: 54, loss=0.0189, train_acc=0.6104\n",
      "Epoch: 55, loss=0.0189, train_acc=0.6149\n",
      "Epoch: 56, loss=0.0189, train_acc=0.6179\n",
      "Epoch: 57, loss=0.0189, train_acc=0.6179\n",
      "Epoch: 58, loss=0.0188, train_acc=0.6224\n",
      "Epoch: 59, loss=0.0188, train_acc=0.6284\n",
      "Epoch: 60, loss=0.0187, train_acc=0.6373\n",
      "Epoch: 61, loss=0.0186, train_acc=0.6478\n",
      "Epoch: 62, loss=0.0184, train_acc=0.6836\n",
      "Epoch: 63, loss=0.0183, train_acc=0.7164\n",
      "Epoch: 64, loss=0.0181, train_acc=0.7478\n",
      "Epoch: 65, loss=0.0179, train_acc=0.7716\n",
      "Epoch: 66, loss=0.0176, train_acc=0.7821\n",
      "Epoch: 67, loss=0.0174, train_acc=0.7925\n",
      "Epoch: 68, loss=0.0172, train_acc=0.8060\n",
      "Epoch: 69, loss=0.0170, train_acc=0.8164\n",
      "Epoch: 70, loss=0.0168, train_acc=0.8299\n",
      "Epoch: 71, loss=0.0166, train_acc=0.8373\n",
      "Epoch: 72, loss=0.0164, train_acc=0.8403\n",
      "Epoch: 73, loss=0.0162, train_acc=0.8522\n",
      "Epoch: 74, loss=0.0160, train_acc=0.8537\n",
      "Epoch: 75, loss=0.0159, train_acc=0.8582\n",
      "Epoch: 76, loss=0.0158, train_acc=0.8612\n",
      "Epoch: 77, loss=0.0157, train_acc=0.8657\n",
      "Epoch: 78, loss=0.0156, train_acc=0.8657\n",
      "Epoch: 79, loss=0.0155, train_acc=0.8672\n",
      "Epoch: 80, loss=0.0154, train_acc=0.8672\n",
      "Epoch: 81, loss=0.0153, train_acc=0.8672\n",
      "Epoch: 82, loss=0.0152, train_acc=0.8672\n",
      "Epoch: 83, loss=0.0152, train_acc=0.8657\n",
      "Epoch: 84, loss=0.0151, train_acc=0.8657\n",
      "Epoch: 85, loss=0.0150, train_acc=0.8642\n",
      "Epoch: 86, loss=0.0150, train_acc=0.8642\n",
      "Epoch: 87, loss=0.0150, train_acc=0.8657\n",
      "Epoch: 88, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 89, loss=0.0149, train_acc=0.8701\n",
      "Epoch: 90, loss=0.0148, train_acc=0.8687\n",
      "Epoch: 91, loss=0.0148, train_acc=0.8716\n",
      "Epoch: 92, loss=0.0148, train_acc=0.8716\n",
      "Epoch: 93, loss=0.0148, train_acc=0.8716\n",
      "Epoch: 94, loss=0.0147, train_acc=0.8716\n",
      "Epoch: 95, loss=0.0147, train_acc=0.8701\n",
      "Epoch: 96, loss=0.0147, train_acc=0.8701\n",
      "Epoch: 97, loss=0.0147, train_acc=0.8701\n",
      "Epoch: 98, loss=0.0147, train_acc=0.8701\n",
      "Epoch: 99, loss=0.0147, train_acc=0.8701\n",
      "Epoch: 100, loss=0.0147, train_acc=0.8701\n",
      "Epoch: 101, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 102, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 103, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 104, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 105, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 106, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 107, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 108, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 109, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 110, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 111, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 112, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 113, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 114, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 115, loss=0.0146, train_acc=0.8701\n",
      "Epoch: 116, loss=0.0146, train_acc=0.8687\n",
      "Epoch: 117, loss=0.0146, train_acc=0.8687\n",
      "Epoch: 118, loss=0.0146, train_acc=0.8687\n",
      "Epoch: 119, loss=0.0146, train_acc=0.8687\n",
      "Epoch: 120, loss=0.0146, train_acc=0.8687\n",
      "Epoch: 121, loss=0.0146, train_acc=0.8687\n",
      "Epoch: 122, loss=0.0146, train_acc=0.8687\n",
      "Epoch: 123, loss=0.0146, train_acc=0.8672\n",
      "Epoch: 124, loss=0.0146, train_acc=0.8672\n",
      "Epoch: 125, loss=0.0146, train_acc=0.8672\n",
      "Epoch: 126, loss=0.0146, train_acc=0.8672\n",
      "Epoch: 127, loss=0.0146, train_acc=0.8672\n",
      "Epoch: 128, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 129, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 130, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 131, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 132, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 133, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 134, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 135, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 136, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 137, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 138, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 139, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 140, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 141, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 142, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 143, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 144, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 145, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 146, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 147, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 148, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 149, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 150, loss=0.0145, train_acc=0.8672\n",
      "Epoch: 151, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 152, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 153, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 154, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 155, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 156, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 157, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 158, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 159, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 160, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 161, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 162, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 163, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 164, loss=0.0145, train_acc=0.8687\n",
      "Epoch: 165, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 166, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 167, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 168, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 169, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 170, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 171, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 172, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 173, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 174, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 175, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 176, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 177, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 178, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 179, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 180, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 181, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 182, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 183, loss=0.0145, train_acc=0.8716\n",
      "Epoch: 184, loss=0.0145, train_acc=0.8716\n",
      "Epoch: 185, loss=0.0145, train_acc=0.8716\n",
      "Epoch: 186, loss=0.0145, train_acc=0.8716\n",
      "Epoch: 187, loss=0.0145, train_acc=0.8716\n",
      "Epoch: 188, loss=0.0145, train_acc=0.8716\n",
      "Epoch: 189, loss=0.0145, train_acc=0.8716\n",
      "Epoch: 190, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 191, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 192, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 193, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 194, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 195, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 196, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 197, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 198, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 199, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 200, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 201, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 202, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 203, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 204, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 205, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 206, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 207, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 208, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 209, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 210, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 211, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 212, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 213, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 214, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 215, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 216, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 217, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 218, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 219, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 220, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 221, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 222, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 223, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 224, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 225, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 226, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 227, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 228, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 229, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 230, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 231, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 232, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 233, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 234, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 235, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 236, loss=0.0145, train_acc=0.8701\n",
      "Epoch: 237, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 238, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 239, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 240, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 241, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 242, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 243, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 244, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 245, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 246, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 247, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 248, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 249, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 250, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 251, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 252, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 253, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 254, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 255, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 256, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 257, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 258, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 259, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 260, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 261, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 262, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 263, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 264, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 265, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 266, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 267, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 268, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 269, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 270, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 271, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 272, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 273, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 274, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 275, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 276, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 277, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 278, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 279, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 280, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 281, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 282, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 283, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 284, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 285, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 286, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 287, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 288, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 289, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 290, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 291, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 292, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 293, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 294, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 295, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 296, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 297, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 298, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 299, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 300, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 301, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 302, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 303, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 304, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 305, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 306, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 307, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 308, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 309, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 310, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 311, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 312, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 313, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 314, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 315, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 316, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 317, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 318, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 319, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 320, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 321, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 322, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 323, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 324, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 325, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 326, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 327, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 328, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 329, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 330, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 331, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 332, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 333, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 334, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 335, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 336, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 337, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 338, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 339, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 340, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 341, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 342, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 343, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 344, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 345, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 346, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 347, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 348, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 349, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 350, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 351, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 352, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 353, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 354, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 355, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 356, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 357, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 358, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 359, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 360, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 361, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 362, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 363, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 364, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 365, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 366, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 367, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 368, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 369, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 370, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 371, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 372, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 373, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 374, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 375, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 376, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 377, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 378, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 379, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 380, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 381, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 382, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 383, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 384, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 385, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 386, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 387, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 388, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 389, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 390, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 391, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 392, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 393, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 394, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 395, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 396, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 397, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 398, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 399, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 400, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 401, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 402, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 403, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 404, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 405, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 406, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 407, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 408, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 409, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 410, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 411, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 412, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 413, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 414, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 415, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 416, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 417, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 418, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 419, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 420, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 421, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 422, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 423, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 424, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 425, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 426, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 427, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 428, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 429, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 430, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 431, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 432, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 433, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 434, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 435, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 436, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 437, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 438, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 439, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 440, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 441, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 442, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 443, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 444, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 445, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 446, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 447, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 448, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 449, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 450, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 451, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 452, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 453, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 454, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 455, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 456, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 457, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 458, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 459, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 460, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 461, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 462, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 463, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 464, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 465, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 466, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 467, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 468, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 469, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 470, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 471, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 472, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 473, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 474, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 475, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 476, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 477, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 478, loss=0.0144, train_acc=0.8701\n",
      "Epoch: 479, loss=0.0144, train_acc=0.8701\n",
      "\n",
      " My model: Classification report of test set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.84       165\n",
      "           1       0.84      0.86      0.85       165\n",
      "\n",
      "    accuracy                           0.85       330\n",
      "   macro avg       0.85      0.85      0.85       330\n",
      "weighted avg       0.85      0.85      0.85       330\n",
      "\n",
      "\n",
      "Scikit-learn MLPClassifier Info:\n",
      "Number of layers: 3\n",
      "Number of neurons in each layer: (100,)\n",
      "Number of output classes: 1\n",
      "Activation function: relu\n",
      "Solver: adam\n",
      "Learning rate: constant\n",
      "Initial learning rate: 0.001\n",
      "Batch size: auto\n",
      "Maximum number of iterations: 200\n",
      "\n",
      " Sklearn model: Classification report of test set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85       165\n",
      "           1       0.84      0.85      0.85       165\n",
      "\n",
      "    accuracy                           0.85       330\n",
      "   macro avg       0.85      0.85      0.85       330\n",
      "weighted avg       0.85      0.85      0.85       330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funny/miniconda3/envs/pytorch-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = run_my_mlp_for_binary_classification3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
