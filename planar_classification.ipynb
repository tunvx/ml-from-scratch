{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import nn\n",
    "from optim import SGDOptimizer\n",
    "\n",
    "from supervised_learning import MyMLPClassifier\n",
    "from dataset.load_data import sklearn_to_df, prepare_data_loader\n",
    "from dataset.make_data import load_planar_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Binary classification with simple data\n",
    "(make classification sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = make_classification(\n",
    "        n_samples=1200,\n",
    "        n_features=5,\n",
    "        n_classes=2,\n",
    "        n_clusters_per_class=1,\n",
    "        n_redundant=0,\n",
    "        n_informative=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    # X, Y = load_planar_dataset()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    print (\"train_x's shape: \" + str(X_tr.shape))\n",
    "    print (\"test_x's shape: \" + str(X_te.shape))\n",
    "    print (\"train_y's shape: \" + str(y_tr.shape))\n",
    "    print (\"test_y's shape: \" + str(y_te.shape))\n",
    "    print()\n",
    "\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=0.2, regularization=0.015)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_class):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[10], n_output=n_class, activation='relu')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_my_mlp_for_binary_classification1():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    \n",
    "    n_in, n_class = X_tr.shape[1], 2\n",
    "\n",
    "    num_epochs = 200\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_class)\n",
    "    optimizer, loss_func = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the data loader for training data\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        \n",
    "        # Initialize counters for tracking training progress\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # forward pass: compute logits and loss\n",
    "            batch_logit = model.forward(batch_X)        # output model: logit\n",
    "            loss = loss_func.forward(batch_logit, batch_y)\n",
    "\n",
    "            # backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = loss_func.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            batch_yp = np.argmax(batch_logit, axis=1)   # logit --> label\n",
    "            total_correct += np.sum(batch_yp == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = np.argmax(model.forward(X_te), axis=1)\n",
    "    print(\"\\n My model: Classification report:\\n\", classification_report(y_te, ypred))\n",
    "\n",
    "    \n",
    "    skmodel = MLPClassifier()\n",
    "    skmodel.fit(X_tr, y_tr)\n",
    "\n",
    "     # Print information about the trained scikit-learn MLPClassifier\n",
    "    print(\"\\nScikit-learn MLPClassifier Info:\")\n",
    "    print(\"Number of layers:\", skmodel.n_layers_)\n",
    "    print(\"Number of neurons in each layer:\", skmodel.hidden_layer_sizes)\n",
    "    print(\"Number of output classes:\", skmodel.n_outputs_)\n",
    "    print(\"Activation function:\", skmodel.activation)\n",
    "    print(\"Solver:\", skmodel.solver)\n",
    "    print(\"Learning rate:\", skmodel.learning_rate)\n",
    "    print(\"Initial learning rate:\", skmodel.learning_rate_init)\n",
    "    print(\"Batch size:\", skmodel.batch_size)\n",
    "    print(\"Maximum number of iterations:\", skmodel.max_iter)\n",
    "    # Add more model-specific information as needed\n",
    "\n",
    "    ypred = skmodel.predict(X_te)\n",
    "    print(\"\\n Sklearn model: Classification report:\\n\", classification_report(y_te, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (804, 5)\n",
      "test_x's shape: (396, 5)\n",
      "train_y's shape: (804,)\n",
      "test_y's shape: (396,)\n",
      "\n",
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=5, out_features=10, bias=True)\n",
      "(relu): ReLU()\n",
      "(linear): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "Shape ((5, 10), (5, 10))\n",
      "Shape ((1, 10), (1, 10))\n",
      "Shape ((1,), (1,))\n",
      "Shape ((10, 2), (10, 2))\n",
      "Shape ((1, 2), (1, 2))\n",
      "Epoch: 0, loss=0.6247, train_acc=0.7201\n",
      "Epoch: 1, loss=0.4695, train_acc=0.8159\n",
      "Epoch: 2, loss=0.4245, train_acc=0.8445\n",
      "Epoch: 3, loss=0.3968, train_acc=0.8532\n",
      "Epoch: 4, loss=0.3758, train_acc=0.8682\n",
      "Epoch: 5, loss=0.3605, train_acc=0.8694\n",
      "Epoch: 6, loss=0.3487, train_acc=0.8756\n",
      "Epoch: 7, loss=0.3398, train_acc=0.8843\n",
      "Epoch: 8, loss=0.3313, train_acc=0.8856\n",
      "Epoch: 9, loss=0.3246, train_acc=0.8868\n",
      "Epoch: 10, loss=0.3185, train_acc=0.8881\n",
      "Epoch: 11, loss=0.3136, train_acc=0.8881\n",
      "Epoch: 12, loss=0.3090, train_acc=0.8930\n",
      "Epoch: 13, loss=0.3050, train_acc=0.8930\n",
      "Epoch: 14, loss=0.3020, train_acc=0.8955\n",
      "Epoch: 15, loss=0.2988, train_acc=0.8943\n",
      "Epoch: 16, loss=0.2960, train_acc=0.8955\n",
      "Epoch: 17, loss=0.2932, train_acc=0.8955\n",
      "Epoch: 18, loss=0.2909, train_acc=0.8955\n",
      "Epoch: 19, loss=0.2888, train_acc=0.8955\n",
      "Epoch: 20, loss=0.2866, train_acc=0.8968\n",
      "Epoch: 21, loss=0.2847, train_acc=0.8993\n",
      "Epoch: 22, loss=0.2830, train_acc=0.8993\n",
      "Epoch: 23, loss=0.2814, train_acc=0.9030\n",
      "Epoch: 24, loss=0.2795, train_acc=0.9017\n",
      "Epoch: 25, loss=0.2779, train_acc=0.9030\n",
      "Epoch: 26, loss=0.2765, train_acc=0.9005\n",
      "Epoch: 27, loss=0.2750, train_acc=0.8993\n",
      "Epoch: 28, loss=0.2735, train_acc=0.9017\n",
      "Epoch: 29, loss=0.2725, train_acc=0.8993\n",
      "Epoch: 30, loss=0.2712, train_acc=0.8993\n",
      "Epoch: 31, loss=0.2699, train_acc=0.9017\n",
      "Epoch: 32, loss=0.2687, train_acc=0.9017\n",
      "Epoch: 33, loss=0.2677, train_acc=0.9017\n",
      "Epoch: 34, loss=0.2666, train_acc=0.9055\n",
      "Epoch: 35, loss=0.2657, train_acc=0.9030\n",
      "Epoch: 36, loss=0.2648, train_acc=0.9067\n",
      "Epoch: 37, loss=0.2635, train_acc=0.9055\n",
      "Epoch: 38, loss=0.2627, train_acc=0.9080\n",
      "Epoch: 39, loss=0.2618, train_acc=0.9080\n",
      "Epoch: 40, loss=0.2609, train_acc=0.9067\n",
      "Epoch: 41, loss=0.2600, train_acc=0.9055\n",
      "Epoch: 42, loss=0.2593, train_acc=0.9080\n",
      "Epoch: 43, loss=0.2583, train_acc=0.9080\n",
      "Epoch: 44, loss=0.2575, train_acc=0.9092\n",
      "Epoch: 45, loss=0.2568, train_acc=0.9092\n",
      "Epoch: 46, loss=0.2561, train_acc=0.9080\n",
      "Epoch: 47, loss=0.2552, train_acc=0.9092\n",
      "Epoch: 48, loss=0.2546, train_acc=0.9092\n",
      "Epoch: 49, loss=0.2538, train_acc=0.9092\n",
      "Epoch: 50, loss=0.2533, train_acc=0.9092\n",
      "Epoch: 51, loss=0.2525, train_acc=0.9092\n",
      "Epoch: 52, loss=0.2519, train_acc=0.9092\n",
      "Epoch: 53, loss=0.2513, train_acc=0.9092\n",
      "Epoch: 54, loss=0.2507, train_acc=0.9092\n",
      "Epoch: 55, loss=0.2502, train_acc=0.9092\n",
      "Epoch: 56, loss=0.2495, train_acc=0.9092\n",
      "Epoch: 57, loss=0.2490, train_acc=0.9092\n",
      "Epoch: 58, loss=0.2484, train_acc=0.9092\n",
      "Epoch: 59, loss=0.2477, train_acc=0.9092\n",
      "Epoch: 60, loss=0.2473, train_acc=0.9104\n",
      "Epoch: 61, loss=0.2467, train_acc=0.9092\n",
      "Epoch: 62, loss=0.2464, train_acc=0.9092\n",
      "Epoch: 63, loss=0.2459, train_acc=0.9092\n",
      "Epoch: 64, loss=0.2455, train_acc=0.9092\n",
      "Epoch: 65, loss=0.2450, train_acc=0.9092\n",
      "Epoch: 66, loss=0.2446, train_acc=0.9092\n",
      "Epoch: 67, loss=0.2442, train_acc=0.9092\n",
      "Epoch: 68, loss=0.2438, train_acc=0.9092\n",
      "Epoch: 69, loss=0.2434, train_acc=0.9092\n",
      "Epoch: 70, loss=0.2429, train_acc=0.9092\n",
      "Epoch: 71, loss=0.2427, train_acc=0.9092\n",
      "Epoch: 72, loss=0.2422, train_acc=0.9092\n",
      "Epoch: 73, loss=0.2419, train_acc=0.9092\n",
      "Epoch: 74, loss=0.2415, train_acc=0.9104\n",
      "Epoch: 75, loss=0.2413, train_acc=0.9092\n",
      "Epoch: 76, loss=0.2409, train_acc=0.9104\n",
      "Epoch: 77, loss=0.2405, train_acc=0.9092\n",
      "Epoch: 78, loss=0.2402, train_acc=0.9104\n",
      "Epoch: 79, loss=0.2399, train_acc=0.9104\n",
      "Epoch: 80, loss=0.2396, train_acc=0.9117\n",
      "Epoch: 81, loss=0.2394, train_acc=0.9117\n",
      "Epoch: 82, loss=0.2389, train_acc=0.9117\n",
      "Epoch: 83, loss=0.2388, train_acc=0.9117\n",
      "Epoch: 84, loss=0.2385, train_acc=0.9117\n",
      "Epoch: 85, loss=0.2382, train_acc=0.9117\n",
      "Epoch: 86, loss=0.2379, train_acc=0.9117\n",
      "Epoch: 87, loss=0.2375, train_acc=0.9117\n",
      "Epoch: 88, loss=0.2372, train_acc=0.9117\n",
      "Epoch: 89, loss=0.2371, train_acc=0.9117\n",
      "Epoch: 90, loss=0.2367, train_acc=0.9117\n",
      "Epoch: 91, loss=0.2366, train_acc=0.9117\n",
      "Epoch: 92, loss=0.2364, train_acc=0.9129\n",
      "Epoch: 93, loss=0.2360, train_acc=0.9117\n",
      "Epoch: 94, loss=0.2358, train_acc=0.9142\n",
      "Epoch: 95, loss=0.2355, train_acc=0.9117\n",
      "Epoch: 96, loss=0.2353, train_acc=0.9117\n",
      "Epoch: 97, loss=0.2351, train_acc=0.9117\n",
      "Epoch: 98, loss=0.2349, train_acc=0.9129\n",
      "Epoch: 99, loss=0.2345, train_acc=0.9117\n",
      "Epoch: 100, loss=0.2343, train_acc=0.9117\n",
      "Epoch: 101, loss=0.2340, train_acc=0.9117\n",
      "Epoch: 102, loss=0.2339, train_acc=0.9129\n",
      "Epoch: 103, loss=0.2337, train_acc=0.9117\n",
      "Epoch: 104, loss=0.2334, train_acc=0.9129\n",
      "Epoch: 105, loss=0.2332, train_acc=0.9142\n",
      "Epoch: 106, loss=0.2330, train_acc=0.9129\n",
      "Epoch: 107, loss=0.2328, train_acc=0.9129\n",
      "Epoch: 108, loss=0.2326, train_acc=0.9129\n",
      "Epoch: 109, loss=0.2324, train_acc=0.9129\n",
      "Epoch: 110, loss=0.2322, train_acc=0.9129\n",
      "Epoch: 111, loss=0.2320, train_acc=0.9129\n",
      "Epoch: 112, loss=0.2318, train_acc=0.9129\n",
      "Epoch: 113, loss=0.2317, train_acc=0.9142\n",
      "Epoch: 114, loss=0.2314, train_acc=0.9129\n",
      "Epoch: 115, loss=0.2313, train_acc=0.9142\n",
      "Epoch: 116, loss=0.2311, train_acc=0.9129\n",
      "Epoch: 117, loss=0.2309, train_acc=0.9129\n",
      "Epoch: 118, loss=0.2307, train_acc=0.9129\n",
      "Epoch: 119, loss=0.2305, train_acc=0.9117\n",
      "Epoch: 120, loss=0.2304, train_acc=0.9117\n",
      "Epoch: 121, loss=0.2303, train_acc=0.9129\n",
      "Epoch: 122, loss=0.2301, train_acc=0.9129\n",
      "Epoch: 123, loss=0.2300, train_acc=0.9117\n",
      "Epoch: 124, loss=0.2298, train_acc=0.9142\n",
      "Epoch: 125, loss=0.2297, train_acc=0.9129\n",
      "Epoch: 126, loss=0.2295, train_acc=0.9142\n",
      "Epoch: 127, loss=0.2293, train_acc=0.9117\n",
      "Epoch: 128, loss=0.2292, train_acc=0.9129\n",
      "Epoch: 129, loss=0.2290, train_acc=0.9129\n",
      "Epoch: 130, loss=0.2289, train_acc=0.9129\n",
      "Epoch: 131, loss=0.2287, train_acc=0.9142\n",
      "Epoch: 132, loss=0.2286, train_acc=0.9142\n",
      "Epoch: 133, loss=0.2284, train_acc=0.9142\n",
      "Epoch: 134, loss=0.2283, train_acc=0.9142\n",
      "Epoch: 135, loss=0.2282, train_acc=0.9142\n",
      "Epoch: 136, loss=0.2281, train_acc=0.9129\n",
      "Epoch: 137, loss=0.2279, train_acc=0.9142\n",
      "Epoch: 138, loss=0.2277, train_acc=0.9142\n",
      "Epoch: 139, loss=0.2276, train_acc=0.9142\n",
      "Epoch: 140, loss=0.2276, train_acc=0.9142\n",
      "Epoch: 141, loss=0.2274, train_acc=0.9129\n",
      "Epoch: 142, loss=0.2272, train_acc=0.9129\n",
      "Epoch: 143, loss=0.2271, train_acc=0.9142\n",
      "Epoch: 144, loss=0.2270, train_acc=0.9142\n",
      "Epoch: 145, loss=0.2269, train_acc=0.9154\n",
      "Epoch: 146, loss=0.2267, train_acc=0.9154\n",
      "Epoch: 147, loss=0.2266, train_acc=0.9142\n",
      "Epoch: 148, loss=0.2265, train_acc=0.9154\n",
      "Epoch: 149, loss=0.2264, train_acc=0.9154\n",
      "Epoch: 150, loss=0.2262, train_acc=0.9154\n",
      "Epoch: 151, loss=0.2261, train_acc=0.9154\n",
      "Epoch: 152, loss=0.2260, train_acc=0.9154\n",
      "Epoch: 153, loss=0.2259, train_acc=0.9154\n",
      "Epoch: 154, loss=0.2257, train_acc=0.9154\n",
      "Epoch: 155, loss=0.2256, train_acc=0.9154\n",
      "Epoch: 156, loss=0.2254, train_acc=0.9154\n",
      "Epoch: 157, loss=0.2253, train_acc=0.9154\n",
      "Epoch: 158, loss=0.2253, train_acc=0.9167\n",
      "Epoch: 159, loss=0.2251, train_acc=0.9167\n",
      "Epoch: 160, loss=0.2250, train_acc=0.9154\n",
      "Epoch: 161, loss=0.2249, train_acc=0.9142\n",
      "Epoch: 162, loss=0.2248, train_acc=0.9154\n",
      "Epoch: 163, loss=0.2246, train_acc=0.9154\n",
      "Epoch: 164, loss=0.2245, train_acc=0.9154\n",
      "Epoch: 165, loss=0.2245, train_acc=0.9154\n",
      "Epoch: 166, loss=0.2243, train_acc=0.9167\n",
      "Epoch: 167, loss=0.2243, train_acc=0.9167\n",
      "Epoch: 168, loss=0.2241, train_acc=0.9167\n",
      "Epoch: 169, loss=0.2240, train_acc=0.9154\n",
      "Epoch: 170, loss=0.2239, train_acc=0.9154\n",
      "Epoch: 171, loss=0.2238, train_acc=0.9167\n",
      "Epoch: 172, loss=0.2238, train_acc=0.9154\n",
      "Epoch: 173, loss=0.2236, train_acc=0.9154\n",
      "Epoch: 174, loss=0.2235, train_acc=0.9167\n",
      "Epoch: 175, loss=0.2234, train_acc=0.9154\n",
      "Epoch: 176, loss=0.2234, train_acc=0.9154\n",
      "Epoch: 177, loss=0.2233, train_acc=0.9154\n",
      "Epoch: 178, loss=0.2232, train_acc=0.9154\n",
      "Epoch: 179, loss=0.2230, train_acc=0.9154\n",
      "Epoch: 180, loss=0.2230, train_acc=0.9154\n",
      "Epoch: 181, loss=0.2229, train_acc=0.9154\n",
      "Epoch: 182, loss=0.2228, train_acc=0.9167\n",
      "Epoch: 183, loss=0.2228, train_acc=0.9154\n",
      "Epoch: 184, loss=0.2227, train_acc=0.9154\n",
      "Epoch: 185, loss=0.2226, train_acc=0.9142\n",
      "Epoch: 186, loss=0.2225, train_acc=0.9154\n",
      "Epoch: 187, loss=0.2225, train_acc=0.9154\n",
      "Epoch: 188, loss=0.2224, train_acc=0.9154\n",
      "Epoch: 189, loss=0.2223, train_acc=0.9167\n",
      "Epoch: 190, loss=0.2223, train_acc=0.9154\n",
      "Epoch: 191, loss=0.2222, train_acc=0.9167\n",
      "Epoch: 192, loss=0.2220, train_acc=0.9167\n",
      "Epoch: 193, loss=0.2220, train_acc=0.9154\n",
      "Epoch: 194, loss=0.2219, train_acc=0.9167\n",
      "Epoch: 195, loss=0.2218, train_acc=0.9167\n",
      "Epoch: 196, loss=0.2217, train_acc=0.9154\n",
      "Epoch: 197, loss=0.2216, train_acc=0.9142\n",
      "Epoch: 198, loss=0.2215, train_acc=0.9154\n",
      "Epoch: 199, loss=0.2214, train_acc=0.9142\n",
      "\n",
      " My model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       201\n",
      "           1       0.88      0.92      0.90       195\n",
      "\n",
      "    accuracy                           0.90       396\n",
      "   macro avg       0.90      0.90      0.90       396\n",
      "weighted avg       0.90      0.90      0.90       396\n",
      "\n",
      "\n",
      "Scikit-learn MLPClassifier Info:\n",
      "Number of layers: 3\n",
      "Number of neurons in each layer: (100,)\n",
      "Number of output classes: 1\n",
      "Activation function: relu\n",
      "Solver: adam\n",
      "Learning rate: constant\n",
      "Initial learning rate: 0.001\n",
      "Batch size: auto\n",
      "Maximum number of iterations: 200\n",
      "\n",
      " Sklearn model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92       201\n",
      "           1       0.90      0.94      0.92       195\n",
      "\n",
      "    accuracy                           0.92       396\n",
      "   macro avg       0.92      0.92      0.92       396\n",
      "weighted avg       0.92      0.92      0.92       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funny/miniconda3/envs/pytorch-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "run_my_mlp_for_binary_classification1()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binary classification with planar data \n",
    "(Use Cross Entropty Loss - CE loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = load_planar_dataset()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    print (\"train_x's shape: \" + str(X_tr.shape))\n",
    "    print (\"test_x's shape: \" + str(X_te.shape))\n",
    "    print (\"train_y's shape: \" + str(y_tr.shape))\n",
    "    print (\"test_y's shape: \" + str(y_te.shape))\n",
    "    print()\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=1, regularization=0.03)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_class):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[4], n_output=n_class, activation='tanh')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_my_mlp_for_binary_classification2():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    \n",
    "    n_in, n_class = X_tr.shape[1], 2\n",
    "\n",
    "    num_epochs = 480\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_class)\n",
    "    optimizer, loss_func = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the data loader for training data\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        \n",
    "        # Initialize counters for tracking training progress\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # forward pass: compute logits and loss\n",
    "            batch_logit = model.forward(batch_X)        # output model: logit\n",
    "            loss = loss_func.forward(batch_logit, batch_y)\n",
    "\n",
    "            # backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = loss_func.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            batch_yp = np.argmax(batch_logit, axis=1)   # logit --> label\n",
    "            total_correct += np.sum(batch_yp == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = np.argmax(model.forward(X_te), axis=1)\n",
    "    print(\"\\n My model: Classification report:\\n\", classification_report(y_te, ypred))\n",
    "\n",
    "    \n",
    "    skmodel = MLPClassifier()\n",
    "    skmodel.fit(X_tr, y_tr)\n",
    "\n",
    "     # Print information about the trained scikit-learn MLPClassifier\n",
    "    print(\"\\nScikit-learn MLPClassifier Info:\")\n",
    "    print(\"Number of layers:\", skmodel.n_layers_)\n",
    "    print(\"Number of neurons in each layer:\", skmodel.hidden_layer_sizes)\n",
    "    print(\"Number of output classes:\", skmodel.n_outputs_)\n",
    "    print(\"Activation function:\", skmodel.activation)\n",
    "    print(\"Solver:\", skmodel.solver)\n",
    "    print(\"Learning rate:\", skmodel.learning_rate)\n",
    "    print(\"Initial learning rate:\", skmodel.learning_rate_init)\n",
    "    print(\"Batch size:\", skmodel.batch_size)\n",
    "    print(\"Maximum number of iterations:\", skmodel.max_iter)\n",
    "    # Add more model-specific information as needed\n",
    "\n",
    "    ypred = skmodel.predict(X_te)\n",
    "    print(\"\\n Sklearn model: Classification report:\\n\", classification_report(y_te, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (670, 2)\n",
      "test_x's shape: (330, 2)\n",
      "train_y's shape: (670,)\n",
      "test_y's shape: (330,)\n",
      "\n",
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=2, out_features=4, bias=True)\n",
      "(tanh): Tanh()\n",
      "(linear): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Shape ((2, 4), (2, 4))\n",
      "Shape ((1, 4), (1, 4))\n",
      "Shape ((1,), (1,))\n",
      "Shape ((4, 2), (4, 2))\n",
      "Shape ((1, 2), (1, 2))\n",
      "Epoch: 0, loss=0.6667, train_acc=0.5821\n",
      "Epoch: 1, loss=0.6075, train_acc=0.6373\n",
      "Epoch: 2, loss=0.5946, train_acc=0.6507\n",
      "Epoch: 3, loss=0.5828, train_acc=0.6672\n",
      "Epoch: 4, loss=0.5676, train_acc=0.7343\n",
      "Epoch: 5, loss=0.5522, train_acc=0.7552\n",
      "Epoch: 6, loss=0.5374, train_acc=0.8179\n",
      "Epoch: 7, loss=0.5233, train_acc=0.8090\n",
      "Epoch: 8, loss=0.5120, train_acc=0.8269\n",
      "Epoch: 9, loss=0.5007, train_acc=0.8313\n",
      "Epoch: 10, loss=0.4915, train_acc=0.8463\n",
      "Epoch: 11, loss=0.4841, train_acc=0.8537\n",
      "Epoch: 12, loss=0.4764, train_acc=0.8522\n",
      "Epoch: 13, loss=0.4703, train_acc=0.8537\n",
      "Epoch: 14, loss=0.4650, train_acc=0.8672\n",
      "Epoch: 15, loss=0.4601, train_acc=0.8582\n",
      "Epoch: 16, loss=0.4557, train_acc=0.8657\n",
      "Epoch: 17, loss=0.4520, train_acc=0.8701\n",
      "Epoch: 18, loss=0.4487, train_acc=0.8582\n",
      "Epoch: 19, loss=0.4452, train_acc=0.8687\n",
      "Epoch: 20, loss=0.4428, train_acc=0.8687\n",
      "Epoch: 21, loss=0.4402, train_acc=0.8731\n",
      "Epoch: 22, loss=0.4382, train_acc=0.8791\n",
      "Epoch: 23, loss=0.4356, train_acc=0.8761\n",
      "Epoch: 24, loss=0.4335, train_acc=0.8687\n",
      "Epoch: 25, loss=0.4322, train_acc=0.8791\n",
      "Epoch: 26, loss=0.4308, train_acc=0.8716\n",
      "Epoch: 27, loss=0.4293, train_acc=0.8791\n",
      "Epoch: 28, loss=0.4274, train_acc=0.8716\n",
      "Epoch: 29, loss=0.4261, train_acc=0.8791\n",
      "Epoch: 30, loss=0.4253, train_acc=0.8821\n",
      "Epoch: 31, loss=0.4238, train_acc=0.8806\n",
      "Epoch: 32, loss=0.4235, train_acc=0.8791\n",
      "Epoch: 33, loss=0.4221, train_acc=0.8866\n",
      "Epoch: 34, loss=0.4214, train_acc=0.8746\n",
      "Epoch: 35, loss=0.4200, train_acc=0.8836\n",
      "Epoch: 36, loss=0.4194, train_acc=0.8791\n",
      "Epoch: 37, loss=0.4186, train_acc=0.8776\n",
      "Epoch: 38, loss=0.4182, train_acc=0.8821\n",
      "Epoch: 39, loss=0.4177, train_acc=0.8821\n",
      "Epoch: 40, loss=0.4171, train_acc=0.8806\n",
      "Epoch: 41, loss=0.4165, train_acc=0.8866\n",
      "Epoch: 42, loss=0.4161, train_acc=0.8776\n",
      "Epoch: 43, loss=0.4154, train_acc=0.8821\n",
      "Epoch: 44, loss=0.4148, train_acc=0.8851\n",
      "Epoch: 45, loss=0.4144, train_acc=0.8806\n",
      "Epoch: 46, loss=0.4142, train_acc=0.8851\n",
      "Epoch: 47, loss=0.4138, train_acc=0.8821\n",
      "Epoch: 48, loss=0.4134, train_acc=0.8881\n",
      "Epoch: 49, loss=0.4130, train_acc=0.8836\n",
      "Epoch: 50, loss=0.4124, train_acc=0.8836\n",
      "Epoch: 51, loss=0.4122, train_acc=0.8866\n",
      "Epoch: 52, loss=0.4121, train_acc=0.8836\n",
      "Epoch: 53, loss=0.4117, train_acc=0.8806\n",
      "Epoch: 54, loss=0.4115, train_acc=0.8866\n",
      "Epoch: 55, loss=0.4113, train_acc=0.8806\n",
      "Epoch: 56, loss=0.4111, train_acc=0.8866\n",
      "Epoch: 57, loss=0.4107, train_acc=0.8851\n",
      "Epoch: 58, loss=0.4104, train_acc=0.8806\n",
      "Epoch: 59, loss=0.4105, train_acc=0.8836\n",
      "Epoch: 60, loss=0.4100, train_acc=0.8836\n",
      "Epoch: 61, loss=0.4100, train_acc=0.8851\n",
      "Epoch: 62, loss=0.4098, train_acc=0.8896\n",
      "Epoch: 63, loss=0.4097, train_acc=0.8836\n",
      "Epoch: 64, loss=0.4096, train_acc=0.8881\n",
      "Epoch: 65, loss=0.4090, train_acc=0.8821\n",
      "Epoch: 66, loss=0.4092, train_acc=0.8851\n",
      "Epoch: 67, loss=0.4089, train_acc=0.8806\n",
      "Epoch: 68, loss=0.4088, train_acc=0.8836\n",
      "Epoch: 69, loss=0.4087, train_acc=0.8896\n",
      "Epoch: 70, loss=0.4086, train_acc=0.8866\n",
      "Epoch: 71, loss=0.4087, train_acc=0.8910\n",
      "Epoch: 72, loss=0.4084, train_acc=0.8866\n",
      "Epoch: 73, loss=0.4085, train_acc=0.8851\n",
      "Epoch: 74, loss=0.4081, train_acc=0.8866\n",
      "Epoch: 75, loss=0.4079, train_acc=0.8896\n",
      "Epoch: 76, loss=0.4080, train_acc=0.8881\n",
      "Epoch: 77, loss=0.4079, train_acc=0.8881\n",
      "Epoch: 78, loss=0.4077, train_acc=0.8881\n",
      "Epoch: 79, loss=0.4076, train_acc=0.8896\n",
      "Epoch: 80, loss=0.4078, train_acc=0.8910\n",
      "Epoch: 81, loss=0.4076, train_acc=0.8896\n",
      "Epoch: 82, loss=0.4077, train_acc=0.8896\n",
      "Epoch: 83, loss=0.4075, train_acc=0.8881\n",
      "Epoch: 84, loss=0.4076, train_acc=0.8851\n",
      "Epoch: 85, loss=0.4073, train_acc=0.8851\n",
      "Epoch: 86, loss=0.4073, train_acc=0.8896\n",
      "Epoch: 87, loss=0.4072, train_acc=0.8896\n",
      "Epoch: 88, loss=0.4072, train_acc=0.8910\n",
      "Epoch: 89, loss=0.4070, train_acc=0.8881\n",
      "Epoch: 90, loss=0.4069, train_acc=0.8851\n",
      "Epoch: 91, loss=0.4070, train_acc=0.8866\n",
      "Epoch: 92, loss=0.4067, train_acc=0.8851\n",
      "Epoch: 93, loss=0.4069, train_acc=0.8896\n",
      "Epoch: 94, loss=0.4069, train_acc=0.8866\n",
      "Epoch: 95, loss=0.4068, train_acc=0.8851\n",
      "Epoch: 96, loss=0.4066, train_acc=0.8910\n",
      "Epoch: 97, loss=0.4066, train_acc=0.8866\n",
      "Epoch: 98, loss=0.4065, train_acc=0.8881\n",
      "Epoch: 99, loss=0.4066, train_acc=0.8940\n",
      "Epoch: 100, loss=0.4063, train_acc=0.8896\n",
      "Epoch: 101, loss=0.4064, train_acc=0.8866\n",
      "Epoch: 102, loss=0.4066, train_acc=0.8851\n",
      "Epoch: 103, loss=0.4065, train_acc=0.8910\n",
      "Epoch: 104, loss=0.4063, train_acc=0.8896\n",
      "Epoch: 105, loss=0.4064, train_acc=0.8866\n",
      "Epoch: 106, loss=0.4063, train_acc=0.8910\n",
      "Epoch: 107, loss=0.4064, train_acc=0.8866\n",
      "Epoch: 108, loss=0.4063, train_acc=0.8866\n",
      "Epoch: 109, loss=0.4063, train_acc=0.8896\n",
      "Epoch: 110, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 111, loss=0.4062, train_acc=0.8896\n",
      "Epoch: 112, loss=0.4062, train_acc=0.8896\n",
      "Epoch: 113, loss=0.4061, train_acc=0.8881\n",
      "Epoch: 114, loss=0.4062, train_acc=0.8910\n",
      "Epoch: 115, loss=0.4062, train_acc=0.8925\n",
      "Epoch: 116, loss=0.4061, train_acc=0.8896\n",
      "Epoch: 117, loss=0.4060, train_acc=0.8896\n",
      "Epoch: 118, loss=0.4061, train_acc=0.8896\n",
      "Epoch: 119, loss=0.4059, train_acc=0.8910\n",
      "Epoch: 120, loss=0.4059, train_acc=0.8866\n",
      "Epoch: 121, loss=0.4059, train_acc=0.8836\n",
      "Epoch: 122, loss=0.4059, train_acc=0.8881\n",
      "Epoch: 123, loss=0.4058, train_acc=0.8896\n",
      "Epoch: 124, loss=0.4058, train_acc=0.8866\n",
      "Epoch: 125, loss=0.4058, train_acc=0.8940\n",
      "Epoch: 126, loss=0.4060, train_acc=0.8881\n",
      "Epoch: 127, loss=0.4057, train_acc=0.8925\n",
      "Epoch: 128, loss=0.4058, train_acc=0.8851\n",
      "Epoch: 129, loss=0.4059, train_acc=0.8866\n",
      "Epoch: 130, loss=0.4060, train_acc=0.8925\n",
      "Epoch: 131, loss=0.4057, train_acc=0.8925\n",
      "Epoch: 132, loss=0.4057, train_acc=0.8925\n",
      "Epoch: 133, loss=0.4057, train_acc=0.8910\n",
      "Epoch: 134, loss=0.4057, train_acc=0.8866\n",
      "Epoch: 135, loss=0.4056, train_acc=0.8881\n",
      "Epoch: 136, loss=0.4058, train_acc=0.8896\n",
      "Epoch: 137, loss=0.4057, train_acc=0.8925\n",
      "Epoch: 138, loss=0.4057, train_acc=0.8940\n",
      "Epoch: 139, loss=0.4057, train_acc=0.8881\n",
      "Epoch: 140, loss=0.4057, train_acc=0.8896\n",
      "Epoch: 141, loss=0.4056, train_acc=0.8866\n",
      "Epoch: 142, loss=0.4056, train_acc=0.8925\n",
      "Epoch: 143, loss=0.4055, train_acc=0.8910\n",
      "Epoch: 144, loss=0.4054, train_acc=0.8896\n",
      "Epoch: 145, loss=0.4057, train_acc=0.8925\n",
      "Epoch: 146, loss=0.4055, train_acc=0.8896\n",
      "Epoch: 147, loss=0.4056, train_acc=0.8896\n",
      "Epoch: 148, loss=0.4055, train_acc=0.8896\n",
      "Epoch: 149, loss=0.4055, train_acc=0.8925\n",
      "Epoch: 150, loss=0.4056, train_acc=0.8925\n",
      "Epoch: 151, loss=0.4056, train_acc=0.8896\n",
      "Epoch: 152, loss=0.4056, train_acc=0.8910\n",
      "Epoch: 153, loss=0.4054, train_acc=0.8925\n",
      "Epoch: 154, loss=0.4054, train_acc=0.8881\n",
      "Epoch: 155, loss=0.4054, train_acc=0.8910\n",
      "Epoch: 156, loss=0.4055, train_acc=0.8925\n",
      "Epoch: 157, loss=0.4053, train_acc=0.8896\n",
      "Epoch: 158, loss=0.4055, train_acc=0.8955\n",
      "Epoch: 159, loss=0.4054, train_acc=0.8896\n",
      "Epoch: 160, loss=0.4054, train_acc=0.8896\n",
      "Epoch: 161, loss=0.4055, train_acc=0.8866\n",
      "Epoch: 162, loss=0.4055, train_acc=0.8940\n",
      "Epoch: 163, loss=0.4054, train_acc=0.8896\n",
      "Epoch: 164, loss=0.4053, train_acc=0.8925\n",
      "Epoch: 165, loss=0.4053, train_acc=0.8925\n",
      "Epoch: 166, loss=0.4054, train_acc=0.8940\n",
      "Epoch: 167, loss=0.4054, train_acc=0.8896\n",
      "Epoch: 168, loss=0.4054, train_acc=0.8881\n",
      "Epoch: 169, loss=0.4054, train_acc=0.8910\n",
      "Epoch: 170, loss=0.4054, train_acc=0.8866\n",
      "Epoch: 171, loss=0.4052, train_acc=0.8896\n",
      "Epoch: 172, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 173, loss=0.4053, train_acc=0.8896\n",
      "Epoch: 174, loss=0.4054, train_acc=0.8925\n",
      "Epoch: 175, loss=0.4053, train_acc=0.8925\n",
      "Epoch: 176, loss=0.4053, train_acc=0.8881\n",
      "Epoch: 177, loss=0.4054, train_acc=0.8940\n",
      "Epoch: 178, loss=0.4053, train_acc=0.8940\n",
      "Epoch: 179, loss=0.4054, train_acc=0.8896\n",
      "Epoch: 180, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 181, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 182, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 183, loss=0.4053, train_acc=0.8881\n",
      "Epoch: 184, loss=0.4054, train_acc=0.8925\n",
      "Epoch: 185, loss=0.4053, train_acc=0.8940\n",
      "Epoch: 186, loss=0.4053, train_acc=0.8896\n",
      "Epoch: 187, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 188, loss=0.4052, train_acc=0.8940\n",
      "Epoch: 189, loss=0.4052, train_acc=0.8910\n",
      "Epoch: 190, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 191, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 192, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 193, loss=0.4053, train_acc=0.8910\n",
      "Epoch: 194, loss=0.4052, train_acc=0.8881\n",
      "Epoch: 195, loss=0.4050, train_acc=0.8881\n",
      "Epoch: 196, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 197, loss=0.4051, train_acc=0.8866\n",
      "Epoch: 198, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 199, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 200, loss=0.4052, train_acc=0.8866\n",
      "Epoch: 201, loss=0.4052, train_acc=0.8866\n",
      "Epoch: 202, loss=0.4052, train_acc=0.8955\n",
      "Epoch: 203, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 204, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 205, loss=0.4053, train_acc=0.8925\n",
      "Epoch: 206, loss=0.4052, train_acc=0.8896\n",
      "Epoch: 207, loss=0.4051, train_acc=0.8910\n",
      "Epoch: 208, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 209, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 210, loss=0.4052, train_acc=0.8910\n",
      "Epoch: 211, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 212, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 213, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 214, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 215, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 216, loss=0.4052, train_acc=0.8910\n",
      "Epoch: 217, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 218, loss=0.4051, train_acc=0.8881\n",
      "Epoch: 219, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 220, loss=0.4052, train_acc=0.8896\n",
      "Epoch: 221, loss=0.4051, train_acc=0.8896\n",
      "Epoch: 222, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 223, loss=0.4051, train_acc=0.8866\n",
      "Epoch: 224, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 225, loss=0.4049, train_acc=0.8896\n",
      "Epoch: 226, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 227, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 228, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 229, loss=0.4052, train_acc=0.8925\n",
      "Epoch: 230, loss=0.4051, train_acc=0.8896\n",
      "Epoch: 231, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 232, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 233, loss=0.4051, train_acc=0.8910\n",
      "Epoch: 234, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 235, loss=0.4050, train_acc=0.8881\n",
      "Epoch: 236, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 237, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 238, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 239, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 240, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 241, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 242, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 243, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 244, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 245, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 246, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 247, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 248, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 249, loss=0.4051, train_acc=0.8896\n",
      "Epoch: 250, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 251, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 252, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 253, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 254, loss=0.4051, train_acc=0.8881\n",
      "Epoch: 255, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 256, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 257, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 258, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 259, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 260, loss=0.4051, train_acc=0.8910\n",
      "Epoch: 261, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 262, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 263, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 264, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 265, loss=0.4051, train_acc=0.8940\n",
      "Epoch: 266, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 267, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 268, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 269, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 270, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 271, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 272, loss=0.4051, train_acc=0.8910\n",
      "Epoch: 273, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 274, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 275, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 276, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 277, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 278, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 279, loss=0.4049, train_acc=0.8970\n",
      "Epoch: 280, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 281, loss=0.4051, train_acc=0.8910\n",
      "Epoch: 282, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 283, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 284, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 285, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 286, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 287, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 288, loss=0.4049, train_acc=0.8896\n",
      "Epoch: 289, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 290, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 291, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 292, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 293, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 294, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 295, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 296, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 297, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 298, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 299, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 300, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 301, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 302, loss=0.4049, train_acc=0.8896\n",
      "Epoch: 303, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 304, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 305, loss=0.4049, train_acc=0.8970\n",
      "Epoch: 306, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 307, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 308, loss=0.4051, train_acc=0.8925\n",
      "Epoch: 309, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 310, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 311, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 312, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 313, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 314, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 315, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 316, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 317, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 318, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 319, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 320, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 321, loss=0.4049, train_acc=0.8970\n",
      "Epoch: 322, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 323, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 324, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 325, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 326, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 327, loss=0.4049, train_acc=0.8896\n",
      "Epoch: 328, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 329, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 330, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 331, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 332, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 333, loss=0.4050, train_acc=0.8896\n",
      "Epoch: 334, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 335, loss=0.4050, train_acc=0.8925\n",
      "Epoch: 336, loss=0.4048, train_acc=0.8910\n",
      "Epoch: 337, loss=0.4049, train_acc=0.8896\n",
      "Epoch: 338, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 339, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 340, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 341, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 342, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 343, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 344, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 345, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 346, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 347, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 348, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 349, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 350, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 351, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 352, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 353, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 354, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 355, loss=0.4050, train_acc=0.8910\n",
      "Epoch: 356, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 357, loss=0.4048, train_acc=0.8970\n",
      "Epoch: 358, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 359, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 360, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 361, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 362, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 363, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 364, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 365, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 366, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 367, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 368, loss=0.4050, train_acc=0.8940\n",
      "Epoch: 369, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 370, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 371, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 372, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 373, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 374, loss=0.4049, train_acc=0.8970\n",
      "Epoch: 375, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 376, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 377, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 378, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 379, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 380, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 381, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 382, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 383, loss=0.4049, train_acc=0.8910\n",
      "Epoch: 384, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 385, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 386, loss=0.4048, train_acc=0.8910\n",
      "Epoch: 387, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 388, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 389, loss=0.4048, train_acc=0.8910\n",
      "Epoch: 390, loss=0.4048, train_acc=0.8970\n",
      "Epoch: 391, loss=0.4050, train_acc=0.8955\n",
      "Epoch: 392, loss=0.4049, train_acc=0.8970\n",
      "Epoch: 393, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 394, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 395, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 396, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 397, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 398, loss=0.4048, train_acc=0.8910\n",
      "Epoch: 399, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 400, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 401, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 402, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 403, loss=0.4048, train_acc=0.8896\n",
      "Epoch: 404, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 405, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 406, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 407, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 408, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 409, loss=0.4047, train_acc=0.8940\n",
      "Epoch: 410, loss=0.4048, train_acc=0.8970\n",
      "Epoch: 411, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 412, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 413, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 414, loss=0.4047, train_acc=0.8970\n",
      "Epoch: 415, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 416, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 417, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 418, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 419, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 420, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 421, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 422, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 423, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 424, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 425, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 426, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 427, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 428, loss=0.4048, train_acc=0.8970\n",
      "Epoch: 429, loss=0.4048, train_acc=0.8910\n",
      "Epoch: 430, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 431, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 432, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 433, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 434, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 435, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 436, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 437, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 438, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 439, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 440, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 441, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 442, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 443, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 444, loss=0.4049, train_acc=0.8940\n",
      "Epoch: 445, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 446, loss=0.4047, train_acc=0.8955\n",
      "Epoch: 447, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 448, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 449, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 450, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 451, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 452, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 453, loss=0.4049, train_acc=0.8955\n",
      "Epoch: 454, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 455, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 456, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 457, loss=0.4049, train_acc=0.8925\n",
      "Epoch: 458, loss=0.4048, train_acc=0.8970\n",
      "Epoch: 459, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 460, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 461, loss=0.4048, train_acc=0.8970\n",
      "Epoch: 462, loss=0.4047, train_acc=0.8955\n",
      "Epoch: 463, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 464, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 465, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 466, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 467, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 468, loss=0.4047, train_acc=0.8955\n",
      "Epoch: 469, loss=0.4047, train_acc=0.8970\n",
      "Epoch: 470, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 471, loss=0.4048, train_acc=0.8925\n",
      "Epoch: 472, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 473, loss=0.4048, train_acc=0.8940\n",
      "Epoch: 474, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 475, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 476, loss=0.4048, train_acc=0.8955\n",
      "Epoch: 477, loss=0.4047, train_acc=0.8970\n",
      "Epoch: 478, loss=0.4048, train_acc=0.8910\n",
      "Epoch: 479, loss=0.4048, train_acc=0.8955\n",
      "\n",
      " My model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88       165\n",
      "           1       0.86      0.92      0.89       165\n",
      "\n",
      "    accuracy                           0.88       330\n",
      "   macro avg       0.89      0.88      0.88       330\n",
      "weighted avg       0.89      0.88      0.88       330\n",
      "\n",
      "\n",
      "Scikit-learn MLPClassifier Info:\n",
      "Number of layers: 3\n",
      "Number of neurons in each layer: (100,)\n",
      "Number of output classes: 1\n",
      "Activation function: relu\n",
      "Solver: adam\n",
      "Learning rate: constant\n",
      "Initial learning rate: 0.001\n",
      "Batch size: auto\n",
      "Maximum number of iterations: 200\n",
      "\n",
      " Sklearn model: Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85       165\n",
      "           1       0.84      0.85      0.85       165\n",
      "\n",
      "    accuracy                           0.85       330\n",
      "   macro avg       0.85      0.85      0.85       330\n",
      "weighted avg       0.85      0.85      0.85       330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funny/miniconda3/envs/pytorch-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = run_my_mlp_for_binary_classification2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Binary classification with planar data \n",
    "(Use binary cross entroy loss - BCE loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = load_planar_dataset()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    print (\"train_x's shape: \" + str(X_tr.shape))\n",
    "    print (\"test_x's shape: \" + str(X_te.shape))\n",
    "    print (\"train_y's shape: \" + str(y_tr.shape))\n",
    "    print (\"test_y's shape: \" + str(y_te.shape))\n",
    "    print()\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=1, regularization=0.03)\n",
    "    loss_func = nn.BCELoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_class):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[4], n_output=1, activation='tanh')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_my_mlp_for_binary_classification3():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    \n",
    "    n_in, n_class = X_tr.shape[1], 2\n",
    "\n",
    "    num_epochs = 480\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_class)\n",
    "    optimizer, loss_func = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the data loader for training data\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        \n",
    "        # Initialize counters for tracking training progress\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # forward pass: compute logits and loss                     \n",
    "            batch_logit = model.forward(batch_X)    # output model: logit \n",
    "            loss = loss_func.forward(batch_logit, batch_y)\n",
    "\n",
    "            # backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = loss_func.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            probability = loss_func.probability     # logit --> probability\n",
    "            batch_yp = (probability >= 0.5).astype(int).flatten()\n",
    "            total_correct += np.sum(batch_yp == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logits_test = model.forward(X_te)\n",
    "    probability = nn.Sigmoid().forward(logits_test)\n",
    "    ypred = (probability >= 0.5).astype(int)\n",
    "    print(\"\\n My model: Classification report of test set:\\n\", classification_report(y_te, ypred))\n",
    "\n",
    "    \n",
    "    skmodel = MLPClassifier()\n",
    "    skmodel.fit(X_tr, y_tr)\n",
    "\n",
    "     # Print information about the trained scikit-learn MLPClassifier\n",
    "    print(\"\\nScikit-learn MLPClassifier Info:\")\n",
    "    print(\"Number of layers:\", skmodel.n_layers_)\n",
    "    print(\"Number of neurons in each layer:\", skmodel.hidden_layer_sizes)\n",
    "    print(\"Number of output classes:\", skmodel.n_outputs_)\n",
    "    print(\"Activation function:\", skmodel.activation)\n",
    "    print(\"Solver:\", skmodel.solver)\n",
    "    print(\"Learning rate:\", skmodel.learning_rate)\n",
    "    print(\"Initial learning rate:\", skmodel.learning_rate_init)\n",
    "    print(\"Batch size:\", skmodel.batch_size)\n",
    "    print(\"Maximum number of iterations:\", skmodel.max_iter)\n",
    "    # Add more model-specific information as needed\n",
    "\n",
    "    ypred = skmodel.predict(X_te)\n",
    "    print(\"\\n Sklearn model: Classification report of test set:\\n\", classification_report(y_te, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (670, 2)\n",
      "test_x's shape: (330, 2)\n",
      "train_y's shape: (670,)\n",
      "test_y's shape: (330,)\n",
      "\n",
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=2, out_features=4, bias=True)\n",
      "(tanh): Tanh()\n",
      "(linear): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "Shape ((2, 4), (2, 4))\n",
      "Shape ((1, 4), (1, 4))\n",
      "Shape ((1,), (1,))\n",
      "Shape ((4, 1), (4, 1))\n",
      "Shape ((1, 1), (1, 1))\n",
      "Epoch: 0, loss=0.0197, train_acc=0.5612\n",
      "Epoch: 1, loss=0.0190, train_acc=0.5881\n",
      "Epoch: 2, loss=0.0189, train_acc=0.5209\n",
      "Epoch: 3, loss=0.0189, train_acc=0.5761\n",
      "Epoch: 4, loss=0.0189, train_acc=0.6090\n",
      "Epoch: 5, loss=0.0189, train_acc=0.6134\n",
      "Epoch: 6, loss=0.0189, train_acc=0.5687\n",
      "Epoch: 7, loss=0.0189, train_acc=0.6119\n",
      "Epoch: 8, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 9, loss=0.0190, train_acc=0.6075\n",
      "Epoch: 10, loss=0.0190, train_acc=0.5896\n",
      "Epoch: 11, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 12, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 13, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 14, loss=0.0190, train_acc=0.6104\n",
      "Epoch: 15, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 16, loss=0.0190, train_acc=0.6075\n",
      "Epoch: 17, loss=0.0190, train_acc=0.5866\n",
      "Epoch: 18, loss=0.0190, train_acc=0.6119\n",
      "Epoch: 19, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 20, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 21, loss=0.0190, train_acc=0.5821\n",
      "Epoch: 22, loss=0.0190, train_acc=0.5836\n",
      "Epoch: 23, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 24, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 25, loss=0.0190, train_acc=0.5881\n",
      "Epoch: 26, loss=0.0191, train_acc=0.6045\n",
      "Epoch: 27, loss=0.0191, train_acc=0.5866\n",
      "Epoch: 28, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 29, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 30, loss=0.0191, train_acc=0.5925\n",
      "Epoch: 31, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 32, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 33, loss=0.0191, train_acc=0.5910\n",
      "Epoch: 34, loss=0.0191, train_acc=0.6045\n",
      "Epoch: 35, loss=0.0191, train_acc=0.5866\n",
      "Epoch: 36, loss=0.0191, train_acc=0.5925\n",
      "Epoch: 37, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 38, loss=0.0191, train_acc=0.5925\n",
      "Epoch: 39, loss=0.0191, train_acc=0.5925\n",
      "Epoch: 40, loss=0.0191, train_acc=0.5910\n",
      "Epoch: 41, loss=0.0191, train_acc=0.5955\n",
      "Epoch: 42, loss=0.0191, train_acc=0.5896\n",
      "Epoch: 43, loss=0.0191, train_acc=0.5896\n",
      "Epoch: 44, loss=0.0191, train_acc=0.5866\n",
      "Epoch: 45, loss=0.0191, train_acc=0.5955\n",
      "Epoch: 46, loss=0.0191, train_acc=0.5881\n",
      "Epoch: 47, loss=0.0191, train_acc=0.6000\n",
      "Epoch: 48, loss=0.0191, train_acc=0.5896\n",
      "Epoch: 49, loss=0.0191, train_acc=0.5836\n",
      "Epoch: 50, loss=0.0191, train_acc=0.6000\n",
      "Epoch: 51, loss=0.0191, train_acc=0.5836\n",
      "Epoch: 52, loss=0.0191, train_acc=0.5925\n",
      "Epoch: 53, loss=0.0191, train_acc=0.5940\n",
      "Epoch: 54, loss=0.0190, train_acc=0.5866\n",
      "Epoch: 55, loss=0.0191, train_acc=0.5896\n",
      "Epoch: 56, loss=0.0191, train_acc=0.5896\n",
      "Epoch: 57, loss=0.0190, train_acc=0.5866\n",
      "Epoch: 58, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 59, loss=0.0191, train_acc=0.5881\n",
      "Epoch: 60, loss=0.0191, train_acc=0.5821\n",
      "Epoch: 61, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 62, loss=0.0190, train_acc=0.5881\n",
      "Epoch: 63, loss=0.0191, train_acc=0.5881\n",
      "Epoch: 64, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 65, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 66, loss=0.0190, train_acc=0.5896\n",
      "Epoch: 67, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 68, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 69, loss=0.0190, train_acc=0.5866\n",
      "Epoch: 70, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 71, loss=0.0190, train_acc=0.5821\n",
      "Epoch: 72, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 73, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 74, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 75, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 76, loss=0.0190, train_acc=0.5866\n",
      "Epoch: 77, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 78, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 79, loss=0.0190, train_acc=0.5851\n",
      "Epoch: 80, loss=0.0190, train_acc=0.5866\n",
      "Epoch: 81, loss=0.0190, train_acc=0.5851\n",
      "Epoch: 82, loss=0.0190, train_acc=0.5851\n",
      "Epoch: 83, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 84, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 85, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 86, loss=0.0190, train_acc=0.5821\n",
      "Epoch: 87, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 88, loss=0.0190, train_acc=0.5881\n",
      "Epoch: 89, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 90, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 91, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 92, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 93, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 94, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 95, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 96, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 97, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 98, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 99, loss=0.0190, train_acc=0.5881\n",
      "Epoch: 100, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 101, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 102, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 103, loss=0.0190, train_acc=0.5836\n",
      "Epoch: 104, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 105, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 106, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 107, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 108, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 109, loss=0.0190, train_acc=0.5866\n",
      "Epoch: 110, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 111, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 112, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 113, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 114, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 115, loss=0.0190, train_acc=0.5925\n",
      "Epoch: 116, loss=0.0190, train_acc=0.5940\n",
      "Epoch: 117, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 118, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 119, loss=0.0190, train_acc=0.5910\n",
      "Epoch: 120, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 121, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 122, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 123, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 124, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 125, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 126, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 127, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 128, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 129, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 130, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 131, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 132, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 133, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 134, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 135, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 136, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 137, loss=0.0190, train_acc=0.5970\n",
      "Epoch: 138, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 139, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 140, loss=0.0190, train_acc=0.5955\n",
      "Epoch: 141, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 142, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 143, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 144, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 145, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 146, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 147, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 148, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 149, loss=0.0190, train_acc=0.5985\n",
      "Epoch: 150, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 151, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 152, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 153, loss=0.0190, train_acc=0.6000\n",
      "Epoch: 154, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 155, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 156, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 157, loss=0.0190, train_acc=0.6075\n",
      "Epoch: 158, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 159, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 160, loss=0.0190, train_acc=0.6075\n",
      "Epoch: 161, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 162, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 163, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 164, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 165, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 166, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 167, loss=0.0190, train_acc=0.6090\n",
      "Epoch: 168, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 169, loss=0.0190, train_acc=0.6075\n",
      "Epoch: 170, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 171, loss=0.0190, train_acc=0.6045\n",
      "Epoch: 172, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 173, loss=0.0190, train_acc=0.6030\n",
      "Epoch: 174, loss=0.0190, train_acc=0.6015\n",
      "Epoch: 175, loss=0.0190, train_acc=0.6060\n",
      "Epoch: 176, loss=0.0190, train_acc=0.6149\n",
      "Epoch: 177, loss=0.0189, train_acc=0.6060\n",
      "Epoch: 178, loss=0.0189, train_acc=0.6045\n",
      "Epoch: 179, loss=0.0190, train_acc=0.6090\n",
      "Epoch: 180, loss=0.0190, train_acc=0.6090\n",
      "Epoch: 181, loss=0.0189, train_acc=0.6060\n",
      "Epoch: 182, loss=0.0189, train_acc=0.6075\n",
      "Epoch: 183, loss=0.0189, train_acc=0.6090\n",
      "Epoch: 184, loss=0.0189, train_acc=0.6090\n",
      "Epoch: 185, loss=0.0189, train_acc=0.6104\n",
      "Epoch: 186, loss=0.0189, train_acc=0.6090\n",
      "Epoch: 187, loss=0.0189, train_acc=0.6060\n",
      "Epoch: 188, loss=0.0189, train_acc=0.6075\n",
      "Epoch: 189, loss=0.0189, train_acc=0.6119\n",
      "Epoch: 190, loss=0.0189, train_acc=0.6119\n",
      "Epoch: 191, loss=0.0189, train_acc=0.6090\n",
      "Epoch: 192, loss=0.0189, train_acc=0.6119\n",
      "Epoch: 193, loss=0.0189, train_acc=0.6119\n",
      "Epoch: 194, loss=0.0189, train_acc=0.6119\n",
      "Epoch: 195, loss=0.0189, train_acc=0.6149\n",
      "Epoch: 196, loss=0.0189, train_acc=0.6134\n",
      "Epoch: 197, loss=0.0189, train_acc=0.6179\n",
      "Epoch: 198, loss=0.0189, train_acc=0.6149\n",
      "Epoch: 199, loss=0.0189, train_acc=0.6179\n",
      "Epoch: 200, loss=0.0189, train_acc=0.6179\n",
      "Epoch: 201, loss=0.0189, train_acc=0.6194\n",
      "Epoch: 202, loss=0.0189, train_acc=0.6149\n",
      "Epoch: 203, loss=0.0189, train_acc=0.6194\n",
      "Epoch: 204, loss=0.0189, train_acc=0.6149\n",
      "Epoch: 205, loss=0.0189, train_acc=0.6179\n",
      "Epoch: 206, loss=0.0189, train_acc=0.6194\n",
      "Epoch: 207, loss=0.0189, train_acc=0.6194\n",
      "Epoch: 208, loss=0.0189, train_acc=0.6149\n",
      "Epoch: 209, loss=0.0189, train_acc=0.6194\n",
      "Epoch: 210, loss=0.0189, train_acc=0.6179\n",
      "Epoch: 211, loss=0.0189, train_acc=0.6224\n",
      "Epoch: 212, loss=0.0189, train_acc=0.6209\n",
      "Epoch: 213, loss=0.0189, train_acc=0.6224\n",
      "Epoch: 214, loss=0.0188, train_acc=0.6194\n",
      "Epoch: 215, loss=0.0188, train_acc=0.6239\n",
      "Epoch: 216, loss=0.0188, train_acc=0.6239\n",
      "Epoch: 217, loss=0.0188, train_acc=0.6239\n",
      "Epoch: 218, loss=0.0188, train_acc=0.6254\n",
      "Epoch: 219, loss=0.0188, train_acc=0.6254\n",
      "Epoch: 220, loss=0.0188, train_acc=0.6239\n",
      "Epoch: 221, loss=0.0188, train_acc=0.6269\n",
      "Epoch: 222, loss=0.0188, train_acc=0.6284\n",
      "Epoch: 223, loss=0.0188, train_acc=0.6299\n",
      "Epoch: 224, loss=0.0188, train_acc=0.6313\n",
      "Epoch: 225, loss=0.0188, train_acc=0.6313\n",
      "Epoch: 226, loss=0.0188, train_acc=0.6313\n",
      "Epoch: 227, loss=0.0188, train_acc=0.6313\n",
      "Epoch: 228, loss=0.0188, train_acc=0.6313\n",
      "Epoch: 229, loss=0.0187, train_acc=0.6313\n",
      "Epoch: 230, loss=0.0187, train_acc=0.6299\n",
      "Epoch: 231, loss=0.0187, train_acc=0.6328\n",
      "Epoch: 232, loss=0.0187, train_acc=0.6313\n",
      "Epoch: 233, loss=0.0187, train_acc=0.6299\n",
      "Epoch: 234, loss=0.0187, train_acc=0.6313\n",
      "Epoch: 235, loss=0.0187, train_acc=0.6313\n",
      "Epoch: 236, loss=0.0187, train_acc=0.6313\n",
      "Epoch: 237, loss=0.0187, train_acc=0.6313\n",
      "Epoch: 238, loss=0.0186, train_acc=0.6313\n",
      "Epoch: 239, loss=0.0186, train_acc=0.6328\n",
      "Epoch: 240, loss=0.0186, train_acc=0.6328\n",
      "Epoch: 241, loss=0.0186, train_acc=0.6373\n",
      "Epoch: 242, loss=0.0186, train_acc=0.6373\n",
      "Epoch: 243, loss=0.0186, train_acc=0.6373\n",
      "Epoch: 244, loss=0.0186, train_acc=0.6403\n",
      "Epoch: 245, loss=0.0186, train_acc=0.6448\n",
      "Epoch: 246, loss=0.0185, train_acc=0.6478\n",
      "Epoch: 247, loss=0.0185, train_acc=0.6552\n",
      "Epoch: 248, loss=0.0185, train_acc=0.6567\n",
      "Epoch: 249, loss=0.0185, train_acc=0.6657\n",
      "Epoch: 250, loss=0.0185, train_acc=0.6657\n",
      "Epoch: 251, loss=0.0184, train_acc=0.6687\n",
      "Epoch: 252, loss=0.0184, train_acc=0.6761\n",
      "Epoch: 253, loss=0.0184, train_acc=0.6896\n",
      "Epoch: 254, loss=0.0184, train_acc=0.6955\n",
      "Epoch: 255, loss=0.0184, train_acc=0.7030\n",
      "Epoch: 256, loss=0.0183, train_acc=0.7104\n",
      "Epoch: 257, loss=0.0183, train_acc=0.7149\n",
      "Epoch: 258, loss=0.0183, train_acc=0.7209\n",
      "Epoch: 259, loss=0.0183, train_acc=0.7194\n",
      "Epoch: 260, loss=0.0183, train_acc=0.7284\n",
      "Epoch: 261, loss=0.0182, train_acc=0.7313\n",
      "Epoch: 262, loss=0.0182, train_acc=0.7373\n",
      "Epoch: 263, loss=0.0182, train_acc=0.7388\n",
      "Epoch: 264, loss=0.0182, train_acc=0.7418\n",
      "Epoch: 265, loss=0.0181, train_acc=0.7433\n",
      "Epoch: 266, loss=0.0181, train_acc=0.7463\n",
      "Epoch: 267, loss=0.0181, train_acc=0.7478\n",
      "Epoch: 268, loss=0.0180, train_acc=0.7507\n",
      "Epoch: 269, loss=0.0180, train_acc=0.7567\n",
      "Epoch: 270, loss=0.0180, train_acc=0.7567\n",
      "Epoch: 271, loss=0.0180, train_acc=0.7597\n",
      "Epoch: 272, loss=0.0179, train_acc=0.7612\n",
      "Epoch: 273, loss=0.0179, train_acc=0.7642\n",
      "Epoch: 274, loss=0.0179, train_acc=0.7642\n",
      "Epoch: 275, loss=0.0178, train_acc=0.7642\n",
      "Epoch: 276, loss=0.0178, train_acc=0.7672\n",
      "Epoch: 277, loss=0.0178, train_acc=0.7731\n",
      "Epoch: 278, loss=0.0178, train_acc=0.7731\n",
      "Epoch: 279, loss=0.0177, train_acc=0.7746\n",
      "Epoch: 280, loss=0.0177, train_acc=0.7761\n",
      "Epoch: 281, loss=0.0177, train_acc=0.7761\n",
      "Epoch: 282, loss=0.0176, train_acc=0.7776\n",
      "Epoch: 283, loss=0.0176, train_acc=0.7821\n",
      "Epoch: 284, loss=0.0176, train_acc=0.7791\n",
      "Epoch: 285, loss=0.0175, train_acc=0.7821\n",
      "Epoch: 286, loss=0.0175, train_acc=0.7836\n",
      "Epoch: 287, loss=0.0175, train_acc=0.7851\n",
      "Epoch: 288, loss=0.0175, train_acc=0.7881\n",
      "Epoch: 289, loss=0.0174, train_acc=0.7940\n",
      "Epoch: 290, loss=0.0174, train_acc=0.7925\n",
      "Epoch: 291, loss=0.0174, train_acc=0.7940\n",
      "Epoch: 292, loss=0.0173, train_acc=0.7955\n",
      "Epoch: 293, loss=0.0173, train_acc=0.7985\n",
      "Epoch: 294, loss=0.0173, train_acc=0.7985\n",
      "Epoch: 295, loss=0.0172, train_acc=0.7970\n",
      "Epoch: 296, loss=0.0172, train_acc=0.7970\n",
      "Epoch: 297, loss=0.0172, train_acc=0.8030\n",
      "Epoch: 298, loss=0.0172, train_acc=0.8045\n",
      "Epoch: 299, loss=0.0171, train_acc=0.8060\n",
      "Epoch: 300, loss=0.0171, train_acc=0.8060\n",
      "Epoch: 301, loss=0.0171, train_acc=0.8090\n",
      "Epoch: 302, loss=0.0171, train_acc=0.8104\n",
      "Epoch: 303, loss=0.0170, train_acc=0.8104\n",
      "Epoch: 304, loss=0.0170, train_acc=0.8104\n",
      "Epoch: 305, loss=0.0170, train_acc=0.8119\n",
      "Epoch: 306, loss=0.0169, train_acc=0.8119\n",
      "Epoch: 307, loss=0.0169, train_acc=0.8119\n",
      "Epoch: 308, loss=0.0169, train_acc=0.8119\n",
      "Epoch: 309, loss=0.0169, train_acc=0.8119\n",
      "Epoch: 310, loss=0.0168, train_acc=0.8134\n",
      "Epoch: 311, loss=0.0168, train_acc=0.8134\n",
      "Epoch: 312, loss=0.0168, train_acc=0.8164\n",
      "Epoch: 313, loss=0.0168, train_acc=0.8179\n",
      "Epoch: 314, loss=0.0167, train_acc=0.8179\n",
      "Epoch: 315, loss=0.0167, train_acc=0.8224\n",
      "Epoch: 316, loss=0.0167, train_acc=0.8224\n",
      "Epoch: 317, loss=0.0167, train_acc=0.8224\n",
      "Epoch: 318, loss=0.0166, train_acc=0.8254\n",
      "Epoch: 319, loss=0.0166, train_acc=0.8224\n",
      "Epoch: 320, loss=0.0166, train_acc=0.8284\n",
      "Epoch: 321, loss=0.0166, train_acc=0.8299\n",
      "Epoch: 322, loss=0.0165, train_acc=0.8284\n",
      "Epoch: 323, loss=0.0165, train_acc=0.8313\n",
      "Epoch: 324, loss=0.0165, train_acc=0.8284\n",
      "Epoch: 325, loss=0.0165, train_acc=0.8343\n",
      "Epoch: 326, loss=0.0164, train_acc=0.8343\n",
      "Epoch: 327, loss=0.0164, train_acc=0.8343\n",
      "Epoch: 328, loss=0.0164, train_acc=0.8358\n",
      "Epoch: 329, loss=0.0164, train_acc=0.8403\n",
      "Epoch: 330, loss=0.0164, train_acc=0.8343\n",
      "Epoch: 331, loss=0.0163, train_acc=0.8403\n",
      "Epoch: 332, loss=0.0163, train_acc=0.8403\n",
      "Epoch: 333, loss=0.0163, train_acc=0.8403\n",
      "Epoch: 334, loss=0.0163, train_acc=0.8403\n",
      "Epoch: 335, loss=0.0163, train_acc=0.8403\n",
      "Epoch: 336, loss=0.0162, train_acc=0.8433\n",
      "Epoch: 337, loss=0.0162, train_acc=0.8418\n",
      "Epoch: 338, loss=0.0162, train_acc=0.8403\n",
      "Epoch: 339, loss=0.0162, train_acc=0.8418\n",
      "Epoch: 340, loss=0.0162, train_acc=0.8403\n",
      "Epoch: 341, loss=0.0161, train_acc=0.8448\n",
      "Epoch: 342, loss=0.0161, train_acc=0.8493\n",
      "Epoch: 343, loss=0.0161, train_acc=0.8478\n",
      "Epoch: 344, loss=0.0161, train_acc=0.8493\n",
      "Epoch: 345, loss=0.0161, train_acc=0.8493\n",
      "Epoch: 346, loss=0.0160, train_acc=0.8493\n",
      "Epoch: 347, loss=0.0160, train_acc=0.8478\n",
      "Epoch: 348, loss=0.0160, train_acc=0.8493\n",
      "Epoch: 349, loss=0.0160, train_acc=0.8507\n",
      "Epoch: 350, loss=0.0160, train_acc=0.8493\n",
      "Epoch: 351, loss=0.0160, train_acc=0.8478\n",
      "Epoch: 352, loss=0.0159, train_acc=0.8493\n",
      "Epoch: 353, loss=0.0159, train_acc=0.8507\n",
      "Epoch: 354, loss=0.0159, train_acc=0.8493\n",
      "Epoch: 355, loss=0.0159, train_acc=0.8493\n",
      "Epoch: 356, loss=0.0159, train_acc=0.8522\n",
      "Epoch: 357, loss=0.0159, train_acc=0.8507\n",
      "Epoch: 358, loss=0.0158, train_acc=0.8537\n",
      "Epoch: 359, loss=0.0158, train_acc=0.8522\n",
      "Epoch: 360, loss=0.0158, train_acc=0.8522\n",
      "Epoch: 361, loss=0.0158, train_acc=0.8507\n",
      "Epoch: 362, loss=0.0158, train_acc=0.8537\n",
      "Epoch: 363, loss=0.0158, train_acc=0.8537\n",
      "Epoch: 364, loss=0.0158, train_acc=0.8537\n",
      "Epoch: 365, loss=0.0157, train_acc=0.8537\n",
      "Epoch: 366, loss=0.0157, train_acc=0.8507\n",
      "Epoch: 367, loss=0.0157, train_acc=0.8552\n",
      "Epoch: 368, loss=0.0157, train_acc=0.8537\n",
      "Epoch: 369, loss=0.0157, train_acc=0.8567\n",
      "Epoch: 370, loss=0.0157, train_acc=0.8522\n",
      "Epoch: 371, loss=0.0157, train_acc=0.8537\n",
      "Epoch: 372, loss=0.0156, train_acc=0.8537\n",
      "Epoch: 373, loss=0.0156, train_acc=0.8567\n",
      "Epoch: 374, loss=0.0156, train_acc=0.8582\n",
      "Epoch: 375, loss=0.0156, train_acc=0.8567\n",
      "Epoch: 376, loss=0.0156, train_acc=0.8552\n",
      "Epoch: 377, loss=0.0156, train_acc=0.8582\n",
      "Epoch: 378, loss=0.0156, train_acc=0.8582\n",
      "Epoch: 379, loss=0.0156, train_acc=0.8552\n",
      "Epoch: 380, loss=0.0155, train_acc=0.8582\n",
      "Epoch: 381, loss=0.0155, train_acc=0.8537\n",
      "Epoch: 382, loss=0.0155, train_acc=0.8582\n",
      "Epoch: 383, loss=0.0155, train_acc=0.8582\n",
      "Epoch: 384, loss=0.0155, train_acc=0.8582\n",
      "Epoch: 385, loss=0.0155, train_acc=0.8612\n",
      "Epoch: 386, loss=0.0155, train_acc=0.8567\n",
      "Epoch: 387, loss=0.0155, train_acc=0.8582\n",
      "Epoch: 388, loss=0.0155, train_acc=0.8582\n",
      "Epoch: 389, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 390, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 391, loss=0.0154, train_acc=0.8582\n",
      "Epoch: 392, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 393, loss=0.0154, train_acc=0.8612\n",
      "Epoch: 394, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 395, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 396, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 397, loss=0.0154, train_acc=0.8582\n",
      "Epoch: 398, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 399, loss=0.0154, train_acc=0.8597\n",
      "Epoch: 400, loss=0.0153, train_acc=0.8612\n",
      "Epoch: 401, loss=0.0153, train_acc=0.8597\n",
      "Epoch: 402, loss=0.0153, train_acc=0.8627\n",
      "Epoch: 403, loss=0.0153, train_acc=0.8612\n",
      "Epoch: 404, loss=0.0153, train_acc=0.8597\n",
      "Epoch: 405, loss=0.0153, train_acc=0.8612\n",
      "Epoch: 406, loss=0.0153, train_acc=0.8597\n",
      "Epoch: 407, loss=0.0153, train_acc=0.8627\n",
      "Epoch: 408, loss=0.0153, train_acc=0.8612\n",
      "Epoch: 409, loss=0.0153, train_acc=0.8612\n",
      "Epoch: 410, loss=0.0153, train_acc=0.8627\n",
      "Epoch: 411, loss=0.0152, train_acc=0.8642\n",
      "Epoch: 412, loss=0.0152, train_acc=0.8627\n",
      "Epoch: 413, loss=0.0152, train_acc=0.8642\n",
      "Epoch: 414, loss=0.0152, train_acc=0.8657\n",
      "Epoch: 415, loss=0.0152, train_acc=0.8612\n",
      "Epoch: 416, loss=0.0152, train_acc=0.8657\n",
      "Epoch: 417, loss=0.0152, train_acc=0.8672\n",
      "Epoch: 418, loss=0.0152, train_acc=0.8642\n",
      "Epoch: 419, loss=0.0152, train_acc=0.8657\n",
      "Epoch: 420, loss=0.0152, train_acc=0.8627\n",
      "Epoch: 421, loss=0.0152, train_acc=0.8642\n",
      "Epoch: 422, loss=0.0152, train_acc=0.8657\n",
      "Epoch: 423, loss=0.0152, train_acc=0.8642\n",
      "Epoch: 424, loss=0.0152, train_acc=0.8657\n",
      "Epoch: 425, loss=0.0152, train_acc=0.8627\n",
      "Epoch: 426, loss=0.0151, train_acc=0.8627\n",
      "Epoch: 427, loss=0.0151, train_acc=0.8642\n",
      "Epoch: 428, loss=0.0151, train_acc=0.8672\n",
      "Epoch: 429, loss=0.0151, train_acc=0.8627\n",
      "Epoch: 430, loss=0.0151, train_acc=0.8657\n",
      "Epoch: 431, loss=0.0151, train_acc=0.8701\n",
      "Epoch: 432, loss=0.0151, train_acc=0.8716\n",
      "Epoch: 433, loss=0.0151, train_acc=0.8657\n",
      "Epoch: 434, loss=0.0151, train_acc=0.8657\n",
      "Epoch: 435, loss=0.0151, train_acc=0.8701\n",
      "Epoch: 436, loss=0.0151, train_acc=0.8672\n",
      "Epoch: 437, loss=0.0151, train_acc=0.8672\n",
      "Epoch: 438, loss=0.0151, train_acc=0.8687\n",
      "Epoch: 439, loss=0.0151, train_acc=0.8672\n",
      "Epoch: 440, loss=0.0150, train_acc=0.8657\n",
      "Epoch: 441, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 442, loss=0.0150, train_acc=0.8701\n",
      "Epoch: 443, loss=0.0150, train_acc=0.8657\n",
      "Epoch: 444, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 445, loss=0.0150, train_acc=0.8672\n",
      "Epoch: 446, loss=0.0150, train_acc=0.8701\n",
      "Epoch: 447, loss=0.0150, train_acc=0.8701\n",
      "Epoch: 448, loss=0.0150, train_acc=0.8672\n",
      "Epoch: 449, loss=0.0150, train_acc=0.8701\n",
      "Epoch: 450, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 451, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 452, loss=0.0150, train_acc=0.8672\n",
      "Epoch: 453, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 454, loss=0.0150, train_acc=0.8672\n",
      "Epoch: 455, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 456, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 457, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 458, loss=0.0150, train_acc=0.8687\n",
      "Epoch: 459, loss=0.0150, train_acc=0.8672\n",
      "Epoch: 460, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 461, loss=0.0149, train_acc=0.8701\n",
      "Epoch: 462, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 463, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 464, loss=0.0149, train_acc=0.8701\n",
      "Epoch: 465, loss=0.0149, train_acc=0.8701\n",
      "Epoch: 466, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 467, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 468, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 469, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 470, loss=0.0149, train_acc=0.8672\n",
      "Epoch: 471, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 472, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 473, loss=0.0149, train_acc=0.8672\n",
      "Epoch: 474, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 475, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 476, loss=0.0149, train_acc=0.8701\n",
      "Epoch: 477, loss=0.0149, train_acc=0.8672\n",
      "Epoch: 478, loss=0.0149, train_acc=0.8687\n",
      "Epoch: 479, loss=0.0149, train_acc=0.8687\n",
      "\n",
      " My model: Classification report of test set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       165\n",
      "           1       0.85      0.86      0.85       165\n",
      "\n",
      "    accuracy                           0.85       330\n",
      "   macro avg       0.85      0.85      0.85       330\n",
      "weighted avg       0.85      0.85      0.85       330\n",
      "\n",
      "\n",
      "Scikit-learn MLPClassifier Info:\n",
      "Number of layers: 3\n",
      "Number of neurons in each layer: (100,)\n",
      "Number of output classes: 1\n",
      "Activation function: relu\n",
      "Solver: adam\n",
      "Learning rate: constant\n",
      "Initial learning rate: 0.001\n",
      "Batch size: auto\n",
      "Maximum number of iterations: 200\n",
      "\n",
      " Sklearn model: Classification report of test set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85       165\n",
      "           1       0.84      0.85      0.85       165\n",
      "\n",
      "    accuracy                           0.85       330\n",
      "   macro avg       0.85      0.85      0.85       330\n",
      "weighted avg       0.85      0.85      0.85       330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funny/miniconda3/envs/pytorch-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = run_my_mlp_for_binary_classification3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
