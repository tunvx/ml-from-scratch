{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit recognition problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import nn\n",
    "from optim import SGDOptimizer\n",
    "\n",
    "from supervised_learning import MyMLPClassifier\n",
    "from dataset.load_data import sklearn_to_df, prepare_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = sklearn_to_df(load_digits())\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_te = scaler.transform(X_te)\n",
    "\n",
    "    print (\"train_x's shape: \" + str(X_tr.shape))\n",
    "    print (\"test_x's shape: \" + str(X_te.shape))\n",
    "    print (\"train_y's shape: \" + str(y_tr.shape))\n",
    "    print (\"test_y's shape: \" + str(y_te.shape))\n",
    "    print()\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=0.2, regularization=0.015)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_classes):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[200, 100], n_output=n_classes, activation='relu')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    n_in, n_class = X_tr.shape[1], 10\n",
    "\n",
    "    num_epochs = 250\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_class)\n",
    "    optimizer, loss_func = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the data loader for training data\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        \n",
    "        # Initialize counters for tracking training progress\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # forward pass: compute logits and loss\n",
    "            batch_logit = model.forward(batch_X)        # output model: logit\n",
    "            loss = loss_func.forward(batch_logit, batch_y)\n",
    "\n",
    "            # backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = loss_func.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            batch_yp = np.argmax(batch_logit, axis=1)   # logit --> label\n",
    "            total_correct += np.sum(batch_yp == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = np.argmax(model.forward(X_te), axis=1)\n",
    "    print(\"\\nClassification report of my MLP model:\\n\", classification_report(y_te, ypred))\n",
    "\n",
    "    \n",
    "    skmodel = MLPClassifier()\n",
    "    skmodel.fit(X_tr, y_tr)\n",
    "\n",
    "     # Print information about the trained scikit-learn MLPClassifier\n",
    "    print(\"\\nScikit-learn MLPClassifier Info:\")\n",
    "    print(\"Number of layers:\", skmodel.n_layers_)\n",
    "    print(\"Number of neurons in each layer:\", skmodel.hidden_layer_sizes)\n",
    "    print(\"Number of output classes:\", skmodel.n_outputs_)\n",
    "    print(\"Activation function:\", skmodel.activation)\n",
    "    print(\"Solver:\", skmodel.solver)\n",
    "    print(\"Learning rate:\", skmodel.learning_rate)\n",
    "    print(\"Initial learning rate:\", skmodel.learning_rate_init)\n",
    "    print(\"Batch size:\", skmodel.batch_size)\n",
    "    print(\"Maximum number of iterations:\", skmodel.max_iter)\n",
    "    # Add more model-specific information as needed\n",
    "\n",
    "    ypred = skmodel.predict(X_te)\n",
    "    print(\"\\nClassification report of sklearn's model:\\n\", classification_report(y_te, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (1203, 64)\n",
      "test_x's shape: (594, 64)\n",
      "train_y's shape: (1203,)\n",
      "test_y's shape: (594,)\n",
      "\n",
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=64, out_features=200, bias=True)\n",
      "(relu): ReLU()\n",
      "(linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "(relu): ReLU()\n",
      "(linear): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 0, loss=1.9851, train_acc=0.4613\n",
      "Epoch: 1, loss=1.6108, train_acc=0.7473\n",
      "Epoch: 2, loss=1.3524, train_acc=0.8113\n",
      "Epoch: 3, loss=1.1574, train_acc=0.8429\n",
      "Epoch: 4, loss=1.0099, train_acc=0.8687\n",
      "Epoch: 5, loss=0.8985, train_acc=0.8861\n",
      "Epoch: 6, loss=0.8132, train_acc=0.8961\n",
      "Epoch: 7, loss=0.7466, train_acc=0.8994\n",
      "Epoch: 8, loss=0.6935, train_acc=0.9027\n",
      "Epoch: 9, loss=0.6502, train_acc=0.9044\n",
      "Epoch: 10, loss=0.6143, train_acc=0.9094\n",
      "Epoch: 11, loss=0.5841, train_acc=0.9135\n",
      "Epoch: 12, loss=0.5582, train_acc=0.9169\n",
      "Epoch: 13, loss=0.5358, train_acc=0.9185\n",
      "Epoch: 14, loss=0.5162, train_acc=0.9235\n",
      "Epoch: 15, loss=0.4989, train_acc=0.9260\n",
      "Epoch: 16, loss=0.4836, train_acc=0.9268\n",
      "Epoch: 17, loss=0.4698, train_acc=0.9277\n",
      "Epoch: 18, loss=0.4574, train_acc=0.9293\n",
      "Epoch: 19, loss=0.4461, train_acc=0.9318\n",
      "Epoch: 20, loss=0.4358, train_acc=0.9327\n",
      "Epoch: 21, loss=0.4264, train_acc=0.9335\n",
      "Epoch: 22, loss=0.4177, train_acc=0.9343\n",
      "Epoch: 23, loss=0.4097, train_acc=0.9377\n",
      "Epoch: 24, loss=0.4023, train_acc=0.9377\n",
      "Epoch: 25, loss=0.3954, train_acc=0.9385\n",
      "Epoch: 26, loss=0.3889, train_acc=0.9393\n",
      "Epoch: 27, loss=0.3829, train_acc=0.9393\n",
      "Epoch: 28, loss=0.3772, train_acc=0.9401\n",
      "Epoch: 29, loss=0.3719, train_acc=0.9410\n",
      "Epoch: 30, loss=0.3669, train_acc=0.9418\n",
      "Epoch: 31, loss=0.3622, train_acc=0.9418\n",
      "Epoch: 32, loss=0.3577, train_acc=0.9418\n",
      "Epoch: 33, loss=0.3535, train_acc=0.9418\n",
      "Epoch: 34, loss=0.3495, train_acc=0.9418\n",
      "Epoch: 35, loss=0.3457, train_acc=0.9418\n",
      "Epoch: 36, loss=0.3421, train_acc=0.9418\n",
      "Epoch: 37, loss=0.3387, train_acc=0.9426\n",
      "Epoch: 38, loss=0.3354, train_acc=0.9435\n",
      "Epoch: 39, loss=0.3323, train_acc=0.9451\n",
      "Epoch: 40, loss=0.3293, train_acc=0.9468\n",
      "Epoch: 41, loss=0.3264, train_acc=0.9476\n",
      "Epoch: 42, loss=0.3237, train_acc=0.9476\n",
      "Epoch: 43, loss=0.3211, train_acc=0.9476\n",
      "Epoch: 44, loss=0.3185, train_acc=0.9485\n",
      "Epoch: 45, loss=0.3161, train_acc=0.9485\n",
      "Epoch: 46, loss=0.3138, train_acc=0.9485\n",
      "Epoch: 47, loss=0.3115, train_acc=0.9485\n",
      "Epoch: 48, loss=0.3094, train_acc=0.9493\n",
      "Epoch: 49, loss=0.3073, train_acc=0.9493\n",
      "Epoch: 50, loss=0.3053, train_acc=0.9501\n",
      "Epoch: 51, loss=0.3033, train_acc=0.9501\n",
      "Epoch: 52, loss=0.3015, train_acc=0.9510\n",
      "Epoch: 53, loss=0.2996, train_acc=0.9510\n",
      "Epoch: 54, loss=0.2979, train_acc=0.9510\n",
      "Epoch: 55, loss=0.2962, train_acc=0.9510\n",
      "Epoch: 56, loss=0.2946, train_acc=0.9510\n",
      "Epoch: 57, loss=0.2930, train_acc=0.9510\n",
      "Epoch: 58, loss=0.2914, train_acc=0.9518\n",
      "Epoch: 59, loss=0.2899, train_acc=0.9518\n",
      "Epoch: 60, loss=0.2885, train_acc=0.9526\n",
      "Epoch: 61, loss=0.2871, train_acc=0.9534\n",
      "Epoch: 62, loss=0.2857, train_acc=0.9534\n",
      "Epoch: 63, loss=0.2844, train_acc=0.9534\n",
      "Epoch: 64, loss=0.2831, train_acc=0.9534\n",
      "Epoch: 65, loss=0.2819, train_acc=0.9534\n",
      "Epoch: 66, loss=0.2806, train_acc=0.9534\n",
      "Epoch: 67, loss=0.2795, train_acc=0.9534\n",
      "Epoch: 68, loss=0.2783, train_acc=0.9534\n",
      "Epoch: 69, loss=0.2772, train_acc=0.9526\n",
      "Epoch: 70, loss=0.2761, train_acc=0.9526\n",
      "Epoch: 71, loss=0.2750, train_acc=0.9526\n",
      "Epoch: 72, loss=0.2740, train_acc=0.9526\n",
      "Epoch: 73, loss=0.2730, train_acc=0.9526\n",
      "Epoch: 74, loss=0.2720, train_acc=0.9526\n",
      "Epoch: 75, loss=0.2710, train_acc=0.9526\n",
      "Epoch: 76, loss=0.2701, train_acc=0.9534\n",
      "Epoch: 77, loss=0.2692, train_acc=0.9543\n",
      "Epoch: 78, loss=0.2683, train_acc=0.9543\n",
      "Epoch: 79, loss=0.2674, train_acc=0.9551\n",
      "Epoch: 80, loss=0.2665, train_acc=0.9551\n",
      "Epoch: 81, loss=0.2657, train_acc=0.9551\n",
      "Epoch: 82, loss=0.2649, train_acc=0.9559\n",
      "Epoch: 83, loss=0.2641, train_acc=0.9568\n",
      "Epoch: 84, loss=0.2633, train_acc=0.9568\n",
      "Epoch: 85, loss=0.2625, train_acc=0.9576\n",
      "Epoch: 86, loss=0.2618, train_acc=0.9576\n",
      "Epoch: 87, loss=0.2610, train_acc=0.9576\n",
      "Epoch: 88, loss=0.2603, train_acc=0.9576\n",
      "Epoch: 89, loss=0.2596, train_acc=0.9576\n",
      "Epoch: 90, loss=0.2589, train_acc=0.9576\n",
      "Epoch: 91, loss=0.2582, train_acc=0.9576\n",
      "Epoch: 92, loss=0.2576, train_acc=0.9568\n",
      "Epoch: 93, loss=0.2569, train_acc=0.9568\n",
      "Epoch: 94, loss=0.2563, train_acc=0.9568\n",
      "Epoch: 95, loss=0.2556, train_acc=0.9568\n",
      "Epoch: 96, loss=0.2550, train_acc=0.9576\n",
      "Epoch: 97, loss=0.2544, train_acc=0.9568\n",
      "Epoch: 98, loss=0.2538, train_acc=0.9568\n",
      "Epoch: 99, loss=0.2533, train_acc=0.9584\n",
      "Epoch: 100, loss=0.2527, train_acc=0.9584\n",
      "Epoch: 101, loss=0.2521, train_acc=0.9584\n",
      "Epoch: 102, loss=0.2516, train_acc=0.9584\n",
      "Epoch: 103, loss=0.2510, train_acc=0.9584\n",
      "Epoch: 104, loss=0.2505, train_acc=0.9593\n",
      "Epoch: 105, loss=0.2500, train_acc=0.9593\n",
      "Epoch: 106, loss=0.2495, train_acc=0.9593\n",
      "Epoch: 107, loss=0.2490, train_acc=0.9593\n",
      "Epoch: 108, loss=0.2485, train_acc=0.9593\n",
      "Epoch: 109, loss=0.2480, train_acc=0.9593\n",
      "Epoch: 110, loss=0.2475, train_acc=0.9601\n",
      "Epoch: 111, loss=0.2470, train_acc=0.9601\n",
      "Epoch: 112, loss=0.2465, train_acc=0.9601\n",
      "Epoch: 113, loss=0.2461, train_acc=0.9601\n",
      "Epoch: 114, loss=0.2456, train_acc=0.9601\n",
      "Epoch: 115, loss=0.2452, train_acc=0.9601\n",
      "Epoch: 116, loss=0.2448, train_acc=0.9601\n",
      "Epoch: 117, loss=0.2443, train_acc=0.9601\n",
      "Epoch: 118, loss=0.2439, train_acc=0.9609\n",
      "Epoch: 119, loss=0.2435, train_acc=0.9609\n",
      "Epoch: 120, loss=0.2431, train_acc=0.9601\n",
      "Epoch: 121, loss=0.2427, train_acc=0.9601\n",
      "Epoch: 122, loss=0.2423, train_acc=0.9601\n",
      "Epoch: 123, loss=0.2419, train_acc=0.9601\n",
      "Epoch: 124, loss=0.2415, train_acc=0.9601\n",
      "Epoch: 125, loss=0.2411, train_acc=0.9601\n",
      "Epoch: 126, loss=0.2408, train_acc=0.9601\n",
      "Epoch: 127, loss=0.2404, train_acc=0.9601\n",
      "Epoch: 128, loss=0.2400, train_acc=0.9601\n",
      "Epoch: 129, loss=0.2397, train_acc=0.9601\n",
      "Epoch: 130, loss=0.2393, train_acc=0.9601\n",
      "Epoch: 131, loss=0.2390, train_acc=0.9601\n",
      "Epoch: 132, loss=0.2386, train_acc=0.9601\n",
      "Epoch: 133, loss=0.2383, train_acc=0.9601\n",
      "Epoch: 134, loss=0.2380, train_acc=0.9601\n",
      "Epoch: 135, loss=0.2376, train_acc=0.9601\n",
      "Epoch: 136, loss=0.2373, train_acc=0.9601\n",
      "Epoch: 137, loss=0.2370, train_acc=0.9601\n",
      "Epoch: 138, loss=0.2367, train_acc=0.9618\n",
      "Epoch: 139, loss=0.2364, train_acc=0.9618\n",
      "Epoch: 140, loss=0.2361, train_acc=0.9618\n",
      "Epoch: 141, loss=0.2358, train_acc=0.9618\n",
      "Epoch: 142, loss=0.2355, train_acc=0.9618\n",
      "Epoch: 143, loss=0.2352, train_acc=0.9618\n",
      "Epoch: 144, loss=0.2349, train_acc=0.9618\n",
      "Epoch: 145, loss=0.2346, train_acc=0.9618\n",
      "Epoch: 146, loss=0.2343, train_acc=0.9618\n",
      "Epoch: 147, loss=0.2340, train_acc=0.9618\n",
      "Epoch: 148, loss=0.2337, train_acc=0.9626\n",
      "Epoch: 149, loss=0.2335, train_acc=0.9634\n",
      "Epoch: 150, loss=0.2332, train_acc=0.9634\n",
      "Epoch: 151, loss=0.2329, train_acc=0.9634\n",
      "Epoch: 152, loss=0.2327, train_acc=0.9634\n",
      "Epoch: 153, loss=0.2324, train_acc=0.9634\n",
      "Epoch: 154, loss=0.2322, train_acc=0.9634\n",
      "Epoch: 155, loss=0.2319, train_acc=0.9634\n",
      "Epoch: 156, loss=0.2316, train_acc=0.9634\n",
      "Epoch: 157, loss=0.2314, train_acc=0.9634\n",
      "Epoch: 158, loss=0.2312, train_acc=0.9634\n",
      "Epoch: 159, loss=0.2309, train_acc=0.9634\n",
      "Epoch: 160, loss=0.2307, train_acc=0.9634\n",
      "Epoch: 161, loss=0.2304, train_acc=0.9643\n",
      "Epoch: 162, loss=0.2302, train_acc=0.9651\n",
      "Epoch: 163, loss=0.2300, train_acc=0.9659\n",
      "Epoch: 164, loss=0.2298, train_acc=0.9659\n",
      "Epoch: 165, loss=0.2295, train_acc=0.9659\n",
      "Epoch: 166, loss=0.2293, train_acc=0.9659\n",
      "Epoch: 167, loss=0.2291, train_acc=0.9659\n",
      "Epoch: 168, loss=0.2289, train_acc=0.9659\n",
      "Epoch: 169, loss=0.2287, train_acc=0.9659\n",
      "Epoch: 170, loss=0.2284, train_acc=0.9659\n",
      "Epoch: 171, loss=0.2282, train_acc=0.9659\n",
      "Epoch: 172, loss=0.2280, train_acc=0.9659\n",
      "Epoch: 173, loss=0.2278, train_acc=0.9667\n",
      "Epoch: 174, loss=0.2276, train_acc=0.9667\n",
      "Epoch: 175, loss=0.2274, train_acc=0.9667\n",
      "Epoch: 176, loss=0.2272, train_acc=0.9667\n",
      "Epoch: 177, loss=0.2270, train_acc=0.9667\n",
      "Epoch: 178, loss=0.2268, train_acc=0.9667\n",
      "Epoch: 179, loss=0.2266, train_acc=0.9667\n",
      "Epoch: 180, loss=0.2265, train_acc=0.9667\n",
      "Epoch: 181, loss=0.2263, train_acc=0.9667\n",
      "Epoch: 182, loss=0.2261, train_acc=0.9667\n",
      "Epoch: 183, loss=0.2259, train_acc=0.9667\n",
      "Epoch: 184, loss=0.2257, train_acc=0.9667\n",
      "Epoch: 185, loss=0.2255, train_acc=0.9667\n",
      "Epoch: 186, loss=0.2254, train_acc=0.9667\n",
      "Epoch: 187, loss=0.2252, train_acc=0.9667\n",
      "Epoch: 188, loss=0.2250, train_acc=0.9667\n",
      "Epoch: 189, loss=0.2248, train_acc=0.9667\n",
      "Epoch: 190, loss=0.2247, train_acc=0.9667\n",
      "Epoch: 191, loss=0.2245, train_acc=0.9667\n",
      "Epoch: 192, loss=0.2243, train_acc=0.9667\n",
      "Epoch: 193, loss=0.2242, train_acc=0.9667\n",
      "Epoch: 194, loss=0.2240, train_acc=0.9667\n",
      "Epoch: 195, loss=0.2239, train_acc=0.9667\n",
      "Epoch: 196, loss=0.2237, train_acc=0.9667\n",
      "Epoch: 197, loss=0.2235, train_acc=0.9667\n",
      "Epoch: 198, loss=0.2234, train_acc=0.9667\n",
      "Epoch: 199, loss=0.2232, train_acc=0.9667\n",
      "Epoch: 200, loss=0.2231, train_acc=0.9667\n",
      "Epoch: 201, loss=0.2229, train_acc=0.9667\n",
      "Epoch: 202, loss=0.2228, train_acc=0.9667\n",
      "Epoch: 203, loss=0.2226, train_acc=0.9667\n",
      "Epoch: 204, loss=0.2225, train_acc=0.9667\n",
      "Epoch: 205, loss=0.2223, train_acc=0.9667\n",
      "Epoch: 206, loss=0.2222, train_acc=0.9667\n",
      "Epoch: 207, loss=0.2220, train_acc=0.9667\n",
      "Epoch: 208, loss=0.2219, train_acc=0.9667\n",
      "Epoch: 209, loss=0.2217, train_acc=0.9667\n",
      "Epoch: 210, loss=0.2216, train_acc=0.9667\n",
      "Epoch: 211, loss=0.2215, train_acc=0.9667\n",
      "Epoch: 212, loss=0.2213, train_acc=0.9667\n",
      "Epoch: 213, loss=0.2212, train_acc=0.9667\n",
      "Epoch: 214, loss=0.2211, train_acc=0.9667\n",
      "Epoch: 215, loss=0.2209, train_acc=0.9667\n",
      "Epoch: 216, loss=0.2208, train_acc=0.9667\n",
      "Epoch: 217, loss=0.2207, train_acc=0.9667\n",
      "Epoch: 218, loss=0.2205, train_acc=0.9667\n",
      "Epoch: 219, loss=0.2204, train_acc=0.9667\n",
      "Epoch: 220, loss=0.2203, train_acc=0.9667\n",
      "Epoch: 221, loss=0.2201, train_acc=0.9667\n",
      "Epoch: 222, loss=0.2200, train_acc=0.9667\n",
      "Epoch: 223, loss=0.2199, train_acc=0.9676\n",
      "Epoch: 224, loss=0.2198, train_acc=0.9676\n",
      "Epoch: 225, loss=0.2196, train_acc=0.9676\n",
      "Epoch: 226, loss=0.2195, train_acc=0.9676\n",
      "Epoch: 227, loss=0.2194, train_acc=0.9676\n",
      "Epoch: 228, loss=0.2193, train_acc=0.9676\n",
      "Epoch: 229, loss=0.2192, train_acc=0.9676\n",
      "Epoch: 230, loss=0.2190, train_acc=0.9676\n",
      "Epoch: 231, loss=0.2189, train_acc=0.9684\n",
      "Epoch: 232, loss=0.2188, train_acc=0.9684\n",
      "Epoch: 233, loss=0.2187, train_acc=0.9684\n",
      "Epoch: 234, loss=0.2186, train_acc=0.9684\n",
      "Epoch: 235, loss=0.2185, train_acc=0.9684\n",
      "Epoch: 236, loss=0.2184, train_acc=0.9684\n",
      "Epoch: 237, loss=0.2182, train_acc=0.9684\n",
      "Epoch: 238, loss=0.2181, train_acc=0.9684\n",
      "Epoch: 239, loss=0.2180, train_acc=0.9684\n",
      "Epoch: 240, loss=0.2179, train_acc=0.9684\n",
      "Epoch: 241, loss=0.2178, train_acc=0.9684\n",
      "Epoch: 242, loss=0.2177, train_acc=0.9684\n",
      "Epoch: 243, loss=0.2176, train_acc=0.9684\n",
      "Epoch: 244, loss=0.2175, train_acc=0.9684\n",
      "Epoch: 245, loss=0.2174, train_acc=0.9684\n",
      "Epoch: 246, loss=0.2173, train_acc=0.9692\n",
      "Epoch: 247, loss=0.2172, train_acc=0.9684\n",
      "Epoch: 248, loss=0.2171, train_acc=0.9684\n",
      "Epoch: 249, loss=0.2170, train_acc=0.9684\n",
      "\n",
      "Classification report of my MLP model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        55\n",
      "           1       0.89      0.89      0.89        55\n",
      "           2       0.96      0.98      0.97        52\n",
      "           3       0.96      0.95      0.95        56\n",
      "           4       0.95      0.95      0.95        64\n",
      "           5       0.96      0.93      0.94        73\n",
      "           6       0.97      0.98      0.97        57\n",
      "           7       0.97      0.98      0.98        62\n",
      "           8       0.94      0.88      0.91        52\n",
      "           9       0.89      0.93      0.91        68\n",
      "\n",
      "    accuracy                           0.95       594\n",
      "   macro avg       0.95      0.95      0.95       594\n",
      "weighted avg       0.95      0.95      0.95       594\n",
      "\n",
      "\n",
      "Scikit-learn MLPClassifier Info:\n",
      "Number of layers: 3\n",
      "Number of neurons in each layer: (100,)\n",
      "Number of output classes: 10\n",
      "Activation function: relu\n",
      "Solver: adam\n",
      "Learning rate: constant\n",
      "Initial learning rate: 0.001\n",
      "Batch size: auto\n",
      "Maximum number of iterations: 200\n",
      "\n",
      "Classification report of sklearn's model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        55\n",
      "           1       0.98      0.91      0.94        55\n",
      "           2       0.96      1.00      0.98        52\n",
      "           3       1.00      0.95      0.97        56\n",
      "           4       0.97      1.00      0.98        64\n",
      "           5       0.97      0.95      0.96        73\n",
      "           6       0.98      0.98      0.98        57\n",
      "           7       1.00      0.98      0.99        62\n",
      "           8       0.89      0.96      0.93        52\n",
      "           9       0.96      0.97      0.96        68\n",
      "\n",
      "    accuracy                           0.97       594\n",
      "   macro avg       0.97      0.97      0.97       594\n",
      "weighted avg       0.97      0.97      0.97       594\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funny/miniconda3/envs/pytorch-env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
