{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
    "\n",
    "import nn\n",
    "from optim import SGDOptimizer\n",
    "\n",
    "from supervised_learning import MyMLPClassifier\n",
    "from dataset.load_data import sklearn_to_df, prepare_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, Y = sklearn_to_df(load_digits())\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def prepare_trainer(model):\n",
    "    optimizer = SGDOptimizer(model, learning_rate=0.1, regularization=0.01)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    return optimizer, loss_func\n",
    "\n",
    "def build_model(n_in, n_out):\n",
    "    np.random.seed(101)\n",
    "    model = MyMLPClassifier(n_input=n_in, hiddens=[128, 64, 32, 10], n_classes=n_out, activation='leaky_relu')   \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    X_tr, X_te, y_tr, y_te = prepare_data()\n",
    "    n_in, n_out = X_tr.shape[1], 10\n",
    "\n",
    "    num_epochs = 650\n",
    "    batch_size = 32\n",
    "\n",
    "    model = build_model(n_in, n_out)\n",
    "    optimizer, criterion = prepare_trainer(model)\n",
    "\n",
    "    model.info()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        data_loader = prepare_data_loader(X_tr, y_tr, batch_size)\n",
    "        step = 0\n",
    "        total_loss, total_correct = 0, 0\n",
    "        total_sample = 0\n",
    "\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            # Forward pass\n",
    "            batch_yp = model.forward(batch_X)\n",
    "            loss = criterion.forward(batch_yp, batch_y)\n",
    "\n",
    "            # Backward pass and an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            dout = criterion.backward()\n",
    "            model.backward(dout)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log training progress\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            total_correct += np.sum(np.argmax(batch_yp, axis=1) == batch_y)\n",
    "            total_sample += len(batch_y)\n",
    "        print(f\"Epoch: {epoch}, loss={total_loss / total_sample:.4f}, train_acc={total_correct / total_sample:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = np.argmax(model.forward(X_te), axis=1)\n",
    "    print(\"\\n Classification report:\\n\", classification_report(y_te, ypred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMLPClassifier(\n",
      "(linear): Linear(in_features=64, out_features=128, bias=True)\n",
      "(leaky_relu): LeakyReLU()\n",
      "(linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "(leaky_relu): LeakyReLU()\n",
      "(linear): Linear(in_features=64, out_features=32, bias=True)\n",
      "(leaky_relu): LeakyReLU()\n",
      "(linear): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 0, loss=2.2499, train_acc=0.1405\n",
      "Epoch: 1, loss=2.1920, train_acc=0.2103\n",
      "Epoch: 2, loss=2.1495, train_acc=0.3192\n",
      "Epoch: 3, loss=2.1119, train_acc=0.4090\n",
      "Epoch: 4, loss=2.0759, train_acc=0.4938\n",
      "Epoch: 5, loss=2.0396, train_acc=0.5503\n",
      "Epoch: 6, loss=2.0030, train_acc=0.5902\n",
      "Epoch: 7, loss=1.9665, train_acc=0.6176\n",
      "Epoch: 8, loss=1.9300, train_acc=0.6351\n",
      "Epoch: 9, loss=1.8931, train_acc=0.6517\n",
      "Epoch: 10, loss=1.8554, train_acc=0.6692\n",
      "Epoch: 11, loss=1.8170, train_acc=0.6825\n",
      "Epoch: 12, loss=1.7777, train_acc=0.6933\n",
      "Epoch: 13, loss=1.7376, train_acc=0.6941\n",
      "Epoch: 14, loss=1.6969, train_acc=0.7024\n",
      "Epoch: 15, loss=1.6557, train_acc=0.7091\n",
      "Epoch: 16, loss=1.6140, train_acc=0.7199\n",
      "Epoch: 17, loss=1.5720, train_acc=0.7257\n",
      "Epoch: 18, loss=1.5302, train_acc=0.7315\n",
      "Epoch: 19, loss=1.4887, train_acc=0.7415\n",
      "Epoch: 20, loss=1.4476, train_acc=0.7490\n",
      "Epoch: 21, loss=1.4071, train_acc=0.7531\n",
      "Epoch: 22, loss=1.3674, train_acc=0.7581\n",
      "Epoch: 23, loss=1.3286, train_acc=0.7639\n",
      "Epoch: 24, loss=1.2909, train_acc=0.7756\n",
      "Epoch: 25, loss=1.2542, train_acc=0.7814\n",
      "Epoch: 26, loss=1.2188, train_acc=0.7855\n",
      "Epoch: 27, loss=1.1845, train_acc=0.7963\n",
      "Epoch: 28, loss=1.1514, train_acc=0.8030\n",
      "Epoch: 29, loss=1.1194, train_acc=0.8063\n",
      "Epoch: 30, loss=1.0886, train_acc=0.8130\n",
      "Epoch: 31, loss=1.0590, train_acc=0.8171\n",
      "Epoch: 32, loss=1.0305, train_acc=0.8196\n",
      "Epoch: 33, loss=1.0031, train_acc=0.8263\n",
      "Epoch: 34, loss=0.9768, train_acc=0.8313\n",
      "Epoch: 35, loss=0.9515, train_acc=0.8337\n",
      "Epoch: 36, loss=0.9273, train_acc=0.8396\n",
      "Epoch: 37, loss=0.9041, train_acc=0.8487\n",
      "Epoch: 38, loss=0.8818, train_acc=0.8545\n",
      "Epoch: 39, loss=0.8605, train_acc=0.8537\n",
      "Epoch: 40, loss=0.8400, train_acc=0.8579\n",
      "Epoch: 41, loss=0.8205, train_acc=0.8562\n",
      "Epoch: 42, loss=0.8017, train_acc=0.8620\n",
      "Epoch: 43, loss=0.7838, train_acc=0.8653\n",
      "Epoch: 44, loss=0.7666, train_acc=0.8645\n",
      "Epoch: 45, loss=0.7501, train_acc=0.8662\n",
      "Epoch: 46, loss=0.7343, train_acc=0.8687\n",
      "Epoch: 47, loss=0.7192, train_acc=0.8695\n",
      "Epoch: 48, loss=0.7047, train_acc=0.8695\n",
      "Epoch: 49, loss=0.6907, train_acc=0.8712\n",
      "Epoch: 50, loss=0.6774, train_acc=0.8728\n",
      "Epoch: 51, loss=0.6646, train_acc=0.8745\n",
      "Epoch: 52, loss=0.6522, train_acc=0.8761\n",
      "Epoch: 53, loss=0.6404, train_acc=0.8753\n",
      "Epoch: 54, loss=0.6291, train_acc=0.8770\n",
      "Epoch: 55, loss=0.6182, train_acc=0.8811\n",
      "Epoch: 56, loss=0.6077, train_acc=0.8828\n",
      "Epoch: 57, loss=0.5976, train_acc=0.8836\n",
      "Epoch: 58, loss=0.5879, train_acc=0.8845\n",
      "Epoch: 59, loss=0.5786, train_acc=0.8853\n",
      "Epoch: 60, loss=0.5695, train_acc=0.8878\n",
      "Epoch: 61, loss=0.5609, train_acc=0.8894\n",
      "Epoch: 62, loss=0.5525, train_acc=0.8911\n",
      "Epoch: 63, loss=0.5445, train_acc=0.8919\n",
      "Epoch: 64, loss=0.5367, train_acc=0.8944\n",
      "Epoch: 65, loss=0.5292, train_acc=0.8953\n",
      "Epoch: 66, loss=0.5220, train_acc=0.8969\n",
      "Epoch: 67, loss=0.5150, train_acc=0.8978\n",
      "Epoch: 68, loss=0.5083, train_acc=0.8978\n",
      "Epoch: 69, loss=0.5017, train_acc=0.8994\n",
      "Epoch: 70, loss=0.4954, train_acc=0.8994\n",
      "Epoch: 71, loss=0.4893, train_acc=0.9002\n",
      "Epoch: 72, loss=0.4834, train_acc=0.9011\n",
      "Epoch: 73, loss=0.4777, train_acc=0.9011\n",
      "Epoch: 74, loss=0.4721, train_acc=0.9019\n",
      "Epoch: 75, loss=0.4668, train_acc=0.9027\n",
      "Epoch: 76, loss=0.4615, train_acc=0.9027\n",
      "Epoch: 77, loss=0.4565, train_acc=0.9036\n",
      "Epoch: 78, loss=0.4516, train_acc=0.9027\n",
      "Epoch: 79, loss=0.4468, train_acc=0.9036\n",
      "Epoch: 80, loss=0.4422, train_acc=0.9036\n",
      "Epoch: 81, loss=0.4377, train_acc=0.9044\n",
      "Epoch: 82, loss=0.4333, train_acc=0.9069\n",
      "Epoch: 83, loss=0.4290, train_acc=0.9069\n",
      "Epoch: 84, loss=0.4248, train_acc=0.9069\n",
      "Epoch: 85, loss=0.4208, train_acc=0.9077\n",
      "Epoch: 86, loss=0.4168, train_acc=0.9077\n",
      "Epoch: 87, loss=0.4129, train_acc=0.9086\n",
      "Epoch: 88, loss=0.4090, train_acc=0.9086\n",
      "Epoch: 89, loss=0.4053, train_acc=0.9086\n",
      "Epoch: 90, loss=0.4017, train_acc=0.9086\n",
      "Epoch: 91, loss=0.3982, train_acc=0.9086\n",
      "Epoch: 92, loss=0.3947, train_acc=0.9094\n",
      "Epoch: 93, loss=0.3913, train_acc=0.9094\n",
      "Epoch: 94, loss=0.3880, train_acc=0.9094\n",
      "Epoch: 95, loss=0.3849, train_acc=0.9094\n",
      "Epoch: 96, loss=0.3818, train_acc=0.9094\n",
      "Epoch: 97, loss=0.3788, train_acc=0.9111\n",
      "Epoch: 98, loss=0.3758, train_acc=0.9119\n",
      "Epoch: 99, loss=0.3730, train_acc=0.9119\n",
      "Epoch: 100, loss=0.3702, train_acc=0.9119\n",
      "Epoch: 101, loss=0.3674, train_acc=0.9135\n",
      "Epoch: 102, loss=0.3648, train_acc=0.9152\n",
      "Epoch: 103, loss=0.3622, train_acc=0.9160\n",
      "Epoch: 104, loss=0.3597, train_acc=0.9160\n",
      "Epoch: 105, loss=0.3572, train_acc=0.9160\n",
      "Epoch: 106, loss=0.3548, train_acc=0.9169\n",
      "Epoch: 107, loss=0.3524, train_acc=0.9169\n",
      "Epoch: 108, loss=0.3501, train_acc=0.9177\n",
      "Epoch: 109, loss=0.3478, train_acc=0.9169\n",
      "Epoch: 110, loss=0.3455, train_acc=0.9169\n",
      "Epoch: 111, loss=0.3434, train_acc=0.9169\n",
      "Epoch: 112, loss=0.3412, train_acc=0.9169\n",
      "Epoch: 113, loss=0.3391, train_acc=0.9169\n",
      "Epoch: 114, loss=0.3371, train_acc=0.9177\n",
      "Epoch: 115, loss=0.3351, train_acc=0.9185\n",
      "Epoch: 116, loss=0.3331, train_acc=0.9194\n",
      "Epoch: 117, loss=0.3312, train_acc=0.9194\n",
      "Epoch: 118, loss=0.3293, train_acc=0.9202\n",
      "Epoch: 119, loss=0.3275, train_acc=0.9210\n",
      "Epoch: 120, loss=0.3256, train_acc=0.9219\n",
      "Epoch: 121, loss=0.3239, train_acc=0.9219\n",
      "Epoch: 122, loss=0.3221, train_acc=0.9219\n",
      "Epoch: 123, loss=0.3204, train_acc=0.9227\n",
      "Epoch: 124, loss=0.3187, train_acc=0.9227\n",
      "Epoch: 125, loss=0.3171, train_acc=0.9235\n",
      "Epoch: 126, loss=0.3154, train_acc=0.9235\n",
      "Epoch: 127, loss=0.3139, train_acc=0.9235\n",
      "Epoch: 128, loss=0.3123, train_acc=0.9235\n",
      "Epoch: 129, loss=0.3108, train_acc=0.9244\n",
      "Epoch: 130, loss=0.3092, train_acc=0.9285\n",
      "Epoch: 131, loss=0.3078, train_acc=0.9285\n",
      "Epoch: 132, loss=0.3063, train_acc=0.9285\n",
      "Epoch: 133, loss=0.3049, train_acc=0.9285\n",
      "Epoch: 134, loss=0.3034, train_acc=0.9285\n",
      "Epoch: 135, loss=0.3021, train_acc=0.9293\n",
      "Epoch: 136, loss=0.3007, train_acc=0.9293\n",
      "Epoch: 137, loss=0.2993, train_acc=0.9293\n",
      "Epoch: 138, loss=0.2980, train_acc=0.9302\n",
      "Epoch: 139, loss=0.2967, train_acc=0.9302\n",
      "Epoch: 140, loss=0.2954, train_acc=0.9310\n",
      "Epoch: 141, loss=0.2941, train_acc=0.9318\n",
      "Epoch: 142, loss=0.2929, train_acc=0.9318\n",
      "Epoch: 143, loss=0.2917, train_acc=0.9318\n",
      "Epoch: 144, loss=0.2905, train_acc=0.9327\n",
      "Epoch: 145, loss=0.2893, train_acc=0.9335\n",
      "Epoch: 146, loss=0.2881, train_acc=0.9343\n",
      "Epoch: 147, loss=0.2870, train_acc=0.9343\n",
      "Epoch: 148, loss=0.2858, train_acc=0.9343\n",
      "Epoch: 149, loss=0.2847, train_acc=0.9343\n",
      "Epoch: 150, loss=0.2836, train_acc=0.9352\n",
      "Epoch: 151, loss=0.2825, train_acc=0.9352\n",
      "Epoch: 152, loss=0.2814, train_acc=0.9352\n",
      "Epoch: 153, loss=0.2804, train_acc=0.9352\n",
      "Epoch: 154, loss=0.2793, train_acc=0.9352\n",
      "Epoch: 155, loss=0.2783, train_acc=0.9352\n",
      "Epoch: 156, loss=0.2773, train_acc=0.9352\n",
      "Epoch: 157, loss=0.2763, train_acc=0.9352\n",
      "Epoch: 158, loss=0.2753, train_acc=0.9352\n",
      "Epoch: 159, loss=0.2743, train_acc=0.9352\n",
      "Epoch: 160, loss=0.2734, train_acc=0.9352\n",
      "Epoch: 161, loss=0.2724, train_acc=0.9352\n",
      "Epoch: 162, loss=0.2715, train_acc=0.9360\n",
      "Epoch: 163, loss=0.2706, train_acc=0.9368\n",
      "Epoch: 164, loss=0.2697, train_acc=0.9377\n",
      "Epoch: 165, loss=0.2688, train_acc=0.9377\n",
      "Epoch: 166, loss=0.2679, train_acc=0.9385\n",
      "Epoch: 167, loss=0.2670, train_acc=0.9385\n",
      "Epoch: 168, loss=0.2661, train_acc=0.9385\n",
      "Epoch: 169, loss=0.2653, train_acc=0.9385\n",
      "Epoch: 170, loss=0.2644, train_acc=0.9385\n",
      "Epoch: 171, loss=0.2636, train_acc=0.9385\n",
      "Epoch: 172, loss=0.2628, train_acc=0.9385\n",
      "Epoch: 173, loss=0.2620, train_acc=0.9385\n",
      "Epoch: 174, loss=0.2612, train_acc=0.9385\n",
      "Epoch: 175, loss=0.2604, train_acc=0.9385\n",
      "Epoch: 176, loss=0.2596, train_acc=0.9385\n",
      "Epoch: 177, loss=0.2589, train_acc=0.9393\n",
      "Epoch: 178, loss=0.2581, train_acc=0.9393\n",
      "Epoch: 179, loss=0.2573, train_acc=0.9393\n",
      "Epoch: 180, loss=0.2566, train_acc=0.9393\n",
      "Epoch: 181, loss=0.2559, train_acc=0.9393\n",
      "Epoch: 182, loss=0.2551, train_acc=0.9401\n",
      "Epoch: 183, loss=0.2544, train_acc=0.9401\n",
      "Epoch: 184, loss=0.2537, train_acc=0.9401\n",
      "Epoch: 185, loss=0.2530, train_acc=0.9410\n",
      "Epoch: 186, loss=0.2523, train_acc=0.9410\n",
      "Epoch: 187, loss=0.2516, train_acc=0.9418\n",
      "Epoch: 188, loss=0.2510, train_acc=0.9418\n",
      "Epoch: 189, loss=0.2503, train_acc=0.9426\n",
      "Epoch: 190, loss=0.2496, train_acc=0.9426\n",
      "Epoch: 191, loss=0.2490, train_acc=0.9426\n",
      "Epoch: 192, loss=0.2483, train_acc=0.9435\n",
      "Epoch: 193, loss=0.2477, train_acc=0.9435\n",
      "Epoch: 194, loss=0.2471, train_acc=0.9443\n",
      "Epoch: 195, loss=0.2464, train_acc=0.9443\n",
      "Epoch: 196, loss=0.2458, train_acc=0.9451\n",
      "Epoch: 197, loss=0.2452, train_acc=0.9451\n",
      "Epoch: 198, loss=0.2446, train_acc=0.9451\n",
      "Epoch: 199, loss=0.2440, train_acc=0.9460\n",
      "Epoch: 200, loss=0.2434, train_acc=0.9451\n",
      "Epoch: 201, loss=0.2428, train_acc=0.9451\n",
      "Epoch: 202, loss=0.2422, train_acc=0.9460\n",
      "Epoch: 203, loss=0.2416, train_acc=0.9460\n",
      "Epoch: 204, loss=0.2411, train_acc=0.9460\n",
      "Epoch: 205, loss=0.2405, train_acc=0.9460\n",
      "Epoch: 206, loss=0.2399, train_acc=0.9460\n",
      "Epoch: 207, loss=0.2394, train_acc=0.9460\n",
      "Epoch: 208, loss=0.2388, train_acc=0.9460\n",
      "Epoch: 209, loss=0.2383, train_acc=0.9468\n",
      "Epoch: 210, loss=0.2377, train_acc=0.9476\n",
      "Epoch: 211, loss=0.2372, train_acc=0.9476\n",
      "Epoch: 212, loss=0.2367, train_acc=0.9476\n",
      "Epoch: 213, loss=0.2361, train_acc=0.9485\n",
      "Epoch: 214, loss=0.2356, train_acc=0.9485\n",
      "Epoch: 215, loss=0.2351, train_acc=0.9485\n",
      "Epoch: 216, loss=0.2346, train_acc=0.9485\n",
      "Epoch: 217, loss=0.2341, train_acc=0.9485\n",
      "Epoch: 218, loss=0.2336, train_acc=0.9485\n",
      "Epoch: 219, loss=0.2331, train_acc=0.9485\n",
      "Epoch: 220, loss=0.2326, train_acc=0.9485\n",
      "Epoch: 221, loss=0.2321, train_acc=0.9485\n",
      "Epoch: 222, loss=0.2316, train_acc=0.9485\n",
      "Epoch: 223, loss=0.2311, train_acc=0.9485\n",
      "Epoch: 224, loss=0.2306, train_acc=0.9485\n",
      "Epoch: 225, loss=0.2301, train_acc=0.9485\n",
      "Epoch: 226, loss=0.2297, train_acc=0.9485\n",
      "Epoch: 227, loss=0.2292, train_acc=0.9485\n",
      "Epoch: 228, loss=0.2288, train_acc=0.9485\n",
      "Epoch: 229, loss=0.2283, train_acc=0.9493\n",
      "Epoch: 230, loss=0.2278, train_acc=0.9501\n",
      "Epoch: 231, loss=0.2274, train_acc=0.9501\n",
      "Epoch: 232, loss=0.2269, train_acc=0.9501\n",
      "Epoch: 233, loss=0.2265, train_acc=0.9501\n",
      "Epoch: 234, loss=0.2261, train_acc=0.9501\n",
      "Epoch: 235, loss=0.2256, train_acc=0.9501\n",
      "Epoch: 236, loss=0.2252, train_acc=0.9501\n",
      "Epoch: 237, loss=0.2248, train_acc=0.9501\n",
      "Epoch: 238, loss=0.2244, train_acc=0.9501\n",
      "Epoch: 239, loss=0.2239, train_acc=0.9501\n",
      "Epoch: 240, loss=0.2235, train_acc=0.9501\n",
      "Epoch: 241, loss=0.2231, train_acc=0.9501\n",
      "Epoch: 242, loss=0.2227, train_acc=0.9501\n",
      "Epoch: 243, loss=0.2223, train_acc=0.9501\n",
      "Epoch: 244, loss=0.2219, train_acc=0.9501\n",
      "Epoch: 245, loss=0.2215, train_acc=0.9501\n",
      "Epoch: 246, loss=0.2211, train_acc=0.9501\n",
      "Epoch: 247, loss=0.2207, train_acc=0.9501\n",
      "Epoch: 248, loss=0.2203, train_acc=0.9501\n",
      "Epoch: 249, loss=0.2199, train_acc=0.9501\n",
      "Epoch: 250, loss=0.2195, train_acc=0.9501\n",
      "Epoch: 251, loss=0.2192, train_acc=0.9501\n",
      "Epoch: 252, loss=0.2188, train_acc=0.9501\n",
      "Epoch: 253, loss=0.2184, train_acc=0.9501\n",
      "Epoch: 254, loss=0.2180, train_acc=0.9501\n",
      "Epoch: 255, loss=0.2177, train_acc=0.9501\n",
      "Epoch: 256, loss=0.2173, train_acc=0.9501\n",
      "Epoch: 257, loss=0.2169, train_acc=0.9510\n",
      "Epoch: 258, loss=0.2166, train_acc=0.9510\n",
      "Epoch: 259, loss=0.2162, train_acc=0.9510\n",
      "Epoch: 260, loss=0.2158, train_acc=0.9510\n",
      "Epoch: 261, loss=0.2155, train_acc=0.9510\n",
      "Epoch: 262, loss=0.2151, train_acc=0.9510\n",
      "Epoch: 263, loss=0.2148, train_acc=0.9510\n",
      "Epoch: 264, loss=0.2144, train_acc=0.9518\n",
      "Epoch: 265, loss=0.2141, train_acc=0.9518\n",
      "Epoch: 266, loss=0.2138, train_acc=0.9518\n",
      "Epoch: 267, loss=0.2134, train_acc=0.9526\n",
      "Epoch: 268, loss=0.2131, train_acc=0.9526\n",
      "Epoch: 269, loss=0.2127, train_acc=0.9526\n",
      "Epoch: 270, loss=0.2124, train_acc=0.9526\n",
      "Epoch: 271, loss=0.2121, train_acc=0.9526\n",
      "Epoch: 272, loss=0.2118, train_acc=0.9526\n",
      "Epoch: 273, loss=0.2114, train_acc=0.9526\n",
      "Epoch: 274, loss=0.2111, train_acc=0.9526\n",
      "Epoch: 275, loss=0.2108, train_acc=0.9526\n",
      "Epoch: 276, loss=0.2105, train_acc=0.9526\n",
      "Epoch: 277, loss=0.2101, train_acc=0.9526\n",
      "Epoch: 278, loss=0.2098, train_acc=0.9526\n",
      "Epoch: 279, loss=0.2095, train_acc=0.9526\n",
      "Epoch: 280, loss=0.2092, train_acc=0.9526\n",
      "Epoch: 281, loss=0.2089, train_acc=0.9526\n",
      "Epoch: 282, loss=0.2086, train_acc=0.9526\n",
      "Epoch: 283, loss=0.2083, train_acc=0.9526\n",
      "Epoch: 284, loss=0.2080, train_acc=0.9526\n",
      "Epoch: 285, loss=0.2077, train_acc=0.9526\n",
      "Epoch: 286, loss=0.2074, train_acc=0.9526\n",
      "Epoch: 287, loss=0.2071, train_acc=0.9526\n",
      "Epoch: 288, loss=0.2068, train_acc=0.9526\n",
      "Epoch: 289, loss=0.2065, train_acc=0.9526\n",
      "Epoch: 290, loss=0.2062, train_acc=0.9526\n",
      "Epoch: 291, loss=0.2059, train_acc=0.9526\n",
      "Epoch: 292, loss=0.2056, train_acc=0.9526\n",
      "Epoch: 293, loss=0.2053, train_acc=0.9526\n",
      "Epoch: 294, loss=0.2050, train_acc=0.9526\n",
      "Epoch: 295, loss=0.2048, train_acc=0.9526\n",
      "Epoch: 296, loss=0.2045, train_acc=0.9526\n",
      "Epoch: 297, loss=0.2042, train_acc=0.9526\n",
      "Epoch: 298, loss=0.2039, train_acc=0.9526\n",
      "Epoch: 299, loss=0.2037, train_acc=0.9526\n",
      "Epoch: 300, loss=0.2034, train_acc=0.9526\n",
      "Epoch: 301, loss=0.2031, train_acc=0.9534\n",
      "Epoch: 302, loss=0.2028, train_acc=0.9534\n",
      "Epoch: 303, loss=0.2026, train_acc=0.9534\n",
      "Epoch: 304, loss=0.2023, train_acc=0.9551\n",
      "Epoch: 305, loss=0.2020, train_acc=0.9551\n",
      "Epoch: 306, loss=0.2018, train_acc=0.9559\n",
      "Epoch: 307, loss=0.2015, train_acc=0.9559\n",
      "Epoch: 308, loss=0.2013, train_acc=0.9559\n",
      "Epoch: 309, loss=0.2010, train_acc=0.9559\n",
      "Epoch: 310, loss=0.2007, train_acc=0.9559\n",
      "Epoch: 311, loss=0.2005, train_acc=0.9568\n",
      "Epoch: 312, loss=0.2002, train_acc=0.9568\n",
      "Epoch: 313, loss=0.2000, train_acc=0.9568\n",
      "Epoch: 314, loss=0.1997, train_acc=0.9568\n",
      "Epoch: 315, loss=0.1995, train_acc=0.9568\n",
      "Epoch: 316, loss=0.1992, train_acc=0.9568\n",
      "Epoch: 317, loss=0.1990, train_acc=0.9568\n",
      "Epoch: 318, loss=0.1987, train_acc=0.9568\n",
      "Epoch: 319, loss=0.1985, train_acc=0.9584\n",
      "Epoch: 320, loss=0.1983, train_acc=0.9584\n",
      "Epoch: 321, loss=0.1980, train_acc=0.9584\n",
      "Epoch: 322, loss=0.1978, train_acc=0.9584\n",
      "Epoch: 323, loss=0.1975, train_acc=0.9584\n",
      "Epoch: 324, loss=0.1973, train_acc=0.9584\n",
      "Epoch: 325, loss=0.1971, train_acc=0.9584\n",
      "Epoch: 326, loss=0.1968, train_acc=0.9584\n",
      "Epoch: 327, loss=0.1966, train_acc=0.9584\n",
      "Epoch: 328, loss=0.1964, train_acc=0.9584\n",
      "Epoch: 329, loss=0.1962, train_acc=0.9584\n",
      "Epoch: 330, loss=0.1959, train_acc=0.9584\n",
      "Epoch: 331, loss=0.1957, train_acc=0.9584\n",
      "Epoch: 332, loss=0.1955, train_acc=0.9584\n",
      "Epoch: 333, loss=0.1952, train_acc=0.9584\n",
      "Epoch: 334, loss=0.1950, train_acc=0.9584\n",
      "Epoch: 335, loss=0.1948, train_acc=0.9584\n",
      "Epoch: 336, loss=0.1946, train_acc=0.9584\n",
      "Epoch: 337, loss=0.1944, train_acc=0.9584\n",
      "Epoch: 338, loss=0.1941, train_acc=0.9584\n",
      "Epoch: 339, loss=0.1939, train_acc=0.9584\n",
      "Epoch: 340, loss=0.1937, train_acc=0.9584\n",
      "Epoch: 341, loss=0.1935, train_acc=0.9584\n",
      "Epoch: 342, loss=0.1933, train_acc=0.9584\n",
      "Epoch: 343, loss=0.1931, train_acc=0.9584\n",
      "Epoch: 344, loss=0.1929, train_acc=0.9584\n",
      "Epoch: 345, loss=0.1926, train_acc=0.9584\n",
      "Epoch: 346, loss=0.1924, train_acc=0.9584\n",
      "Epoch: 347, loss=0.1922, train_acc=0.9584\n",
      "Epoch: 348, loss=0.1920, train_acc=0.9593\n",
      "Epoch: 349, loss=0.1918, train_acc=0.9593\n",
      "Epoch: 350, loss=0.1916, train_acc=0.9593\n",
      "Epoch: 351, loss=0.1914, train_acc=0.9593\n",
      "Epoch: 352, loss=0.1912, train_acc=0.9593\n",
      "Epoch: 353, loss=0.1910, train_acc=0.9593\n",
      "Epoch: 354, loss=0.1908, train_acc=0.9593\n",
      "Epoch: 355, loss=0.1906, train_acc=0.9593\n",
      "Epoch: 356, loss=0.1904, train_acc=0.9593\n",
      "Epoch: 357, loss=0.1902, train_acc=0.9593\n",
      "Epoch: 358, loss=0.1900, train_acc=0.9593\n",
      "Epoch: 359, loss=0.1898, train_acc=0.9593\n",
      "Epoch: 360, loss=0.1896, train_acc=0.9593\n",
      "Epoch: 361, loss=0.1894, train_acc=0.9593\n",
      "Epoch: 362, loss=0.1892, train_acc=0.9593\n",
      "Epoch: 363, loss=0.1891, train_acc=0.9593\n",
      "Epoch: 364, loss=0.1889, train_acc=0.9593\n",
      "Epoch: 365, loss=0.1887, train_acc=0.9593\n",
      "Epoch: 366, loss=0.1885, train_acc=0.9593\n",
      "Epoch: 367, loss=0.1883, train_acc=0.9593\n",
      "Epoch: 368, loss=0.1881, train_acc=0.9593\n",
      "Epoch: 369, loss=0.1879, train_acc=0.9593\n",
      "Epoch: 370, loss=0.1878, train_acc=0.9593\n",
      "Epoch: 371, loss=0.1876, train_acc=0.9593\n",
      "Epoch: 372, loss=0.1874, train_acc=0.9601\n",
      "Epoch: 373, loss=0.1872, train_acc=0.9601\n",
      "Epoch: 374, loss=0.1870, train_acc=0.9601\n",
      "Epoch: 375, loss=0.1868, train_acc=0.9609\n",
      "Epoch: 376, loss=0.1867, train_acc=0.9609\n",
      "Epoch: 377, loss=0.1865, train_acc=0.9609\n",
      "Epoch: 378, loss=0.1863, train_acc=0.9609\n",
      "Epoch: 379, loss=0.1861, train_acc=0.9609\n",
      "Epoch: 380, loss=0.1860, train_acc=0.9609\n",
      "Epoch: 381, loss=0.1858, train_acc=0.9609\n",
      "Epoch: 382, loss=0.1856, train_acc=0.9609\n",
      "Epoch: 383, loss=0.1854, train_acc=0.9609\n",
      "Epoch: 384, loss=0.1853, train_acc=0.9609\n",
      "Epoch: 385, loss=0.1851, train_acc=0.9609\n",
      "Epoch: 386, loss=0.1849, train_acc=0.9609\n",
      "Epoch: 387, loss=0.1848, train_acc=0.9609\n",
      "Epoch: 388, loss=0.1846, train_acc=0.9609\n",
      "Epoch: 389, loss=0.1844, train_acc=0.9609\n",
      "Epoch: 390, loss=0.1843, train_acc=0.9609\n",
      "Epoch: 391, loss=0.1841, train_acc=0.9609\n",
      "Epoch: 392, loss=0.1839, train_acc=0.9609\n",
      "Epoch: 393, loss=0.1838, train_acc=0.9609\n",
      "Epoch: 394, loss=0.1836, train_acc=0.9609\n",
      "Epoch: 395, loss=0.1834, train_acc=0.9609\n",
      "Epoch: 396, loss=0.1833, train_acc=0.9609\n",
      "Epoch: 397, loss=0.1831, train_acc=0.9609\n",
      "Epoch: 398, loss=0.1830, train_acc=0.9609\n",
      "Epoch: 399, loss=0.1828, train_acc=0.9609\n",
      "Epoch: 400, loss=0.1826, train_acc=0.9609\n",
      "Epoch: 401, loss=0.1825, train_acc=0.9609\n",
      "Epoch: 402, loss=0.1823, train_acc=0.9609\n",
      "Epoch: 403, loss=0.1822, train_acc=0.9609\n",
      "Epoch: 404, loss=0.1820, train_acc=0.9609\n",
      "Epoch: 405, loss=0.1819, train_acc=0.9609\n",
      "Epoch: 406, loss=0.1817, train_acc=0.9609\n",
      "Epoch: 407, loss=0.1815, train_acc=0.9609\n",
      "Epoch: 408, loss=0.1814, train_acc=0.9618\n",
      "Epoch: 409, loss=0.1812, train_acc=0.9618\n",
      "Epoch: 410, loss=0.1811, train_acc=0.9618\n",
      "Epoch: 411, loss=0.1809, train_acc=0.9618\n",
      "Epoch: 412, loss=0.1808, train_acc=0.9618\n",
      "Epoch: 413, loss=0.1806, train_acc=0.9618\n",
      "Epoch: 414, loss=0.1805, train_acc=0.9618\n",
      "Epoch: 415, loss=0.1803, train_acc=0.9626\n",
      "Epoch: 416, loss=0.1802, train_acc=0.9626\n",
      "Epoch: 417, loss=0.1800, train_acc=0.9626\n",
      "Epoch: 418, loss=0.1799, train_acc=0.9626\n",
      "Epoch: 419, loss=0.1798, train_acc=0.9626\n",
      "Epoch: 420, loss=0.1796, train_acc=0.9626\n",
      "Epoch: 421, loss=0.1795, train_acc=0.9634\n",
      "Epoch: 422, loss=0.1793, train_acc=0.9634\n",
      "Epoch: 423, loss=0.1792, train_acc=0.9634\n",
      "Epoch: 424, loss=0.1790, train_acc=0.9634\n",
      "Epoch: 425, loss=0.1789, train_acc=0.9634\n",
      "Epoch: 426, loss=0.1788, train_acc=0.9634\n",
      "Epoch: 427, loss=0.1786, train_acc=0.9634\n",
      "Epoch: 428, loss=0.1785, train_acc=0.9634\n",
      "Epoch: 429, loss=0.1783, train_acc=0.9634\n",
      "Epoch: 430, loss=0.1782, train_acc=0.9643\n",
      "Epoch: 431, loss=0.1781, train_acc=0.9634\n",
      "Epoch: 432, loss=0.1779, train_acc=0.9643\n",
      "Epoch: 433, loss=0.1778, train_acc=0.9643\n",
      "Epoch: 434, loss=0.1776, train_acc=0.9643\n",
      "Epoch: 435, loss=0.1775, train_acc=0.9643\n",
      "Epoch: 436, loss=0.1774, train_acc=0.9643\n",
      "Epoch: 437, loss=0.1772, train_acc=0.9643\n",
      "Epoch: 438, loss=0.1771, train_acc=0.9643\n",
      "Epoch: 439, loss=0.1770, train_acc=0.9643\n",
      "Epoch: 440, loss=0.1768, train_acc=0.9643\n",
      "Epoch: 441, loss=0.1767, train_acc=0.9643\n",
      "Epoch: 442, loss=0.1766, train_acc=0.9643\n",
      "Epoch: 443, loss=0.1764, train_acc=0.9643\n",
      "Epoch: 444, loss=0.1763, train_acc=0.9643\n",
      "Epoch: 445, loss=0.1762, train_acc=0.9643\n",
      "Epoch: 446, loss=0.1760, train_acc=0.9643\n",
      "Epoch: 447, loss=0.1759, train_acc=0.9643\n",
      "Epoch: 448, loss=0.1758, train_acc=0.9643\n",
      "Epoch: 449, loss=0.1757, train_acc=0.9643\n",
      "Epoch: 450, loss=0.1755, train_acc=0.9643\n",
      "Epoch: 451, loss=0.1754, train_acc=0.9643\n",
      "Epoch: 452, loss=0.1753, train_acc=0.9643\n",
      "Epoch: 453, loss=0.1752, train_acc=0.9643\n",
      "Epoch: 454, loss=0.1750, train_acc=0.9643\n",
      "Epoch: 455, loss=0.1749, train_acc=0.9643\n",
      "Epoch: 456, loss=0.1748, train_acc=0.9643\n",
      "Epoch: 457, loss=0.1747, train_acc=0.9643\n",
      "Epoch: 458, loss=0.1745, train_acc=0.9643\n",
      "Epoch: 459, loss=0.1744, train_acc=0.9643\n",
      "Epoch: 460, loss=0.1743, train_acc=0.9643\n",
      "Epoch: 461, loss=0.1742, train_acc=0.9643\n",
      "Epoch: 462, loss=0.1740, train_acc=0.9643\n",
      "Epoch: 463, loss=0.1739, train_acc=0.9643\n",
      "Epoch: 464, loss=0.1738, train_acc=0.9643\n",
      "Epoch: 465, loss=0.1737, train_acc=0.9643\n",
      "Epoch: 466, loss=0.1736, train_acc=0.9643\n",
      "Epoch: 467, loss=0.1734, train_acc=0.9643\n",
      "Epoch: 468, loss=0.1733, train_acc=0.9643\n",
      "Epoch: 469, loss=0.1732, train_acc=0.9643\n",
      "Epoch: 470, loss=0.1731, train_acc=0.9643\n",
      "Epoch: 471, loss=0.1730, train_acc=0.9643\n",
      "Epoch: 472, loss=0.1729, train_acc=0.9643\n",
      "Epoch: 473, loss=0.1727, train_acc=0.9643\n",
      "Epoch: 474, loss=0.1726, train_acc=0.9643\n",
      "Epoch: 475, loss=0.1725, train_acc=0.9643\n",
      "Epoch: 476, loss=0.1724, train_acc=0.9643\n",
      "Epoch: 477, loss=0.1723, train_acc=0.9643\n",
      "Epoch: 478, loss=0.1722, train_acc=0.9643\n",
      "Epoch: 479, loss=0.1721, train_acc=0.9643\n",
      "Epoch: 480, loss=0.1719, train_acc=0.9643\n",
      "Epoch: 481, loss=0.1718, train_acc=0.9643\n",
      "Epoch: 482, loss=0.1717, train_acc=0.9643\n",
      "Epoch: 483, loss=0.1716, train_acc=0.9643\n",
      "Epoch: 484, loss=0.1715, train_acc=0.9643\n",
      "Epoch: 485, loss=0.1714, train_acc=0.9643\n",
      "Epoch: 486, loss=0.1713, train_acc=0.9643\n",
      "Epoch: 487, loss=0.1712, train_acc=0.9651\n",
      "Epoch: 488, loss=0.1710, train_acc=0.9651\n",
      "Epoch: 489, loss=0.1709, train_acc=0.9651\n",
      "Epoch: 490, loss=0.1708, train_acc=0.9651\n",
      "Epoch: 491, loss=0.1707, train_acc=0.9651\n",
      "Epoch: 492, loss=0.1706, train_acc=0.9651\n",
      "Epoch: 493, loss=0.1705, train_acc=0.9651\n",
      "Epoch: 494, loss=0.1704, train_acc=0.9651\n",
      "Epoch: 495, loss=0.1703, train_acc=0.9651\n",
      "Epoch: 496, loss=0.1702, train_acc=0.9651\n",
      "Epoch: 497, loss=0.1701, train_acc=0.9651\n",
      "Epoch: 498, loss=0.1700, train_acc=0.9651\n",
      "Epoch: 499, loss=0.1699, train_acc=0.9651\n",
      "Epoch: 500, loss=0.1698, train_acc=0.9651\n",
      "Epoch: 501, loss=0.1697, train_acc=0.9651\n",
      "Epoch: 502, loss=0.1695, train_acc=0.9659\n",
      "Epoch: 503, loss=0.1694, train_acc=0.9651\n",
      "Epoch: 504, loss=0.1693, train_acc=0.9651\n",
      "Epoch: 505, loss=0.1692, train_acc=0.9651\n",
      "Epoch: 506, loss=0.1691, train_acc=0.9651\n",
      "Epoch: 507, loss=0.1690, train_acc=0.9651\n",
      "Epoch: 508, loss=0.1689, train_acc=0.9651\n",
      "Epoch: 509, loss=0.1688, train_acc=0.9651\n",
      "Epoch: 510, loss=0.1687, train_acc=0.9651\n",
      "Epoch: 511, loss=0.1686, train_acc=0.9651\n",
      "Epoch: 512, loss=0.1685, train_acc=0.9651\n",
      "Epoch: 513, loss=0.1684, train_acc=0.9651\n",
      "Epoch: 514, loss=0.1683, train_acc=0.9651\n",
      "Epoch: 515, loss=0.1682, train_acc=0.9651\n",
      "Epoch: 516, loss=0.1681, train_acc=0.9651\n",
      "Epoch: 517, loss=0.1680, train_acc=0.9651\n",
      "Epoch: 518, loss=0.1679, train_acc=0.9651\n",
      "Epoch: 519, loss=0.1678, train_acc=0.9651\n",
      "Epoch: 520, loss=0.1677, train_acc=0.9651\n",
      "Epoch: 521, loss=0.1676, train_acc=0.9651\n",
      "Epoch: 522, loss=0.1675, train_acc=0.9651\n",
      "Epoch: 523, loss=0.1674, train_acc=0.9659\n",
      "Epoch: 524, loss=0.1673, train_acc=0.9659\n",
      "Epoch: 525, loss=0.1673, train_acc=0.9659\n",
      "Epoch: 526, loss=0.1672, train_acc=0.9659\n",
      "Epoch: 527, loss=0.1671, train_acc=0.9659\n",
      "Epoch: 528, loss=0.1670, train_acc=0.9659\n",
      "Epoch: 529, loss=0.1669, train_acc=0.9659\n",
      "Epoch: 530, loss=0.1668, train_acc=0.9659\n",
      "Epoch: 531, loss=0.1667, train_acc=0.9659\n",
      "Epoch: 532, loss=0.1666, train_acc=0.9659\n",
      "Epoch: 533, loss=0.1665, train_acc=0.9659\n",
      "Epoch: 534, loss=0.1664, train_acc=0.9667\n",
      "Epoch: 535, loss=0.1663, train_acc=0.9667\n",
      "Epoch: 536, loss=0.1662, train_acc=0.9667\n",
      "Epoch: 537, loss=0.1661, train_acc=0.9667\n",
      "Epoch: 538, loss=0.1660, train_acc=0.9667\n",
      "Epoch: 539, loss=0.1659, train_acc=0.9667\n",
      "Epoch: 540, loss=0.1658, train_acc=0.9667\n",
      "Epoch: 541, loss=0.1658, train_acc=0.9667\n",
      "Epoch: 542, loss=0.1657, train_acc=0.9667\n",
      "Epoch: 543, loss=0.1656, train_acc=0.9667\n",
      "Epoch: 544, loss=0.1655, train_acc=0.9667\n",
      "Epoch: 545, loss=0.1654, train_acc=0.9667\n",
      "Epoch: 546, loss=0.1653, train_acc=0.9667\n",
      "Epoch: 547, loss=0.1652, train_acc=0.9667\n",
      "Epoch: 548, loss=0.1651, train_acc=0.9667\n",
      "Epoch: 549, loss=0.1650, train_acc=0.9667\n",
      "Epoch: 550, loss=0.1650, train_acc=0.9667\n",
      "Epoch: 551, loss=0.1649, train_acc=0.9667\n",
      "Epoch: 552, loss=0.1648, train_acc=0.9667\n",
      "Epoch: 553, loss=0.1647, train_acc=0.9667\n",
      "Epoch: 554, loss=0.1646, train_acc=0.9667\n",
      "Epoch: 555, loss=0.1645, train_acc=0.9667\n",
      "Epoch: 556, loss=0.1644, train_acc=0.9667\n",
      "Epoch: 557, loss=0.1643, train_acc=0.9667\n",
      "Epoch: 558, loss=0.1643, train_acc=0.9667\n",
      "Epoch: 559, loss=0.1642, train_acc=0.9667\n",
      "Epoch: 560, loss=0.1641, train_acc=0.9667\n",
      "Epoch: 561, loss=0.1640, train_acc=0.9667\n",
      "Epoch: 562, loss=0.1639, train_acc=0.9667\n",
      "Epoch: 563, loss=0.1638, train_acc=0.9667\n",
      "Epoch: 564, loss=0.1637, train_acc=0.9667\n",
      "Epoch: 565, loss=0.1637, train_acc=0.9667\n",
      "Epoch: 566, loss=0.1636, train_acc=0.9667\n",
      "Epoch: 567, loss=0.1635, train_acc=0.9667\n",
      "Epoch: 568, loss=0.1634, train_acc=0.9667\n",
      "Epoch: 569, loss=0.1633, train_acc=0.9667\n",
      "Epoch: 570, loss=0.1632, train_acc=0.9667\n",
      "Epoch: 571, loss=0.1632, train_acc=0.9667\n",
      "Epoch: 572, loss=0.1631, train_acc=0.9667\n",
      "Epoch: 573, loss=0.1630, train_acc=0.9667\n",
      "Epoch: 574, loss=0.1629, train_acc=0.9676\n",
      "Epoch: 575, loss=0.1628, train_acc=0.9684\n",
      "Epoch: 576, loss=0.1628, train_acc=0.9684\n",
      "Epoch: 577, loss=0.1627, train_acc=0.9684\n",
      "Epoch: 578, loss=0.1626, train_acc=0.9684\n",
      "Epoch: 579, loss=0.1625, train_acc=0.9684\n",
      "Epoch: 580, loss=0.1624, train_acc=0.9684\n",
      "Epoch: 581, loss=0.1624, train_acc=0.9684\n",
      "Epoch: 582, loss=0.1623, train_acc=0.9684\n",
      "Epoch: 583, loss=0.1622, train_acc=0.9684\n",
      "Epoch: 584, loss=0.1621, train_acc=0.9684\n",
      "Epoch: 585, loss=0.1620, train_acc=0.9684\n",
      "Epoch: 586, loss=0.1620, train_acc=0.9684\n",
      "Epoch: 587, loss=0.1619, train_acc=0.9684\n",
      "Epoch: 588, loss=0.1618, train_acc=0.9684\n",
      "Epoch: 589, loss=0.1617, train_acc=0.9684\n",
      "Epoch: 590, loss=0.1616, train_acc=0.9684\n",
      "Epoch: 591, loss=0.1616, train_acc=0.9684\n",
      "Epoch: 592, loss=0.1615, train_acc=0.9684\n",
      "Epoch: 593, loss=0.1614, train_acc=0.9684\n",
      "Epoch: 594, loss=0.1613, train_acc=0.9684\n",
      "Epoch: 595, loss=0.1613, train_acc=0.9684\n",
      "Epoch: 596, loss=0.1612, train_acc=0.9684\n",
      "Epoch: 597, loss=0.1611, train_acc=0.9684\n",
      "Epoch: 598, loss=0.1610, train_acc=0.9684\n",
      "Epoch: 599, loss=0.1610, train_acc=0.9684\n",
      "Epoch: 600, loss=0.1609, train_acc=0.9684\n",
      "Epoch: 601, loss=0.1608, train_acc=0.9692\n",
      "Epoch: 602, loss=0.1607, train_acc=0.9692\n",
      "Epoch: 603, loss=0.1607, train_acc=0.9692\n",
      "Epoch: 604, loss=0.1606, train_acc=0.9692\n",
      "Epoch: 605, loss=0.1605, train_acc=0.9692\n",
      "Epoch: 606, loss=0.1604, train_acc=0.9692\n",
      "Epoch: 607, loss=0.1604, train_acc=0.9692\n",
      "Epoch: 608, loss=0.1603, train_acc=0.9692\n",
      "Epoch: 609, loss=0.1602, train_acc=0.9692\n",
      "Epoch: 610, loss=0.1602, train_acc=0.9692\n",
      "Epoch: 611, loss=0.1601, train_acc=0.9692\n",
      "Epoch: 612, loss=0.1600, train_acc=0.9692\n",
      "Epoch: 613, loss=0.1599, train_acc=0.9692\n",
      "Epoch: 614, loss=0.1599, train_acc=0.9692\n",
      "Epoch: 615, loss=0.1598, train_acc=0.9692\n",
      "Epoch: 616, loss=0.1597, train_acc=0.9692\n",
      "Epoch: 617, loss=0.1597, train_acc=0.9692\n",
      "Epoch: 618, loss=0.1596, train_acc=0.9692\n",
      "Epoch: 619, loss=0.1595, train_acc=0.9692\n",
      "Epoch: 620, loss=0.1594, train_acc=0.9692\n",
      "Epoch: 621, loss=0.1594, train_acc=0.9692\n",
      "Epoch: 622, loss=0.1593, train_acc=0.9692\n",
      "Epoch: 623, loss=0.1592, train_acc=0.9692\n",
      "Epoch: 624, loss=0.1592, train_acc=0.9692\n",
      "Epoch: 625, loss=0.1591, train_acc=0.9692\n",
      "Epoch: 626, loss=0.1590, train_acc=0.9692\n",
      "Epoch: 627, loss=0.1590, train_acc=0.9692\n",
      "Epoch: 628, loss=0.1589, train_acc=0.9692\n",
      "Epoch: 629, loss=0.1588, train_acc=0.9692\n",
      "Epoch: 630, loss=0.1587, train_acc=0.9701\n",
      "Epoch: 631, loss=0.1587, train_acc=0.9701\n",
      "Epoch: 632, loss=0.1586, train_acc=0.9701\n",
      "Epoch: 633, loss=0.1585, train_acc=0.9701\n",
      "Epoch: 634, loss=0.1585, train_acc=0.9701\n",
      "Epoch: 635, loss=0.1584, train_acc=0.9701\n",
      "Epoch: 636, loss=0.1583, train_acc=0.9701\n",
      "Epoch: 637, loss=0.1583, train_acc=0.9701\n",
      "Epoch: 638, loss=0.1582, train_acc=0.9701\n",
      "Epoch: 639, loss=0.1581, train_acc=0.9701\n",
      "Epoch: 640, loss=0.1581, train_acc=0.9701\n",
      "Epoch: 641, loss=0.1580, train_acc=0.9701\n",
      "Epoch: 642, loss=0.1579, train_acc=0.9701\n",
      "Epoch: 643, loss=0.1579, train_acc=0.9701\n",
      "Epoch: 644, loss=0.1578, train_acc=0.9701\n",
      "Epoch: 645, loss=0.1577, train_acc=0.9701\n",
      "Epoch: 646, loss=0.1577, train_acc=0.9701\n",
      "Epoch: 647, loss=0.1576, train_acc=0.9701\n",
      "Epoch: 648, loss=0.1576, train_acc=0.9701\n",
      "Epoch: 649, loss=0.1575, train_acc=0.9701\n",
      "\n",
      " Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97        55\n",
      "           1       0.91      0.89      0.90        55\n",
      "           2       0.94      0.98      0.96        52\n",
      "           3       0.98      0.91      0.94        56\n",
      "           4       0.94      0.98      0.96        64\n",
      "           5       0.94      0.92      0.93        73\n",
      "           6       0.98      0.98      0.98        57\n",
      "           7       0.98      0.98      0.98        62\n",
      "           8       0.89      0.90      0.90        52\n",
      "           9       0.91      0.94      0.93        68\n",
      "\n",
      "    accuracy                           0.95       594\n",
      "   macro avg       0.95      0.95      0.95       594\n",
      "weighted avg       0.95      0.95      0.95       594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
