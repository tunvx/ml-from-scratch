{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[VN]Lab 10.1-data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Công cụ Tiền xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axNaaAB1TR91"
      },
      "source": [
        "Bây giờ chúng ta sẽ xử lý rất nhanh bước đầu tiên của bộ công cụ này, đó là thư viện, cách import thư viện, cách chuẩn bị sẵn sàng để có thể bắt đầu xây dựng  mô hình học máy mới bất cứ lúc nào. Nhân tiện, thư viện là một biểu tượng của các mô-đun chứa các hàm và lớp mà bạn có thể thực hiện một số hành động và thao tác. Bạn sẽ thấy rằng các thư viện rất hữu ích để xây dựng các mô hình học máy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Import các thư viện\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n29ZDJywTY5J"
      },
      "source": [
        "Bây giờ chúng ta sẽ import 3 thư viện là Matplotlib, Pandas và NumPy. Các thư viện này rất hữu ích vì:\n",
        "*   **NumPy**: cho phép chúng ta làm việc với mảng vì bạn sẽ thấy các mô hình học máy trong tương lai sẽ cần một số mảng làm input.\n",
        "*   **Matplotlib**: cho phép chúng ta vẽ một số biểu đồ rất tuyệt. \n",
        "*   **Pandas**: không chỉ cho phép chúng ta import tập dữ liệu mà còn tạo ma trận các feature và vectơ biến phụ thuộc. Tất nhiên những điều này sẽ được giải thích sau.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-qiINBQSK2g"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Import tập dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80PwVmBbV1rq"
      },
      "source": [
        " Chúng ta sẽ tìm hiểu cách import tập dữ liệu sau: **Data.csv** là một tập dữ liệu rất đơn giản: giả sử một công ty bán lẻ đang thực hiện một số phân tích về những khách hàng đã mua một trong các sản phẩm của họ. Vì vậy, các hàng trong tập dữ liệu này tương ứng với các khách hàng khác nhau. Và đối với mỗi khách hàng này, chúng ta có quốc gia, tuổi tác, mức lương và liệu họ có mua sản phẩm hay không."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdRmTU85WWvj"
      },
      "source": [
        "\n",
        "Chúng ta import ** Data.csv ** trong Python, sử dụng thư viện Pandas và lưu trữ nó trong biến \"dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwEPNDWySTKm"
      },
      "source": [
        "dataset = pd.read_csv('Data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJMFCUUAW6KW"
      },
      "source": [
        "Có một nguyên tắc quan trọng hàng đầu trong ML. Trong bất kỳ tập dữ liệu nào mà bạn huấn luyện mô hình học máy sẽ có các thực thể giống nhau là các vectơ **features** và **dependent variable (biến phụ thuộc)**.\n",
        "\n",
        "**features** là các cột mà bạn sẽ dự đoán biến phụ thuộc và **dependent variable** là cột cuối cùng bởi vì công ty này muốn dự đoán xem liệu một số khách hàng tương lai có mua một sản phẩm nhất định dựa trên thông tin này. Vì vậy, các feature hay còn gọi là các biến độc lập, là các biến chứa một số thông tin mà bạn có thể dự đoán những gì bạn muốn dự đoán được gọi là biến phụ thuộc.\n",
        "\n",
        "Bạn sẽ có các feature riêng biệt, thường ở các cột đầu tiên của tập tập dữ liệu và biến phụ thuộc thường ở cột cuối cùng của tập dữ liệu. Hãy tạo hai thực thể này, gọi chúng là X cho ma trận các feature và Y cho vectơ biến phụ thuộc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MWACHTGYFnG"
      },
      "source": [
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R295ocAfaShc"
      },
      "source": [
        "Bây giờ chúng ta sẽ thực thi cell này để in ma trận của Feature X, kiểm tra xem liệu chúng ta có thực sự được tất cả các cột đầu tiên bên trong ma trận này không. Hãy nhớ rằng cột đầu tiên là các feature mà chúng ta muốn đưa vào ma trận X này, đầu tiên là: country (quốc gia), thứ hai là: age (tuổi) và thứ ba là: salary (lương). Đây là 3 cột. Và bên trong X, đầu tiên chúng ta có cột country với tất cả quốc gia của những khách hàng này, cột thứ hai là age: tuổi của họ và cột thứ ba là salary: mức lương hoặc mức lương ước tính của họ. Vậy nó đã hoàn hảo. Chúng ta thực sự nhận được ma trận của Feature X chứa tất cả các feature hay còn gọi là các biến độc lập."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "hCsz2yCebe1R",
        "outputId": "1e4cc568-4e51-4b38-9d46-4aa3f15204be"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-71AQLvar7V"
      },
      "source": [
        "Và bây giờ hãy chạy cell để in y, vectơ biến phụ thuộc. Và nó thực sự nhận được vectơ biến phụ thuộc chứa tất cả các quyết định liệu khách hàng có mua sản phẩm hay không."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "eYrOQ43XcJR3",
        "outputId": "e0873b2a-3b08-4bab-ef0d-15b88858ca44"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Xử lý dữ liệu bị thiếu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcFDf9T0bKas"
      },
      "source": [
        "Nếu xem lại tập dữ liệu **Data.csv**, chúng ta thấy rằng ở đây còn thiếu mức lương cho khách hàng cụ thể 40 tuổi đến từ Đức và đã mua một sản phẩm. Nhìn chung, bạn không muốn có bất kỳ dữ liệu nào bị thiếu trong tập dữ liệu đơn giản là vì nó có thể gây ra một số lỗi khi huấn luyện mô hình, do đó bạn phải xử lý chúng. Thực tế có một số cách để xử lý:\n",
        "\n",
        "\n",
        "*   Bỏ qua quan sát bằng cách xóa nó.\n",
        "*   Thay thế giá trị bị thiếu bằng giá trị trung bình của tất cả các giá trị trong cột mà dữ liệu bị thiếu, **đây là những gì mà bây giờ chúng ta đang thêm trong bộ công cụ.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c93k7ipkSexq"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qhC90IZc3sC"
      },
      "source": [
        "Chúng ta có thể thấy rõ ràng mức lương bị thiếu trong ma trận các feature trước đó, X đã được thay thế bằng mức lương trung bình của cột này."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "3UgLdMS_bjq_",
        "outputId": "254af4e0-681e-47f5-aaa7-b9c6f43258e9"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Mã hóa dữ liệu phân loại"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTkZIoJbeA-o"
      },
      "source": [
        "Như chúng ta có thể thấy, tập dữ liệu này chứa một cột với các category (hạng mục): Pháp, Tây Ban Nha hoặc Đức. Có thể đoán rằng sẽ rất khó để mô hình học máy tính toán một số mối tương quan giữa các cột này, các feature và outcome là biến phụ thuộc và do đó sẽ phải chuyển các string này, các hạng mục này thành số."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Mã hóa biến độc lập"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfS_yogMe3EW"
      },
      "source": [
        "Sử dụng biểu diễn One-hot (One-hot Encoding) để mã hóa các biến độc lập."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hwuVddlSwVi"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "f7QspewyeBfx",
        "outputId": "5b35feef-7fe2-46ef-ce70-80495f94f4ed"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Mã hóa biến phụ thuộc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6i4DViBfK1V"
      },
      "source": [
        "Sử dụng Label Encoding để mã hóa biến phụ thuộc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHCShVyTOYY"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FyhY8-gPpFCa",
        "outputId": "7f76ef29-5423-4c3e-cf69-45fbc366a997"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Chia tập dữ liệu thành Training set và Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBEHcb6BjjSS"
      },
      "source": [
        "Việc chia tập dữ liệu thành training set (tập huấn luyện) trong một test bao gồm việc tạo hai tập riêng biệt, một training set mà bạn sẽ huấn luyện mô hình máy về các quan sát hiện có và một test set (tập kiểm tra) để đánh giá hiệu suất của mô hình dựa trên các quan sát mới. Bạn sẽ sử dụng **train_test_split** từ *scikit-learning* để thực hiện việc tách với **test_size = 0.2** của tập dữ liệu gốc và **random_state = 1** để đảm bảo bạn sẽ có cùng tập dữ liệu khi thực hiện chia tách."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXgA6CzlqbCl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "GuwQhFdKrYTM",
        "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "TUrX_Tvcrbi4",
        "outputId": "9a041a9b-2642-4828-fa2f-a431d7d77631"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pSMHiIsWreQY",
        "outputId": "5afe91e0-9244-4bf5-ec1b-e3e092b85c08"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "I_tW7H56rgtW",
        "outputId": "2a93f141-2a99-4a69-eec5-c82a3bb8d36b"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5bwZXD7jnxt"
      },
      "source": [
        "Feature scaling là một phương pháp được sử dụng để chuẩn hóa biên độ (range) của các biến độc lập hoặc các feature của dữ liệu.\n",
        "\n",
        "Tại sao chúng ta cần làm điều này? Đó là do với một số mô hình học máy, để tránh một số feature bị chi phối bởi các feature khác theo cách mà các feature bị chi phối thậm chí không được mô hình học máy xem xét. Cũng cần lưu ý rằng chúng ta **sẽ không phải áp dụng feature scaling cho tất cả các mô hình học máy** mà chỉ áp dụng cho một số mô hình."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foE54_r9oLhz"
      },
      "source": [
        "Bạn sẽ mở rộng tập dữ liệu của mình bằng cách sử dụng **StandardScaler** từ *scikit-learning*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxjSUXFQqo-3"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "DWPET8ZdlMnu",
        "outputId": "dea86927-5124-4e2a-e974-2804df9a913c"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "sTXykB_QlRjE",
        "outputId": "b68f0cfc-d07c-48cb-80d0-6800028c41f9"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntwq1pMAkvja"
      },
      "source": [
        "Liệu chúng ta có phải áp dụng feature scaling trước khi tách tập dữ liệu thành training set (tập huấn luyện) và kiểm tra lúc đó hay sau đó không? Một số người cho rằng chúng ta cần áp dụng feature scaling trước khi nó tách ra. Một số người cho rằng cần thực hiện sau khi tách. **Câu trả lời là chúng ta cần áp dụng feature scaling sau khi tách tập dữ liệu thành tập huấn luyện**.\n",
        "\n",
        "Hãy để tôi giải thích lý do tại sao chúng ta cần áp dụng feature scaling sau khi tách tập dữ liệu thành tập huấn luyện và kiểm tra nó thực sự rõ ràng. Đơn giản là vì test set (tập kiểm tra) được cho là một tập hợp hoàn toàn mới mà bạn sẽ đánh giá mô hình học máy. Vì vậy, nó khá giống như bạn đang huấn luyện mô hình học máy trên tập huấn luyện. Sau khi nó được huấn luyện, bạn sẽ triển khai nó trên các quan sát mới. Điều này có nghĩa là tập kiểm tra là thứ mà bạn không nên làm cho huấn luyện và các feature. Scaling là một kỹ thuật sẽ lấy mean và độ lệch chuẩn của feature để thực hiện chia tỷ lệ. Vì vậy, nếu chúng ta áp dụng feature scaling trước khi phân tách thì nó sẽ thực sự nhận được mean và độ lệch chuẩn của tất cả các giá trị, gồm các giá trị trong tập kiểm tra. Và vì test là thứ bạn không cần phải có, giống như một số dữ liệu tương lai trong quá trình sản xuất. Việc áp dụng các feature trên tập dữ liệu gốc trước khi phân tách sẽ gây ra **rò rỉ thông tin** trên tập kiểm tra. Chúng ta sẽ lấy một số thông tin từ tập kiểm tra mà chúng ta không nên lấy vì nó được cho là dữ liệu mới với các quan sát mới.\n",
        "\n",
        "Vì vậy, hãy nhớ điều này. Lý do quan trọng mà bạn không nên áp dụng feature scaling trước khi phân tách là để ngăn chặn rò rỉ thông tin trên tập kiểm tra, điều mà bạn không nên có cho đến khi thực hiện xong quá trình huấn luyện."
      ]
    }
  ]
}