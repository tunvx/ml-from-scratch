{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeT7fL_v5nXl"
   },
   "source": [
    "# Hồi quy đa biến (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xARKoAT5nXs"
   },
   "source": [
    "Trong notebook đầu tiên, chúng ta đã khám phá hồi quy đa biến sử dụng mô hình mặc định của sklearn. Giờ chúng ta sẽ sử dụng riêng numpy để giải quyết các trọng số hồi quy với gradient descent.\n",
    "\n",
    "Trong notebook này, chúng ta sẽ đề cập tới các trọng số của hồi quy đa biến qua gradient descent. Chúng ta sẽ:\n",
    "* Thêm một cột không đổi của 1 vào DataFrame để tính intercept.\n",
    "* Xuất DataFrame hoặc cột (Series) thành một mảng Numpy.\n",
    "* Viết hàm predict_output() sử dụng Numpy.\n",
    "* Viết một hàm numpy để tính đạo hàm của các trọng số hồi quy với một đặc trưng duy nhất.\n",
    "* Viết hàm gradient descent tính các trọng số hồi quy biết vectơ trọng số ban đầu, kích thước bước và dung sai.\n",
    "* Sử dụng hàm gradient descent để ước tính các trọng số hồi quy cho nhiều đặc trưng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O19m30_u5nXu"
   },
   "source": [
    "# Load thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--qBSgGz5nXv"
   },
   "source": [
    "Đảm bảo đã có các thư viện theo yêu cầu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DpbkSP_55nXv"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28gSYcBl5nXx"
   },
   "source": [
    "## Load dữ liệu doanh số bán nhà\n",
    "\n",
    "Tập dữ liệu doanh số bán nhà ở quận King, Seatle, WA. Nghe quen chứ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XB4Vgr7j5nXy"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../house_data/kc_house_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../house_data/kc_house_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../house_data/kc_house_data.csv'"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv(\"../house_data/kc_house_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUU24OJw5nXz"
   },
   "source": [
    "Nếu muốn thực hiện bất kỳ \"feature engineering\" nào như tạo các đặc trưng mới hoặc điều chỉnh đặc trưng sẵn có, chúng ta có thể sửa DataFrame của pandas như trong lab trước (*Lab 2*). Tuy nhiên, với notebook này, chúng ta sẽ làm việc với các đặc trưng có sẵn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgZKhkjW5nXz"
   },
   "source": [
    "## Chuyển thành mảng Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkbZTwl05nX0"
   },
   "source": [
    "Để hiểu chi tiết về việc triển khai các thuật toán, cần làm việc với một thư viện cho phép thoa tác trực tiếp với ma trận (và được tối ưu hóa). Numpy là một giải pháp Python để làm việc với ma trận (hoặc bất kỳ \"mảng\" đa chiều nào).\n",
    "\n",
    "Giá trị dự đoán cho các trọng số và đặc trưng chỉ là tích vô hướng giữa đặc trưng và vectơ trọng số. Tương tự, nếu chúng ta đặt tất cả các đặc trưng thành từng hàng trong một ma trận thì có thể tính giá trị dự đoán cho *tất cả* các quan sát bằng cách nhân \"ma trận đặc trưng\" với \"vectơ trọng số\".\n",
    "\n",
    "Trước tiên, chúng ta cần lấy DataFrame đã sắp xếp và trích xuất dữ liệu bên dưới dưới thành một mảng numpy 2D (còn được gọi là ma trận). Để làm điều này, chúng ta có thể sử dụng thuộc tính .values của Panda để chuyển đổi dataframe thành một ma trận numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyBvnRXG5nX1"
   },
   "outputs": [],
   "source": [
    "import numpy as np # điều này cho phép gọi numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoemrJkn5nX2"
   },
   "source": [
    "Bây giờ chúng ta sẽ viết một hàm nhận DataFrame, một list tên các đặc trưng (chẳng hạn: ['sqft_living', 'bedrooms']) và một đặc trưng mục tiêu (ví dụ: 'price') và trả về 2 điều sau:\n",
    "* Một ma trận numpy có các cột là các đặc trưng mong muốn cộng với một cột không đổi (đây là cách chúng ta tạo 'intercept').\n",
    "* Một mảng numpy chứa các giá trị của đầu ra.\n",
    "\n",
    "Với những điều này, hãy hoàn thành hàm sau (nếu có dòng trống, hãy viết một dòng code thực hiện những gì chú giải ở trên chỉ ra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mxo-gvmw5nX3"
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data, features_title, output_title):\n",
    "    if('constant' not in data):\n",
    "        data['constant'] = 1 # đây là cách thêm cột constant. Chỉ thực hiện khi cần \n",
    "    # thêm cột 'constant' vào trước list các đặc trưng để chúng ta có thể trích xuất cùng với những thứ khác:\n",
    "    features_title = ['constant'] + features_title # đây là cách kết hợp 2 list\n",
    "    # chia dữ liệu thành sub-DataFrame chứa các đặc trưng đã chỉ định (gồm constant)\n",
    "    # gọi nó là features_columns.\n",
    "\n",
    "    # dòng tiếp theo sẽ trích xuất ma trận numpy từ biến features_columns:\n",
    "    feature_matrix = features_columns.values\n",
    "    # truy xuất dữ liệu được liên kết với đầu ra trong pandas Series\n",
    "    # gọi nó là output_column\n",
    "\n",
    "    # tiếp theo sẽ chuyển đổi Series đã nhắc thành một mảng numpy\n",
    "    output_array = output_column.values\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaQHjGpi5nX4"
   },
   "source": [
    "**Suy nghĩ: Lab trước không hề chỉ định bất kỳ cột không đổi nào. Chúng ta có mắc lỗi ở đâu không?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpooIcIk5nX5"
   },
   "source": [
    "Chúng ta sẽ sử dụng đặc trưng 'sqft_living' và một hằng số làm đặc trưng và giá làm đầu ra để kiểm tra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ppDhRXgS5nX5",
    "outputId": "74e9afcc-fa51-4504-9663-64d3eabe2839"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (example_features, example_output) \u001b[38;5;241m=\u001b[39m get_numpy_data(full_data, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqft_living\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# [] quanh 'sqft_living' tạo một list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(example_features[\u001b[38;5;241m0\u001b[39m,:]) \u001b[38;5;66;03m# điều này truy cập hàng đầu tiên của dữ liệu, ':' chỉ 'all columns'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(example_output[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_data' is not defined"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(full_data, ['sqft_living'], 'price') # [] quanh 'sqft_living' tạo một list\n",
    "print(example_features[0,:]) # điều này truy cập hàng đầu tiên của dữ liệu, ':' chỉ 'all columns'\n",
    "print(example_output[0]) # và đầu ra tương ứng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14MpQ_Zw5nX7"
   },
   "source": [
    "## Dự đoán đầu ra với các trọng số hồi quy đã cho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XDIFfPN5nX8"
   },
   "source": [
    "Giả sử chúng ta có trọng số [1.0, 1.0] và đặc trưng [1.0, 1180.0], chúng ta muốn tính kết quả dự đoán $1.0*1.0 + 1.0*1180.0 = 1181.0$ (đây là tích vô hướng giữa 2 mảng). Nếu chúng là mảng numpy, chúng ta có thể dùng `np.dot` để tính:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBXEyW1d5nX9",
    "outputId": "dc503049-a23c-4cdb-fa75-a5e16b8c1737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181.0\n"
     ]
    }
   ],
   "source": [
    "my_weights = np.array([1., 1.]) # trọng số mẫu\n",
    "my_features = example_features[0,] # chúng ta sẽ dùng điểm dữ liệu đầu tiên\n",
    "predicted_value = np.dot(my_features, my_weights)\n",
    "print(predicted_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ois947xL5nX9"
   },
   "source": [
    "`np.dot` cũng hoạt động khi xử lý ma trận và vectơ. Dự đoán từ các quan sát là tích vô hướng ĐÚNG (như trọng số ở bên phải) giữa các *ma trận* đặc trưng và *vectơ* trọng số. Hãy hoàn thành hàm `predict_output` sau để tính các dự đoán cho toàn bộ ma trận đặc trưng với ma trận và các trọng số đã cho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rB68O-o5nX-"
   },
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # giả sử ma trận feature_matrix chứa các đặc trưng ở dạng các cột và trọng số là mảng numpy tương ứng\n",
    "    # tạo vectơ dự đoán sử dụng np.dot()\n",
    "\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDmVLS1m5nX-"
   },
   "source": [
    "Chạy cell say và quan sát kết quả nếu muốn kiểm tra code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EJw_pJ95nX_",
    "outputId": "e7534f25-7f01-4a22-dfdb-1f2332648e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181.0\n",
      "2571.0\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict_output(example_features, my_weights)\n",
    "print(test_predictions[0]) # nên là 1181.0\n",
    "print(test_predictions[1]) # nên là 2571.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caBRkKhr5nYA"
   },
   "source": [
    "# Tính đạo hàm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbdwUX8p5nYA"
   },
   "source": [
    "Bây giờ chúng ta sẽ chuyển sang tính đạo hàm của hàm chi phí hồi quy. Hàm chi phí là tổng các điểm dữ liệu của hiệu bình phương giữa kết quả quan sát và kết quả dự đoán.\n",
    "\n",
    "Vì đạo hàm của một tổng là tổng các đạo hàm nên chúng ta có thể tính đạo hàm cho từng điểm dữ liệu và rồi tính tổng các điểm dữ liệu. Chúng ta có thể viết hiệu bình phương giữa kết quả quan sát và kết quả dự đoán như sau: \n",
    "\n",
    "$(w_0 * const + w_1 *feature_1 + ... + w_i  * feature_i + ... +  w_k * feature_k - output)^2$\n",
    "\n",
    "Chúng ta có k đặc trưng và một hằng số. Như vậy theo quy tắc dây chuyền, đạo hàm với trọng số $w_i$ là:\n",
    "\n",
    "$2 * (w_0 * const + w_1 *feature_1 + ... + w_i  * feature_i + ... +  w_k * feature_k - output) * feature_i$\n",
    "\n",
    "Phần tử bên trong ngoặc là sai số (hiệu giữa dự đoán và kết quả). Như vậy, chúng ta có thể viết lại thành:\n",
    "\n",
    "$2 * error * feature_i$\n",
    "\n",
    "Đạo hàm cho trọng số của đặc trưng $ i $ là tổng (các điểm dữ liệu) của 2 nhân tích của error và đặc trưng đó. Trong trường hợp đặc trưng là hằng số thì là gấp đôi tổng sai số!\n",
    "\n",
    "\n",
    "2 lần tổng của hai vectơ chỉ là hai nhân tích của hai vectơ. Do đó, đạo hàm cho trọng số của $ feature_i $ bằng hai lần tích vô hướng giữa các giá trị của $ feature_i $ và các sai số hiện tại.\n",
    "\n",
    "Hãy hoàn thành hàm đạo hàm sau đây để tính đạo hàm của trọng số cho giá trị của đặc trưng (trên tất cả các điểm dữ liệu) và sai số (trên tất cả các điểm dữ liệu). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qarip5dK5nYC"
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    # Giả sử sai số và đặc trưng đều là mảng numpy có cùng độ dài (số điểm dữ liệu)\n",
    "    # tính 2 lần tích vô hướng của các vectơ đó làm 'đạo hàm' và trả về giá trị\n",
    "\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29q2CgvV5nYD"
   },
   "source": [
    "Để kiểm tra đạo hàm, chạy cell sau: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsyybEmM5nYD",
    "outputId": "c4153c10-3806-41a3-e9f7-eb35ff49298d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23345850016.0\n",
      "-23345850016.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(full_data, ['sqft_living'], 'price') \n",
    "my_weights = np.array([0., 0.]) # this makes all the predictions 0 điều này làm cho tất cả dự đoán là 0\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "# cũng giống như SFrames, 2 mảng numpy có thể trừ với '-':\n",
    "errors = test_predictions - example_output # sai số dự đoán trong trường hợp này chỉ là example_output\n",
    "feature = example_features[:,0] # tính đạo hàm với 'constant', \":\" chỉ tất cả các hàng\n",
    "derivative = feature_derivative(errors, feature)\n",
    "print(derivative)\n",
    "print(-np.sum(example_output)*2) # nên giống với đạo hàm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNMQ43DX5nYE"
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYZ_J5gP5nYE"
   },
   "source": [
    "Bây giờ chúng ta sẽ viết một hàm thực hiện gradient descent. Tiền đề cơ bản khá đơn giản. Với một điểm bắt đầu, chúng ta cập nhật các trọng số hiện tại bằng cách di chuyển theo hướng gradient âm. Gradient có hướng *tăng* nên gradient âm có hướng *giảm* và chúng ta đang cố gắng *giảm thiểu* hàm chi phí.\n",
    "\n",
    "Lượng mà chúng ta di chuyển theo *hướng* gradient âm được gọi là 'kích thước bước'. Chúng ta dừng lại khi chúng ta 'đủ gần' với mức tối ưu. Điều này được xác định bằng cách yêu cầu độ lớn (chiều dài) của vectơ gradient phải nhỏ hơn một 'dung sai' cố định.\n",
    "\n",
    "Hãy hoàn thành hàm gradient descent sau bằng cách sử dụng hàm đạo hàm ở trên. Với mỗi bước trong gradient descent, chúng ta cập nhật trọng số cho từng đặc trưng trước khi tính tiêu chí dừng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZR5wwvNs5nYF"
   },
   "outputs": [],
   "source": [
    "from math import sqrt # độ lớn/chiều dài của một vectơ [g[0], g[1], g[2]] là căn bậc hai của (g[0]^2 + g[1]^2 + g[2]^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDtfAVhR5nYG"
   },
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # đảm bảo đây là mảng numpy\n",
    "    while not converged:\n",
    "        # tính các dự đoán dựa trên feature_matrix và các trọng số sử dụng hàm predict_output() function\n",
    "\n",
    "        # tính sai số dưới dạng dự đoán - đầu ra\n",
    "\n",
    "        gradient_sum_squares = 0 # khởi tạo gradient_sum_squares\n",
    "        # khi chưa đạt tới dung sai, hãy cập nhất trọng số cho từng đặc trưng\n",
    "        for i in range(len(weights)): # lặp qua từng trọng số\n",
    "            # feature_matrix[:, i] là cột đặc trưng liên kết với weights[i]\n",
    "            # tính đạo hàm cho weight[i]:\n",
    "            \n",
    "            # cộng bình phương giá trị của đạo hàm vào tổng bình phương gradient (để đánh giá hội tụ)\n",
    "\n",
    "            # trọng số hiện tại trừ stepsize nhân với đạo hàm\n",
    "            \n",
    "        # tính căn bậc hai của tổng bình phương gradient để lấy độ lớn của gradient:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ofhOXK_5nYG"
   },
   "source": [
    "Một số điều cần lưu ý trước khi chạy gradient descent: Vì gradient là tổng của tất cả các điểm dữ liệu và liên quan đến tích của sai số và đặc trưng nên bản thân gradient sẽ rất lớn do các đặc trưng (squarefeet) và đầu ra (giá) lớn. Vì vậy, mặc dù chúng ta dự kiến \"dung sai\" nhỏ, nhưng chỉ nhỏ tương đối với kích thước các đặc trưng.\n",
    "\n",
    "Tương tự, kích thước bước sẽ nhỏ hơn nhiều so với dự kiến nhưng điều này là do gradient có các giá trị lớn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_kHZeI_5nYH"
   },
   "source": [
    "# Chạy Gradient Descent như Hồi quy đơn giản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmbYyPpw5nYH"
   },
   "source": [
    "Trước tiên, hãy chia thành tập huấn luyện và tập kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbAitUbi5nYI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(full_data, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J49m0JwN5nYI"
   },
   "source": [
    "Mặc dù gradient descent được thiết kế cho hồi quy đa biến vì hằng số bây giờ là một đặc trưng, chúng ta có thể sử dụng hàm gradient descent để ước tính các tham số trong hồi quy đơn giản trên squarefeet. Các cell sau thiết lập feature_matrix, đầu ra, trọng số ban đầu và kích thước bước cho mô hình đầu tiên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbfxWwD85nYJ"
   },
   "outputs": [],
   "source": [
    "# hãy kiểm tra gradient descent\n",
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ2yAogo5nYK"
   },
   "source": [
    "Tiếp theo, chúng ta sẽ chạy gradient descent với các tham số trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-_WCDHf5nYK"
   },
   "outputs": [],
   "source": [
    "updated_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RymlGyVR5nYK"
   },
   "source": [
    "Các trọng số này so với những trọng số đạt được ở tuần 1 thế nào (không dự kiến chúng giống hệt nhau)? \n",
    "\n",
    "**Quiz: Giá trị của trọng số cho sqft_living -- phần tử thứ hai của ‘simple_weights’ là bao nhiêu (làm trong tới chữ số thập phân thứ nhất)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvnqlw-W5nYL"
   },
   "outputs": [],
   "source": [
    "# Có thể in ra tất cả trọng số nếu muốn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqB7AA655nYL"
   },
   "source": [
    "Hãy sử dụng các trọng số mới ước tính và `predict_output` để tính các dự đoán trên dữ liệu KIỂM TRA (cần tạo một mảng numpy của feature_matrix và kiểm tra đầu ra trước tiên):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLNnmcUp5nYM"
   },
   "outputs": [],
   "source": [
    "(test_simple_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kld_6AwV5nYM"
   },
   "source": [
    "Bây giờ có thể tính các dự đoán sử dụng test_simple_feature_matrix và các trọng số ở trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f7MdXNF5nYM"
   },
   "outputs": [],
   "source": [
    "# sử dụng predict_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj7v7wFe5nYN"
   },
   "source": [
    "**Quiz: Giá dự đoán cho ngôi nhà đầu tiên trong tập dữ liệu KIỂM TRA cho mô hình 1 là bao nhiêu (làm tròn thành đô la)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj23bSZG5nYN"
   },
   "outputs": [],
   "source": [
    "# index đầu tiên trong ngôn ngữ lập trình là gì?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wiy6rtqT5nYN"
   },
   "source": [
    "Giờ chúng ta đã có các dự đoán trên dữ liệu kiểm tra, tính RSS trên tập kiểm tra. Lưu giá trị này để so sánh sau. Nhắc lại rằng RSS là tổng các sai số bình phương (hiệu giữa dự đoán và kết quả)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stL-aW0u5nYP"
   },
   "outputs": [],
   "source": [
    "# trừ, bình phương, cộng lại và in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZdV5UzX5nYQ"
   },
   "source": [
    "# Chạy hồi quy đa biến "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu3FlcvW5nYQ"
   },
   "source": [
    "Bây giờ chúng ta sẽ dùng nhiều hơn một đặc trưng. Sử dụng code sau để tạo ra các trọng số cho mô hình thứ hai với các tham số sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee5Bpq-t5nYQ"
   },
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 là diện tích trung bình cho 15 hàng xóm gần nhất. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQF0TeNK5nYR"
   },
   "source": [
    "Sử dụng các tham số trên để ước tính trọng số mô hình. Ghi lại các giá trị này cho quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_o8qRNt5nYR"
   },
   "outputs": [],
   "source": [
    "# Thực hiện như phần trước"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqkP2ArG5nYS"
   },
   "source": [
    "Sử dụng các trọng số mới ước tính và hàm `predict_output` để tính các dự đoán trên dữ liệu KIỂM TRA. Đừng quên tạo một mảng numpy cho các đặc trưng từ tập kiểm tra đầu tiên!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILkvDh8J5nYT"
   },
   "outputs": [],
   "source": [
    "# ba cái thứ hai vẫn chưa được truyền."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRSb7XNG5nYT"
   },
   "source": [
    "**Quiz: Giá dự đoán cho ngôi nhà thứ nhất trong tập dữ liệu KIỂM TRA cho mô hình 2 là bao nhiêu (làm tròn thành đô la)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txA9TIoU5nYU"
   },
   "outputs": [],
   "source": [
    "# một lần nữa, index đầu tiên"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxZmBbU75nYU"
   },
   "source": [
    "Giá trị thực cho ngôi nhà thứ nhất trong tập dữ liệu kiểm tra là bao nhiêu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCMeNZGY5nYV"
   },
   "outputs": [],
   "source": [
    "# tìm nó trong test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDAxJ9tB5nYV"
   },
   "source": [
    "**Quiz: Ước tính nào gần với giá thực cho ngôi nhà thứ nhất trong tập KIỂM TRA: mô hình 1 hay 2?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF9E54oP5nYV"
   },
   "source": [
    "Sử dụng các dự đoán và kết quả để tính RSS cho mô hình 2 trên dữ liệu KIỂM TRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-aDHZKR5nYW"
   },
   "outputs": [],
   "source": [
    "# trừ,... Đợi chút. Copy, paste và sửa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peFdJePZ5nYW"
   },
   "source": [
    "**Quiz: Mô hình nào (1 hay 2) có RSS thấp nhất trong tất cả dữ liệu KIỂM TRA?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "HHxq-owW5nYW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[VN]exercise-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
